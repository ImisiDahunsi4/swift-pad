---
title: Get YouTube Video Transcripts with yt-dlp
---

In this guide, we'll show you how to transcribe YouTube videos.

For this, we use the [yt-dlp](https://github.com/yt-dlp/yt-dlp) library to download YouTube videos and then transcribe it with the AssemblyAI API.

`yt-dlp` is a [youtube-dl](https://github.com/ytdl-org/youtube-dl) fork with additional features and fixes. It is better maintained and preferred over `youtube-dl` nowadays.

In this guide we'll show 2 different approaches:

- Option 1: Download video via CLI
- Option 2: Download video via code

Let's get started!

## Quickstart

```python
import assemblyai as aai
import yt_dlp

def transcribe_youtube_video(video_url: str, api_key: str) -> str:
    """
    Transcribe a YouTube video given its URL.

    Args:
        video_url: The YouTube video URL to transcribe
        api_key: AssemblyAI API key

    Returns:
        The transcript text
    """
    # Configure yt-dlp options for audio extraction
    ydl_opts = {
        'format': 'm4a/bestaudio/best',
        'outtmpl': '%(id)s.%(ext)s',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'm4a',
        }]
    }

    # Download and extract audio
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([video_url])
        # Get video ID from info dict
        info = ydl.extract_info(video_url, download=False)
        video_id = info['id']

    # Configure AssemblyAI
    aai.settings.api_key = api_key

    # Transcribe the downloaded audio file
    transcriber = aai.Transcriber()
    transcript = transcriber.transcribe(f"{video_id}.m4a")

    return transcript.text

transcript_text = transcribe_youtube_video("https://www.youtube.com/watch?v=wtolixa9XTg", "YOUR-API-KEY")
print(transcript_text)
```

# Step-by-step guide

## Install Dependencies

Install [yt-dlp](https://github.com/yt-dlp/yt-dlp) and the [AssemblyAI Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk) via pip.

```bash
pip install -U yt-dlp
```

```bash
pip install assemblyai
```

## Option 1: Download video via CLI

In this approach we download the YouTube video via the command line and then transcribe it via the AssemblyAI API. We use the following video here:

- https://www.youtube.com/watch?v=wtolixa9XTg

To download it, use the `yt-dlp` command with the following options:

- `-f m4a/bestaudio`: The format should be the best audio version in m4a format.
- `-o "%(id)s.%(ext)s"`: The output name should be the id followed by the extension. In this example, the video gets saved to "wtolixa9XTg.m4a".
- `wtolixa9XTg`: the id of the video.

```bash
yt-dlp -f m4a/bestaudio -o "%(id)s.%(ext)s" https://www.youtube.com/watch?v=wtolixa9XTg
```

Next, set up the AssemblyAI SDK and trancribe the file. Replace `YOUR_API_KEY` with your own key. If you don't have one, you can [sign up here](https://assemblyai.com/dashboard/signup) for free.

Make sure that the path you pass to the `transcribe()` function corresponds to the saved filename.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe("wtolixa9XTg.m4a")
print(transcript.text)
```

## Option 2: Download video via code

In this approach we download the video with a Python script instead of the command line.

You can download the file with the following code:

```python
import yt_dlp

URLS = ['https://www.youtube.com/watch?v=wtolixa9XTg']

ydl_opts = {
    'format': 'm4a/bestaudio/best',  # The best audio version in m4a format
    'outtmpl': '%(id)s.%(ext)s',  # The output name should be the id followed by the extension
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',
    }]
}


with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)
```

After downloading, you can use the same code from option 1 to transcribe the file:

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe("wtolixa9XTg.m4a")
```


---
title: Build a UI for Transcription with Gradio and Python
---

In this guide, we'll show you how to use the [Gradio](https://www.gradio.app/) library in Python to build a simple front-end that allows you to drag-and-drop files for transcription. If you've ever gotten tired of running your Python program from the command-line, now you can have a self-hosted UI for processing your transcripts!

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Installing Dependencies

Firstly, we'll need to get all of our dependencies installed. This demo depends on both the AssemblyAI Python SDK as well as Gradio, so we'll need to run the following command to get them both installed, or updated to their latest version if you've already gotten them installed.

```bash
pip install -U assemblyai gradio
```

## Creating Your Transcription Function

Now we'll move on to setting up a function that can handle transcription for us. This function will submit the filepath for the audio file you've uploaded via Gradio to our API, and will wait until the transcript has finished, or log an error to the console.

```python
import assemblyai

# Set your API key here, which can be found on your Dashboard as mentioned above.
assemblyai.settings.api_key = "API_KEY_HERE"


def transcribe(audio_path):
    transcriber = assemblyai.Transcriber()
    transcript = transcriber.transcribe(audio_path)

    if transcript.status == assemblyai.TranscriptStatus.error:
        print(f"Transcription failed: {transcript.error}")
```

## Setting Up our Gradio UI

Now we get to move on to setting up what our Gradio UI will look like. This project will be fairly simple, so it will only use a few components, but Gradio offers a wide range of components and ways to customize how they look to suit your needs. Check out their [documentation](https://www.gradio.app/docs) here for more detailed information.

We'll be using Gradio's Blocks API for a more custom way of setting up our UI. Specifically, we'll use their Markdown, File, Button, and Textbox components to create a title, enable file uploads, and submit files for transcription to render them to the screen.

Since we only want audio or video files to be submitted for transcription, we'll limit what can be uploaded with the `file_types` parameter.

```python
import gradio

with gradio.Blocks() as demo:
    gradio.Markdown("# AssemblyAI Transcription with Gradio")
    filepath = gradio.File(file_types=["audio", "video"])
    transcribe_button = gradio.Button(value="Transcribe")
    transcript = gradio.Textbox(value="", label="Transcript")
    transcribe_button.click(transcribe, inputs=[filepath], outputs=[transcript])

if __name__ == "__main__":
    demo.launch(debug=True, show_error=True)
```

Now you can run this code to deploy your Gradio app locally, with some helpful debug information in case you add more functionality. You can also get a publicly shareable link to your app by enabling `share=True` in your `.launch()` config. Also note that you should run this in a separate file outside of this Jupyter notebook so that your program doesn't time out. We hope this helps you create better interfaces for your integrations with our API, and please reach out to support@assemblyai.com if you have any questions we can help with!


---
title: Detect Low Confidence Words in a Transcript
---

In this guide, we'll show you how to detect sentences that contain words with low confidence scores. Confidence scores represent how confident the model was in predicting the transcribed word. Detecting words with low confidence scores can be important for manually editing transcripts.
Each transcribed word will contain a corresponding confidence score between 0.0 (low confidence) and 1.0 (high confidence).
You can decide what your confidence threshold will be when implementing this logic in your application. For this guide, we will use a threshold of 0.4.

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can sign up for an account and get your API key from your [dashboard](https://www.assemblyai.com/app/account). This guide will use AssemblyAI's [node SDK](https://github.com/AssemblyAI/assemblyai-node-sdk). If you haven't already, install the SDK by following these [instructions](https://github.com/AssemblyAI/assemblyai-node-sdk#installation).

## Step-by-Step Instructions

Import the AssemblyAI package and create an AssemblyAI object with your API key:

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: process.env.ASSEMBLYAI_API_KEY,
});
```

Next create the transcript with your audio file, either via local audio file or URL (AssemblyAI's servers need to be able to access the URL, make sure the URL links to a downloadable file).

```javascript
const transcript = await client.transcripts.transcribe({
  audio_url: "./sample.mp4",
});
```

From there use the `id` from the transcript to request the transcript broken down into sentences.

```javascript
let { id } = transcript;
let { sentences } = await client.transcripts.sentences(id);
```

Set the confidence score threshold to a value of you choice (0.5 or less is a good start). In this guide, we'll use 0.4.

```javascript
let confidenceThreshold = 0.4;
```

Next, we will filter the sentences array down to just sentences that contain words with confidence scores of under 0.4.

```javascript
const sentencesWithLowConfidenceWords = (sentences, confidenceThreshold) => {
  return sentences.filter((sentence) => {
    const hasLowConfidenceWord = sentence.words.some(
      (word) => word.confidence < confidenceThreshold
    );
    return hasLowConfidenceWord;
  });
};

const filteredSentences = sentencesWithLowConfidenceWords(
  sentences,
  confidenceThreshold
);
```

Next we'll alter the `filteredSentences` array so that the `words` array for each sentence only contains the words with confidence scores under of 0.4.

```javascript
const filterScores = filteredSentences.map((item) => {
  return {
    ...item,
    words: item.words.filter((word) => word.confidence < confidenceThreshold),
  };
});
```

Finally, we'll display the final results. The final results will include the timestamp of the sentence that contains low confidence words, the sentence, the words that scored poorly, and their scores.

```javascript
//This function is optional but can be used to format the timestamps from milleseconds to HH:MM:SS
const formatMilliseconds = (milliseconds) => {
  // Calculate hours, minutes, and seconds
  const hours = Math.floor(milliseconds / 3600000);
  const minutes = Math.floor((milliseconds % 3600000) / 60000);
  const seconds = Math.floor((milliseconds % 60000) / 1000);

  // Ensure the values are displayed with leading zeros if needed
  const formattedHours = hours.toString().padStart(2, "0");
  const formattedMinutes = minutes.toString().padStart(2, "0");
  const formattedSeconds = seconds.toString().padStart(2, "0");

  return `${formattedHours}:${formattedMinutes}:${formattedSeconds}`;
};

//Format the final results to contain the sentence, low confidence words, timestamps, and confidence scores.
const finalResults = filterScores.map((res) => {
  return `The following sentence at timestamp ${formatMilliseconds(res.start)} contained low confidence words: ${res.text} \n  Low confidence word(s) from this sentence: ${res.words
    .map((res) => {
      return `${res.text}[score: ${res.confidence}]`;
    })
    .join(", ")}}`;
});

console.log(finalResults);
```

The output will look something like this:

```
[
  'The following sentence at timestamp 00:04:34 contained low confidence words: I am contacting you first when I could just have phoned my bank and marked you as fraud in an instant. \n' +
    '  Low confidence word(s) from this sentence: marked[score: 0.33049]}',
  'The following sentence at timestamp 00:06:40 contained low confidence words: Sabitha, as much as I would like to help you, this is the best I can do for you. \n' +
    '  Low confidence word(s) from this sentence: Sabitha,[score: 0.22706]}',
  'The following sentence at timestamp 00:07:37 contained low confidence words: Thank you for calling Queston. \n' +
    '  Low confidence word(s) from this sentence: Queston.[score: 0.16557]}'
]
```


---
title: Select The EU Region for EU Data Residency
---

In this guide, we'll show you how to configure your code to utilise the EU endpoint when interacting directly with the API, as well as across our SDKs.

<Warning>
  The EU endpoint is available for **Async STT** and **LeMUR**. **Streaming STT** is not currently supported.
</Warning>

### Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

### Python (Requests)

The only difference here is adding the `eu.` subdomain to the base URL. Everything else in your request code stays the same:

```python
# US endpoint (default)
url = "https://api.assemblyai.com"

# EU endpoint - just change the base URL
url = "https://api.eu.assemblyai.com"
```

### Python SDK

To set the EU endpoint as the base URL when using the Python SDK, add the following line of code before setting the `Transcriber` object:

```python
import assemblyai as aai

aai.settings.base_url = "https://api.eu.assemblyai.com" // Set the base URL to the EU endpoint
```

### Javascript (Axios)

Similar to the requests library for Python, the only difference here is adding the `eu.` subdomain to the base URL as follows:

```python
// US endpoint (default)
const url = 'https://api.assemblyai.com'

// EU endpoint - just change the base URL
const url = 'https://api.eu.assemblyai.com'
```

### Javascript SDK

To set the EU endpoint as the base URL when using the Javascript SDK, set the `baseUrl` parameter in your AssemblyAI client as follows:

```python
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: process.env.ASSEMBLYAI_API_KEY,
  baseUrl: "https://api.eu.assemblyai.com"  // Set the baseUrl to the EU endpoint
});
```

If you encounter any issues or have any questions, reach out to our [Support team.](https://www.assemblyai.com/contact/support)


---
title: Transcribe Multiple Files Simultaneously Using the Node SDK
---

In this guide, we'll show you how to transcribe multiple files simultaneously using the Node SDK.

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can sign up for an account and get your API key from your [dashboard](https://www.assemblyai.com/app/account). This guide will use AssemblyAI's [node SDK](https://github.com/AssemblyAI/assemblyai-node-sdk). If you haven't already, install the SDK in your project by following these [instructions](https://github.com/AssemblyAI/assemblyai-node-sdk#installation).

## Step-by-Step Instructions

Set up your application folder structure by adding an audio folder which will house the files you would like to transcribe, a transcripts folder which will house your completed transcriptions, and a new `.js` file in the root of the project. Your file structure should look like this:

```
BatchApp
├───audio
│   ├───audio-1.mp3
│   └───audio-2.mp3
├───transcripts
├───batch.js
```

In the `batch.js` file import the AssemblyAI package, as well as the node fs and node path packages. Create an AssemblyAI object with your API key:

```
import { AssemblyAI } from "assemblyai";
import * as path from 'path';
import * as fs from 'fs';

const client = new AssemblyAI({
  apiKey: <Your API Key>,
});
```

Declare the variables `audioFolder`, `files`, `filePathArr`, and `transcriptsFolder`.

- `audioFolder` will be the relative path to the folder containing your audio files.
- `files` will read the files in the audio folder, and return them in an array.
- `filePathArr` will join the file names with the audio folder name to create the relative path to each individual file.
- `transcriptsFolder` will be the relative path to the folder containing your transcription files.

```
const audioFolder = './audio';
const files = await fs.promises.readdir(audioFolder);
const filePathArr = files.map(file => path.join(audioFolder, file));
const transcriptsFolder = './transcripts';
```

Next, we'll create a promise that will submit the file path for transcription. Make sure to add the parameters for the models you would like to run.

```
const getTranscript = (filePath) => new Promise((resolve, reject) => {
  client.transcripts.transcribe({
    audio: filePath,
    language_detection: true
  })
  .then(result => resolve(result))
  .catch(error => reject(error));
});
```

Next, we will create an async function that will call the `getTranscript` function and write the transcription text from each audio file to an individual text file in the transcripts folder.

```
const processFile = async (file) => {
  const getFileName = file.split('audio/'); //Separate the folder name and file name into substrings
  const fileName = getFileName[1]; //Grab the 2nd substring which is the file name
  const filePath = path.join(transcriptsFolder, `${fileName}.txt`); //Relative path for transcription text files.

  const transcript = await getTranscript(file); //Request the transcript
  const text = transcript.text; //Grab transcription text from the JSON response

  //Write the transcription text to a text file
  return new Promise((resolve, reject) => {
    fs.writeFile(filePath, text, err => {
      if (err) {
        reject(err);
        return;
      }

      resolve({
        ok: true,
        message: 'Text File created!'
      });
    });
  });
}
```

Next, we will create the run function. This function will:

- Create an array of unresolved promises with each promise requesting a transcript.
- Use `Promise.all` to iterate over the array of unresolved promises.

Then we'll call the run function

```
const run = async () => {
  const unresolvedPromises = filePathArr.map(processFile);
  await Promise.all(unresolvedPromises);
}

run()
```

Your final file will look like this:

```
import { AssemblyAI } from "assemblyai";
import * as path from 'path';
import * as fs from 'fs';

const client = new AssemblyAI({
  apiKey: <Your API>,
});

const audioFolder = './audio';
const files = await fs.promises.readdir(audioFolder);
const filePathArr = files.map(file => path.join(audioFolder, file));
const transcriptsFolder = './transcripts'

const getTranscript = (filePath) => new Promise((resolve, reject) => {
  client.transcripts.transcribe({
    audio: filePath,
    language_detection: true,
  })
  .then(result => resolve(result))
  .catch(error => reject(error));
});

const processFile = async (file) => {
  const getFileName = file.split('audio/')
  const fileName = getFileName[1]
  const filePath = path.join(transcriptsFolder, `${fileName}.txt`);

  const transcript = await getTranscript(file);
  const text = transcript.text

  return new Promise((resolve, reject) => {
    fs.writeFile(filePath, text, err => {
      if (err) {
        reject(err);
        return;
      }

      resolve({
        ok: true,
        message: 'Text File created!'
      });
    });
  });
}

const run = async () => {
  const unresolvedPromises = filePathArr.map(processFile);
  await Promise.all(unresolvedPromises);
}

run()
```

If you have any questions, please feel free to reach out to our Support team - support@assemblyai.com or in our Community Discord!


---
title: Transcribe from an S3 Bucket
---

AssemblyAI's Speech-to-Text APIs can be used with both local files and publicly accessible online files, but what if you want to transcribe an audio file that has restricted access? Luckily, you can do this with AssemblyAI too!

Read on to learn how you can transcribe an audio file stored in an AWS S3 bucket using AssemblyAI's APIs.

## Intro

In order to transcribe an audio file from an S3 bucket, AssemblyAI will need temporary access to the file. To provide this access, we'll generate a **presigned URL**, which is simply a URL that has temporary access rights baked-in.

The overall process looks like this:

1. Generate a presigned URL for the S3 audio file with [boto](http://boto.cloudhackers.com/en/latest).
2. Pass this URL through to AssemblyAI's API with a POST request.
3. Wait until the transcription is complete, and then fetch it with a GET request.

## Prerequisites

First, you'll need an AssemblyAI account. You can sign up [here](https://app.assemblyai.com/signup) for a free account if you don't already have one.

Next, you'll need to **take note of your AssemblyAI API key**, which you can find on your [account dashboard](https://www.assemblyai.com/app/account) after signing in. It will be on the left-hand side of the screen under _Your API Key_.

You'll need the value of this key later, so leave the browser window open or copy the value into a text file.

## AWS IAM User

Second, you'll need an AWS IAM user with `Programmatic` access and the `AmazonS3ReadOnlyAccess` permission. If you already have such an IAM user and you know its public and private keys, then you can move on to the next section. Otherwise, create one now as follows:

First, log into AWS as a root user or as another IAM user with the appropriate access, and then go to the [IAM Management Console](https://us-east-1.console.aws.amazon.com/iamv2/home?ref=assemblyai.com#/users) to add a new user.

<img src="file:1789c50c-45f8-4049-ad19-be4458323f95" alt="image" />

Set the user name you would like, and select _Programmatic access_ under _Select AWS access type:_

<img src="file:09101788-3e70-4ad5-8bc3-8dee0750fa55" alt="image" />

Click _Next_, and then _Attach existing policies directly_. Copy and paste _AmazonS3ReadOnlyAccess_ into the _Filter policies_ search box, and then add this permission by clicking on the checkbox next to it:

<img src="file:1616d846-8a89-494b-85e0-185b43283457" alt="image" />

Click _Next_ and add tags if you wish. Then click _Next_ and review the IAM user profile to ensure that everything looks copacetic before clicking _Create user_.

<img src="file:f4aa39ce-c05e-44a4-a7d7-54b89e23ce0f" alt="image" />

Finally, take note of the IAM user's _Access key ID_ and _Secret access key_. Again, **we will need these values later**, so copy them into a text file before moving on.

**Warning**
Make sure to copy the IAM user's _Secret access key_ and record it somewhere safe. Once you close the final window of the _Add user_ sequence, you will not be able to access this key again and will need to regenerate it if you forget/lose the original.

## Code

First, the necessary packages are installed.

```bash
pip install -U boto3 botocore
```

Then we can import them and set our relevant variable values. You'll need to edit these variables to be equivalent to the relevant values for your application:

1. `bucket_name` - The name of your AWS S3 bucket.
2. `object_name` - The name of the audio file in the S3 bucket that you want to transcribe.
3. `iam_access_id` - The access ID of the IAM user with programmatic access and S3 read permission.
4. `iam_secret_key` - The secret key of the IAM user.
5. `assembly_key` - Your AssemblyAI API key.

```python
import boto3
from botocore.exceptions import ClientError
import logging
import requests
import time

bucket_name = "<BUCKET_NAME>"
object_name = "<AUDIO_FILE_NAME>"

iam_access_id = "<IAM_ACCESS_ID>"
iam_secret_key = "<IAM_SECRET_KEY>"

assembly_key = "<ASSEMBLYAI_API_KEY>"
```

From here, we simply follow the sequence outlined in the introduction of this Colab:

1. Generate a presigned URL for the S3 audio file with [boto](http://boto.cloudhackers.com/en/latest).

```python
# Create a low-level service client with the IAM credentials.
s3_client = boto3.client(
    "s3", aws_access_key_id=iam_access_id, aws_secret_access_key=iam_secret_key
)

# Generate a pre-signed URL for the audio file that expires after 30 minutes.
try:
    p_url = s3_client.generate_presigned_url(
        ClientMethod="get_object",
        Params={"Bucket": bucket_name, "Key": object_name},
        ExpiresIn=1800,
    )

except ClientError as e:
    logging.error(e)
```

2. Pass the presigned URL through to AssemblyAI's API with a POST request.

```python
# Use your AssemblyAI API Key for authorization.
headers = {"authorization": assembly_key, "content-type": "application/json"}

# Specify AssemblyAI's transcription API endpoint.
transcript_endpoint = "https://api.assemblyai.com/v2/transcript"

# Use the presigned URL as the `audio_url` in the POST request.
json = {"audio_url": p_url}

# Queue the audio file for transcription with a POST request.
post_response = requests.post(transcript_endpoint, json=json, headers=headers)
```

3. Wait until the transcription is complete, and then fetch it with a GET request.

```python
# Specify the endpoint of the transaction.
get_endpoint = transcript_endpoint + "/" + post_response.json()["id"]

# GET request the transcription.
get_response = requests.get(get_endpoint, headers=headers)

# If the transcription has not finished, wait util it has.
while get_response.json()["status"] != "completed":
    get_response = requests.get(get_endpoint, headers=headers)
    time.sleep(5)

# Once the transcription is complete, print it out.
print(get_response.json()["text"])
```


---
title: Transcribe Google Drive Files
---

### **Step 1: Upload Your Audio File to Google Drive**

- **File Requirements**: Ensure your audio file is smaller than 100MB, as files larger than this cannot be directly downloaded from Google Drive links.
- **Uploading**: Log into your Google Drive account and upload the audio file you want to use.

### **Step 2: Make Your File Publicly Accessible**

- **Right-Click** on the uploaded file in Google Drive.
- Select **'Get Link'**.
- Change the setting from “Restricted” to “Anyone with the link”. This makes the file publicly accessible.

### **Step 3: Obtain the Downloadable URL**

- Click on `Copy link` to copy your shared link.
- Initially, the shared link will look something like this: https://drive.google.com/file/d/1YvY3gX-4ZwY7K4r3J0THKNTvvolB3D-S/view?usp=sharing.
- To make it a downloadable link, modify it to this format:  
   `https://drive.google.com/u/0/uc?id=FILE_ID&export=download`.
- **Example**: If your shared link is `https://drive.google.com/file/d/1YvY3gX-4ZwY7K4r3J0THKNTvvolB3D-S/view?usp=sharing`,  
   change it to `https://drive.google.com/u/0/uc?id=1YvY3gX-4ZwY7K4r3J0THKNTvvolB3D-S&export=download`.

<img src="file:e431991d-943c-4408-8c1f-be7f318a94d2" alt="Google Drive screenshot" />

### **Step 4: Use the URL with AssemblyAI**

- Now, you can use this downloadable link in your AssemblyAI API request. This URL directly points to your audio file, allowing AssemblyAI to access and process it.

```python
transcriber = aai.Transcriber()

audio_url = (
"https://storage.googleapis.com/aai-web-samples/5_common_sports_injuries.mp3"
)

transcript = transcriber.transcribe(audio_url)
```

### **Notes**

- **Security**: Ensure that sharing your audio file publicly complies with your privacy and security policies.
- If you prefer not to share your file publicly, you can [upload your file to our servers instead.](https://www.assemblyai.com/docs/guides/transcribing-an-audio-file#step-by-step-instructions)
- **File Format**: Check that your audio file is in a format supported by AssemblyAI.


---
title: Transcribe GitHub Files
---

## Step 1: Upload Your Audio Files to a Public GitHub Repository

- **File Requirements**: GitHub has a file size limit of 100MB so ensure your audio files are 100MB in size or less. The files must be in a public repository otherwise you will receive an error saying the file is not publicly accessible. For a more secure way to host files check out our [Transcribing from an S3 Bucket Cookbook](transcribe_from_s3.ipynb).

## Step 2: Obtain the Raw Audio URL from GitHub

1. Navigate to the repository that houses the audio file.
2. Click on the audio file. On the next page, right-click the "View raw" link and select "copy the link address" from the context menu.

{/* <img width="649" alt="An image of an audio file in a GitHub repository" src="../guide-images/view-raw.png"> */}

Downloadable file URLs are formatted as `"https://github.com/<github-username>/<repo-name>/raw/<branch-name>/<file-name-and extension>"`

## Step 3: Add the Audio URL to your Request

```
POST v2/transcript endpoint

{
    "audio_url":"https://github.com/user/audio-files/raw/main/audio.mp3"
}
```

```
Python SDK

transcript = transcriber.transcribe("https://github.com/user/audio-files/raw/main/audio.mp3")
```

```
Typescript SDK

const transcript = await client.transcripts.transcribe("https://github.com/user/audio-files/raw/main/audio.mp3")
```

## **Resources**

[AssemblyAI's Supported File Types](https://www.assemblyai.com/docs/concepts/faq) <br/>
[Transcribe an Audio File](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file)


---
title: Identifying speakers in audio recordings
hide-nav-links: true
description: Add speaker labels to your transcript
---

When applying the [Speaker Diarization model](/docs/speech-to-text/speaker-diarization), the transcription not only contains the text but also includes speaker labels, enhancing the overall structure and organization of the output.

In this step-by-step guide, you'll learn how to apply the model. In short, you have to send the `speaker_labels` parameter in your request, and then find the results inside a field called `utterances`.

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

The complete source code for this guide can be viewed [here](https://github.com/AssemblyAI-Community/docs-snippets/tree/main/speakers).

Here is an audio example for this guide:

```bash
https://assembly.ai/wildfires.mp3
```

## Step-by-step instructions

<Steps>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Install the SDK.

  </Tab>
  <Tab fallback>

Create a new file and
request.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
pip install -U assemblyai
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python



```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript

```

  </Tab>

  <Tab language="php" title="PHP">

```php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'
require 'rest-client'
require 'httparty'
```

  </Tab>

<Tab language="csharp" title="C#">

```csharp
using System.Net.Http;
using System.Threading;
```

</Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Import the `assemblyai` package and set the API key.

  </Tab>
  <Tab fallback>

Set up the API endpoint and headers. The headers should include your API
key.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
base_url = "https://api.assemblyai.com/v2"

headers = {
    "authorization": "<YOUR_API_KEY>"
}
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const baseUrl = "https://api.assemblyai.com/v2";

const headers = {
  authorization: "<YOUR_API_KEY>",
};
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$base_url = "https://api.assemblyai.com/v2";

$headers = array(
  "authorization: <YOUR_API_KEY>",
  "content-type: application/json"
);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
base_url = "https://api.assemblyai.com/v2"

headers = {
    "authorization" => "<YOUR_API_KEY>",
    "content-type" => "application/json"
}
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
string apiKey = "<YOUR_API_KEY>";
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Create a `TranscriptionConfig` with `speaker_labels` set to `True`.

  </Tab>
  <Tab fallback>

Upload your local file to the AssemblyAI API.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
# highlight-next-line
config = aai.TranscriptionConfig(speaker_labels=True)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
with open("./my-audio.mp3", "rb") as f:
  response = requests.post(base_url + "/upload",
                          headers=headers,
                          data=f)

upload_url = response.json()["upload_url"]
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$path = "/my_audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
path = "/my_audio.mp3"
response = RestClient.post("#{base_url}/upload", File.read(path), headers)
upload_url = JSON.parse(response.body)["upload_url"]
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
public async Task<string> UploadFileAsync(string apiKey, string path)
{
    using var client = new HttpClient();
    client.DefaultRequestHeaders.Authorization = new System.Net.Http.Headers.AuthenticationHeaderValue(apiKey);

    using var fileContent = new ByteArrayContent(File.ReadAllBytes(path));
    fileContent.Headers.ContentType = new System.Net.Http.Headers.MediaTypeHeaderValue("application/octet-stream");

    HttpResponseMessage response;
    try
    {
        response = await client.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
    }
    catch (Exception e)
    {
        Console.Error.WriteLine($"Error: {e.Message}");
        return null;
    }

    if (response.IsSuccessStatusCode)
    {
        string responseBody = await response.Content.ReadAsStringAsync();
        var json = JObject.Parse(responseBody);
        return json["upload_url"].ToString();
    }
    else
    {
        Console.Error.WriteLine($"Error: {response.StatusCode} - {response.ReasonPhrase}");
        return null;
    }
}
```

  </Tab>

</Tabs>

</Step>
<Step>

<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Create a `Transcriber` object and pass in the configuration.

  </Tab>

  <Tab fallback>

Use the `upload_url` returned by the AssemblyAI API to create a JSON payload
containing the `audio_url` parameter and the `speaker_labels` paramter set to
`True`.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
transcriber = aai.Transcriber(config=config)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
data = {
    "audio_url": upload_url,
    "speaker_labels": True
}
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const data = {
  audio_url: uploadUrl,
  speaker_labels: true,
};
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$data = array(
    "audio_url" => upload_url
    "speaker_labels" => True
);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
data = {
    "audio_url" => upload_url
    "speaker_labels" => true
}
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
var data = new Dictionary<string, dynamic>(){
    { "audio_url", upload_url },
    { "speaker_labels", true }
};
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Use the `Transcriber` object's transcribe method and pass in the audio file's
path as a parameter. The `transcribe` method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

  </Tab>
  <Tab fallback>

Make a `POST` request to the AssemblyAI API endpoint with the payload and
headers.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
FILE_URL = "https://assembly.ai/wildfires.mp3"

transcript = transcriber.transcribe(FILE_URL)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
url = base_url + "/transcript"
response = requests.post(url, json=data, headers=headers)
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const url = `${baseUrl}/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$url = $base_url . "/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

curl_close($curl);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
uri = URI.parse("#{base_url}/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
using (var client = new HttpClient())
{
    client.DefaultRequestHeaders.Add("authorization", apiKey);
    var content = new StringContent(JsonConvert.SerializeObject(data), Encoding.UTF8, "application/json");
    HttpResponseMessage response = await client.PostAsync("https://api.assemblyai.com/v2/transcript", content);
    var responseContent = await response.Content.ReadAsStringAsync();
    var responseJson = JsonConvert.DeserializeObject<dynamic>(responseContent);
}
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

You can access the speaker label results through the transcription object's `utterances` attribute.

  </Tab>
  <Tab fallback>

After making the request, you'll receive an ID for the transcription. Use it
to poll the API every few seconds to check the status of the transcript job.
Once the status is `completed`, you can retrieve the transcript from the API
response, using the `utterances` key to access the results.

  </Tab>
</Tabs>
<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
# extract all utterances from the response
utterances = transcript.utterances

# For each utterance, print its speaker and what was said
for utterance in utterances:
  speaker = utterance.speaker
  text = utterance.text
  print(f"Speaker {speaker}: {text}")
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
transcript_id = response.json()['id']
polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

while True:
  transcription_result = requests.get(polling_endpoint, headers=headers).json()

  if transcription_result['status'] == 'completed':
    # when the transcript is complete, extract all utterances from the response
    transcript_text = transcription_result['text']
    utterances = transcription_result['utterances']

    # For each utterance, print its speaker and what was said
    for utterance in utterances:
        speaker = utterance['speaker']
        text = utterance['text']
        print(f"Speaker {speaker}: {text}")

    break

  elif transcription_result['status'] == 'error':
    raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

  else:
    time.sleep(3)
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    const utterances = transcriptionResult.utterances;

    // Iterate through each utterance and print the speaker and the text they spoke
    for (const utterance of utterances) {
      const speaker = utterance.speaker;
      const text = utterance.text;
      console.log(`Speaker ${speaker}: ${text}`);
    }

    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$transcript_id = $response['id'];
$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        $utterances = $transcription_result['utterances'];

        // Iterate through each utterance and print the speaker and the text they spoke
        foreach ($utterances as $utterance) {
            $speaker = $utterance['speaker'];
            $text = $utterance['text'];
            echo "Speaker $speaker: $text\n";
        }

        break;
    } else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
transcript_id = response.parsed_response["id"]
polling_endpoint = "https://api.assemblyai.com/v2/transcript/#{transcript_id}"

while true
    polling_response = HTTParty.get(polling_endpoint, headers: headers)
    transcription_result = polling_response.parsed_response

    if transcription_result["status"] == "completed"
        utterances = transcription_result["utterances"]

        # Iterate through each utterance and print the speaker and the text they spoke
        utterances.each do |utterance|
            speaker = utterance["speaker"]
            text = utterance["text"]
            puts "Speaker #{speaker}: #{text}"
        end

        break
    elsif transcription_result["status"] == "error"
        raise "Transcription failed: #{transcription_result["error"]}"
    else
        sleep(3)
    end
end
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
using (var client = new HttpClient())
{
    ...

    var responseJson = JsonConvert.DeserializeObject<dynamic>(responseContent);

    string transcriptId = responseJson.id;
    string pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcriptId}";

    while (true)
    {
        var pollingResponse = await client.GetAsync(pollingEndpoint);
        var pollingResponseContent = await pollingResponse.Content.ReadAsStringAsync();
        var pollingResponseJson = JsonConvert.DeserializeObject<dynamic>(pollingResponseContent);

        if (pollingResponseJson.status == "completed")
        {
            JArray utterances = (JArray)pollingResponseJson["utterances"];

            // Iterate through each utterance and print the speaker and the text they spoke
            foreach (JObject utterance in utterances) {
                string speaker = utterance["speaker"].ToString();
                string text = utterance["text"].ToString();
                Console.WriteLine($"Speaker {speaker}: {text}");
            }

            return pollingResponseJson;
        }
        else if (pollingResponseJson.status == "error")
        {
            throw new Exception($"Transcription failed: {pollingResponseJson.error}");
        }
        else
        {
            Thread.Sleep(3000);
        }
    }
}
```

  </Tab>

</Tabs>

</Step>
</Steps>

## Understanding the response

The speaker label information is included in the `utterances` key of the response. Each utterance object in the list includes a `speaker` field, which contains a string identifier for the speaker (e.g., "A", "B", etc.). The utterances list also contains a `text` field for each utterance containing the spoken text, and `confidence` scores both for utterances and their individual words.

<CodeBlock>
  <JsonViewer
    displayDataTypes={false}
    quotesOnKeys={false}
    displayObjectSize={false}
    collapsed={3}
    src={{
      utterances: [
        {
          confidence: 0.7246133333333334,
          end: 3738,
          speaker: "A",
          start: 570,
          text: "Um hey, Erica.",
          words: [
            {
              text: "Um",
              start: 570,
              end: 1120,
              confidence: 0.42915,
              speaker: "A",
            },
            {
              text: "hey,",
              start: 2690,
              end: 3054,
              confidence: 0.98465,
              speaker: "A",
            },
            {
              text: "Erica.",
              start: 3092,
              end: 3738,
              confidence: 0.76004,
              speaker: "A",
            },
          ],
        },
        {
          confidence: 0.6015349999999999,
          end: 4430,
          speaker: "B",
          start: 3834,
          text: "One in.",
          words: [
            {
              text: "One",
              start: 3834,
              end: 4094,
              confidence: 0.25,
              speaker: "B",
            },
            {
              text: "in.",
              start: 4132,
              end: 4430,
              confidence: 0.95307,
              speaker: "B",
            },
          ],
        },
      ],
    }}
  />
</CodeBlock>

For more information, see the [Speaker Diarization model documentation](/docs/speech-to-text/speaker-diarization#specifying-the-number-of-speakers) or see the [API reference](https://assemblyai.com/docs/api-reference/transcripts).

## Specifying the number of speakers

You can provide the optional parameter `speakers_expected`, that can be used to specify the expected number of speakers in an audio file.

<Button
  variant="text"
  color="yellow"
  theme="dark"
  endIcon="chevron"
  link={{
    href: "/speech-to-text/speaker-diarization#specifying-the-number-of-speakers",
  }}
>
  API/Model Reference
</Button>

## Conclusion

Automatically identifying different speakers from an audio recording, also called **speaker diarization**, is a multi-step process. It can unlock additional value from many genres of recording, including conference call transcripts, broadcast media, podcasts, and more. You can learn more about use cases for speaker diarization and the underlying research from the [AssemblyAI blog](https://www.assemblyai.com/blog/speaker-diarization-speaker-labels-for-mono-channel-files).


---
title: Calculate the Talk / Listen Ratio of Speakers
---

This guide will show you how to use AssemblyAI's API to calculate the talk/listen ratio of speakers in a transcript. The following code uses the [Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk).

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"

def calculate_talk_listen_ratios(transcript):
    """
    :param transcript: AssemblyAI Transcript object
    :return: Dictionary with talk time, percentage, and talk-listen ratios for each speaker
    """
    # Ensure speaker labels were enabled
    if not transcript.utterances:
        raise ValueError("Speaker labels were not enabled for this transcript.")

    speaker_talk_time = {}
    total_time = 0

    for utterance in transcript.utterances:
        speaker = f"Speaker {utterance.speaker}"
        duration = utterance.end - utterance.start

        speaker_talk_time[speaker] = speaker_talk_time.get(speaker, 0) + duration
        total_time += duration

    # Calculate percentages and ratios
    result = {}
    for speaker, talk_time in speaker_talk_time.items():
        percentage = (talk_time / total_time) * 100
        result[speaker] = {
            "talk_time_ms": talk_time,
            "percentage": round(percentage, 2)
        }

    # Calculate talk-listen ratios for each speaker against all others
    for speaker in result.keys():
        other_speakers_time = sum(talk_time for spk, talk_time in speaker_talk_time.items() if spk != speaker)
        if other_speakers_time > 0:
            ratio = speaker_talk_time[speaker] / other_speakers_time
            result[speaker]["talk_listen_ratio"] = round(ratio, 2)
        else:
            result[speaker]["talk_listen_ratio"] = None  # Handle cases with only one speaker

    return result

transcriber = aai.Transcriber()
audio_url = ("YOUR_AUDIO_URL")
config = aai.TranscriptionConfig(speaker_labels=True)
transcript = transcriber.transcribe(audio_url, config)

talk_listen_stats = calculate_talk_listen_ratios(transcript)
print(talk_listen_stats)

```

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## Step-by-Step Instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` package and set the API key.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
```

Define a function called `calculate_talk_listen_ratios`, which will calculate the talk-listen ratios for all speakers from a transcript with speaker labels. Speaker labels must be enabled for the ratios to be calculated.

```python
def calculate_talk_listen_ratios(transcript):
    """
    :param transcript: AssemblyAI Transcript object
    :return: Dictionary with talk time, percentage, and talk-listen ratios for each speaker
    """
    # Ensure speaker labels were enabled
    if not transcript.utterances:
        raise ValueError("Speaker labels were not enabled for this transcript.")

    speaker_talk_time = {}
    total_time = 0

    for utterance in transcript.utterances:
        speaker = f"Speaker {utterance.speaker}"
        duration = utterance.end - utterance.start

        speaker_talk_time[speaker] = speaker_talk_time.get(speaker, 0) + duration
        total_time += duration

    # Calculate percentages and ratios
    result = {}
    for speaker, talk_time in speaker_talk_time.items():
        percentage = (talk_time / total_time) * 100
        result[speaker] = {
            "talk_time_ms": talk_time,
            "percentage": round(percentage, 2)
        }

    # Calculate talk-listen ratios for each speaker against all others
    for speaker in result.keys():
        other_speakers_time = sum(talk_time for spk, talk_time in speaker_talk_time.items() if spk != speaker)
        if other_speakers_time > 0:
            ratio = speaker_talk_time[speaker] / other_speakers_time
            result[speaker]["talk_listen_ratio"] = round(ratio, 2)
        else:
            result[speaker]["talk_listen_ratio"] = None  # Handle cases with only one speaker

    return result
```

Define a `transcriber`, an `audio_url` set to a link to the audio file (replace the example link that is provided with your own), and a `TranscriptionConfig` with `speaker_labels=True`. Then create a transcript which will be sent to the function `calculate_talk_listen_ratios` and print out the results.

```python
transcriber = aai.Transcriber()
audio_url = ("https://api.assemblyai-solutions.com/storage/v1/object/public/dual-channel-phone-data/Fisher_Call_Centre/audio05851.wav")
config = aai.TranscriptionConfig(speaker_labels=True)
transcript = transcriber.transcribe(audio_url, config)

talk_listen_stats = calculate_talk_listen_ratios(transcript)
print(talk_listen_stats)
```

Example output when using the above sample audio file:

```
{'Speaker A': {'talk_time_ms': 244196, 'percentage': 42.77, 'talk_listen_ratio': 0.75}, 'Speaker B': {'talk_time_ms': 326766, 'percentage': 57.23, 'talk_listen_ratio': 1.34}}
```


---
title: Plot A Speaker Timeline with Matplotlib
---

In this guide, we'll show you how to plot a speaker timeline with matplotlib, using results from the speaker diarization model.

## Quickstart

```python
import assemblyai as aai
import matplotlib.pyplot as plt

aai.settings.api_key = "YOUR_API_KEY"

config = aai.TranscriptionConfig(speaker_labels=True)
transcriber = aai.Transcriber()
transcript = transcriber.transcribe("./my-audio.mp3", config)
utterances = transcript.utterances

def plot_speaker_timeline(utterances):
    fig, ax = plt.subplots(figsize=(12, 4))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
    speaker_colors = {}

    for utterance in utterances:
        start = utterance.start / 60000 # in minutes
        end = utterance.end / 60000 # in minutes
        speaker = utterance.speaker

        if speaker not in speaker_colors:
            speaker_colors[speaker] = colors[len(speaker_colors) % len(colors)] # set a colour for each new speaker

        ax.barh(speaker, end - start, left=start, color=speaker_colors[speaker], height=0.4) # create horizontal bar plot

    ax.set_xlabel('Time (mins)')
    ax.set_ylabel('Speakers')
    ax.set_title('Speaker Timeline')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.show()

plot_speaker_timeline(utterances)
```

### Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

### Step-by-Step Instructions

Install the SDK.

```bash
pip install -U assemblyai
!pip install -U matplotlib
```

Import the `assemblyai` package and set the API key.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
```

Create a `TranscriptionConfig` object and set speaker labels to `True`.

```python
config = aai.TranscriptionConfig(speaker_labels=True)
```

Create a `Transcriber` object.

```python
transcriber = aai.Transcriber()
```

Use the Transcriber object's `transcribe` method and pass in the audio file's path and `config` object as parameters. The transcribe method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcript = transcriber.transcribe("./my-audio.mp3", config)
```

<Tip> Alternatively, you can use an audio URL available on the internet. </Tip>

Extract the utterances from the transcript and set this to `utterances`.

```python
utterances = transcript.utterances
```

Import the `matplotlib.pyplot` library. Then use the following `plot_speaker_timeline` function which results in a plot image of the speaker timeline. This function extracts the `start` and `end` timestamps of each `utterance` per `speaker` and plots the data onto the horizontal bar chart. The X and Y axis are labelled accordingly.

```python
import matplotlib.pyplot as plt

def plot_speaker_timeline(utterances):
    fig, ax = plt.subplots(figsize=(12, 4))
    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
    speaker_colors = {}

    for utterance in utterances:
        start = utterance.start / 60000 # in minutes
        end = utterance.end / 60000 # in minutes
        speaker = utterance.speaker

        if speaker not in speaker_colors:
            speaker_colors[speaker] = colors[len(speaker_colors) % len(colors)] # set a colour for each new speaker

        ax.barh(speaker, end - start, left=start, color=speaker_colors[speaker], height=0.4) # create horizontal bar plot

    ax.set_xlabel('Time (mins)')
    ax.set_ylabel('Speakers')
    ax.set_title('Speaker Timeline')
    ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.show()
```

Finally, call the `plot_speaker_timeline` function passing `utterances` as a parameter to see the plot image result.

```python
plot_speaker_timeline(utterances)
```


---
title: Generate Custom Speaker Labels with Pyannote
---

In this guide, we'll show you how to generate Speaker Labels using Pyannote with an AssemblyAI transcript. This can be used to generate Speaker Labels for languages we currently do not support for speaker labelling.

## Quickstart

```python
import os
import assemblyai as aai
from pyannote.audio import Pipeline
import torch
import pandas as pd
import numpy as np

# Assign your API keys
HUGGING_FACE_TOKEN = os.getenv("HF_TOKEN")
ASSEMBLYAI_API_KEY = os.getenv("ASSEMBLYAI_API_KEY")

# Authenticate with AssemblyAI
aai.settings.api_key = ASSEMBLYAI_API_KEY

def transcribe_audio(audio_file, language="en"):
    """
    Transcribe an audio file using AssemblyAI.

    Args:
        audio_file (str): Path to the audio file.
        language (str, optional): Language code for transcription. Defaults to "en".

    Returns:
        aai.Transcript: The transcription result.
    """

    transcriber = aai.Transcriber(config=aai.TranscriptionConfig(speech_model='nano', language_code=language))
    transcript = transcriber.transcribe(audio_file)
    print(f"Transcript ID: {transcript.id}")
    return transcript

def get_speaker_labels(audio_file, transcript: aai.Transcript):
    """
    Perform speaker diarization on an audio file and combine results with the transcript.

    Args:
        audio_file (str): Path to the audio file.
        transcript (aai.Transcript): The transcription result from AssemblyAI.

    Returns:
        str: A formatted string containing the transcript with speaker labels and timestamps.
    """
    # Initialize the speaker diarization pipeline with GPU support
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization",
        use_auth_token=HUGGING_FACE_TOKEN,
    )

    if pipeline is None:
        raise ValueError("Failed to initialize the pipeline. Please check your authentication token and internet connection.")
    else:
        pipeline = pipeline.to(device)

    # Apply the pipeline to the audio file
    diarization = pipeline(audio_file)

    # Create a dictionary to store speaker segments
    speaker_segments = {}

    # Process diarization results
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        start, end = turn.start, turn.end
        if speaker not in speaker_segments:
            speaker_segments[speaker] = []
        speaker_segments[speaker].append((start, end))

    # Convert speaker_segments to a DataFrame
    diarize_df = pd.DataFrame([(speaker, start, end)
                               for speaker, segments in speaker_segments.items()
                               for start, end in segments],
                              columns=['speaker', 'start', 'end'])

    # Assign speakers to transcript words
    for word in transcript.words:
        word_start = float(word.start) / 1000
        word_end = float(word.end) / 1000

        overlaps = diarize_df[
            (diarize_df['start'] <= word_end) & (diarize_df['end'] >= word_start)
        ].copy()

        if not overlaps.empty:
            overlaps['overlap'] = np.minimum(overlaps['end'], word_end) - np.maximum(overlaps['start'], word_start)
            word.speaker = overlaps.loc[overlaps['overlap'].idxmax(), 'speaker']
        else:
            word.speaker = "Unknown"

    full_transcript = ''

    # Update segment speakers based on the majority speaker of its words
    for segment in transcript.get_sentences():
        segment_start = float(segment.start) / 1000
        segment_end = float(segment.end) / 1000

        overlaps = diarize_df[
            (diarize_df['start'] <= segment_end) & (diarize_df['end'] >= segment_start)
        ].copy()

        if not overlaps.empty:
            overlaps['overlap'] = np.minimum(overlaps['end'], segment_end) - np.maximum(overlaps['start'], segment_start)
            segment.speaker = overlaps.loc[overlaps['overlap'].idxmax(), 'speaker']
            speaker_label = segment.speaker.replace('SPEAKER_', 'SPEAKER ')
            full_transcript += f'[{format_timestamp(segment_start)}] {speaker_label}: {segment.text}\n'
        else:
            segment.speaker = "Unknown"
            full_transcript += f'[{format_timestamp(segment_start)}] Unknown: {segment.text}\n'

    return full_transcript

def format_timestamp(seconds):
    """
    Convert seconds to a formatted timestamp string (HH:MM:SS).

    Args:
        seconds (float): Time in seconds.

    Returns:
        str: Formatted timestamp string.
    """
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

audio_file = "audio.wav" # your local file path
transcript: aai.Transcript = transcribe_audio(audio_file, language="hr") # select a language code
transcript_with_speakers = get_speaker_labels(audio_file, transcript)
print(transcript_with_speakers)
```

### Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

You'll also need a HuggingFace account and API key. You can [sign up](https://huggingface.co/join) for a free account and get your API key [here](https://huggingface.co/settings/tokens). Create a **Read** type API token to ensure the necessary permissions are enabled.

<Info>
  Browse to the
  [speaker-diarization](https://huggingface.co/pyannote/speaker-diarization) and
  [segmentation](https://huggingface.co/pyannote/segmentation) model pages and
  accept the **Gated Model** Terms & Conditions by entering your
  **Company/University**, **Website** and **Use Case** details in order to gain
  access to the use of these models.
</Info>

### Step-by-Step Instructions

Install the necessary dependencies.

```bash
pip install assemblyai pyannote.audio torch pandas numpy
```

Import the necessary dependencies, assign your API keys and authenticate with AssemblyAI.

```python
import os
import assemblyai as aai
from pyannote.audio import Pipeline
import torch
import pandas as pd
import numpy as np

# Assign your API keys
HUGGING_FACE_TOKEN = os.getenv("HF_TOKEN")
ASSEMBLYAI_API_KEY = os.getenv("ASSEMBLYAI_API_KEY")

# Authenticate with AssemblyAI
aai.settings.api_key = ASSEMBLYAI_API_KEY
```

Create the `transcribe_audio` function, this will handle the transcription process with AssemblyAI.

```python
def transcribe_audio(audio_file, language="en"):
    """
    Transcribe an audio file using AssemblyAI.

    Args:
        audio_file (str): Path to the audio file.
        language (str, optional): Language code for transcription. Defaults to "en".

    Returns:
        aai.Transcript: The transcription result.
    """

    transcriber = aai.Transcriber(config=aai.TranscriptionConfig(speech_model='nano', language_code=language))
    transcript = transcriber.transcribe(audio_file)
    print(f"Transcript ID: {transcript.id}")
    return transcript
```

Create the `get_speaker_labels`function, this will handle the speaker diarization model processing to generate the custom speaker labels for the transcript.

Firstly, it initializes and applies the pipeline to the audio file.

Secondly, it processes the diarization results and converts the speaker segments into a DataFrame so we can compare the results with the transcript.

Lastly, the speaker segments are compared and assigned to the words and sentences of the transcript to create the speaker labelled transcript.

```python
def get_speaker_labels(audio_file, transcript: aai.Transcript):
    """
    Perform speaker diarization on an audio file and combine results with the transcript.

    Args:
        audio_file (str): Path to the audio file.
        transcript (aai.Transcript): The transcription result from AssemblyAI.

    Returns:
        str: A formatted string containing the transcript with speaker labels and timestamps.
    """
    # Initialize the speaker diarization pipeline with GPU support
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization",
        use_auth_token=HUGGING_FACE_TOKEN,
    )

    if pipeline is None:
        raise ValueError("Failed to initialize the pipeline. Please check your authentication token and internet connection.")
    else:
        pipeline = pipeline.to(device)

    # Apply the pipeline to the audio file
    diarization = pipeline(audio_file)

    # Create a dictionary to store speaker segments
    speaker_segments = {}

    # Process diarization results
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        start, end = turn.start, turn.end
        if speaker not in speaker_segments:
            speaker_segments[speaker] = []
        speaker_segments[speaker].append((start, end))

    # Convert speaker_segments to a DataFrame
    diarize_df = pd.DataFrame([(speaker, start, end)
                               for speaker, segments in speaker_segments.items()
                               for start, end in segments],
                              columns=['speaker', 'start', 'end'])

    # Assign speakers to transcript words
    for word in transcript.words:
        word_start = float(word.start) / 1000
        word_end = float(word.end) / 1000

        overlaps = diarize_df[
            (diarize_df['start'] <= word_end) & (diarize_df['end'] >= word_start)
        ].copy()

        if not overlaps.empty:
            overlaps['overlap'] = np.minimum(overlaps['end'], word_end) - np.maximum(overlaps['start'], word_start)
            word.speaker = overlaps.loc[overlaps['overlap'].idxmax(), 'speaker']
        else:
            word.speaker = "Unknown"

    full_transcript = ''

    # Update segment speakers based on the majority speaker of its words
    for segment in transcript.get_sentences():
        segment_start = float(segment.start) / 1000
        segment_end = float(segment.end) / 1000

        overlaps = diarize_df[
            (diarize_df['start'] <= segment_end) & (diarize_df['end'] >= segment_start)
        ].copy()

        if not overlaps.empty:
            overlaps['overlap'] = np.minimum(overlaps['end'], segment_end) - np.maximum(overlaps['start'], segment_start)
            segment.speaker = overlaps.loc[overlaps['overlap'].idxmax(), 'speaker']
            speaker_label = segment.speaker.replace('SPEAKER_', 'SPEAKER ')
            full_transcript += f'[{format_timestamp(segment_start)}] {speaker_label}: {segment.text}\n'
        else:
            segment.speaker = "Unknown"
            full_transcript += f'[{format_timestamp(segment_start)}] Unknown: {segment.text}\n'

    return full_transcript
```

<Accordion title="How can I set the number of speakers?">

If you know the number of speakers in advance, you can use the `num_speakers` parameter to set the number of speakers:

```python
# Apply the pipeline to the audio file
diarization = pipeline(audio_file, num_speakers=4)
```

You can also provide upper/lower bands on the number of speakers using the `min_speakers` and `max_speakers` parameters:

```python
# Apply the pipeline to the audio file
diarization = pipeline(audio_file, min_speakers=2, max_speakers=5)
```

</Accordion>

Create the `format_timestamp`, this will handle the timestamps conversion to improve the readability of the final speaker labelled transcript.

```python
def format_timestamp(seconds):
    """
    Convert seconds to a formatted timestamp string (HH:MM:SS).

    Args:
        seconds (float): Time in seconds.

    Returns:
        str: Formatted timestamp string.
    """
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
```

Finally, select a local file and call the functions to generate and print your custom Speaker Labelled transcript.

```python
audio_file = "audio.wav" # your local file path
transcript: aai.Transcript = transcribe_audio(audio_file, language="hr") # select a language code
transcript_with_speakers = get_speaker_labels(audio_file, transcript)
print(transcript_with_speakers)
```

Here's an example speaker labelled output from a Croatian file:

```
[00:00:05] SPEAKER 04: Nalazimo se u Centro Zagreba, u parku Zrinjevac, gdje je kao što vidite jako ljepo, vreme je prekrasno, a danas ćemo ljude pitati što im se sviđa u Zagrebu ili što im se možda ne sviđa u Zagrebu.

[00:00:42] SPEAKER 04: Dobar dan, može jednokratko pitanje samo.

[00:00:46] SPEAKER 04: Može?

[00:00:48] SPEAKER 04: Evo lako, što vam se najviše sviđa u Zagrebu?

[00:00:50] SPEAKER 07: Što mi se najviše sviđa u Zagrebu?

[00:00:53] SPEAKER 07: E sad, teško pitanje, ali trenutno mi se najviše sviđa što nije klasična jesen, nego više prođeče u zraku.

[00:01:06] SPEAKER 07: Dobre.

[00:01:09] SPEAKER 07: Može sigurnost još uvijek s osišam sigurno u Zagrebu.

[00:01:13] SPEAKER 04: I po noći?

[00:01:15] SPEAKER 07: Pa po noći ne šetam baš toliko po noći, ali centar grada mi je dosta siguran, osvijetljen i to mi je okej.
```


---
title: Use Speaker Diarization with Async Chunking
---

This guide uses AssemblyAI and [Nvidia's NeMo](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/titanet_large) framework. We'll be using TitaNet, a state of the art open source model that is trained for speaker recognition tasks. TitaNet will allow us to generate audio embeddings for speakers, which can be used to identify semantic similarity matches between two speakers.

## Quickstart

```python
import assemblyai as aai
import requests
import json
import time
import requests
import copy
from pydub import AudioSegment
import os
import nemo.collections.asr as nemo_asr
from pydub import AudioSegment

speaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained("nvidia/speakerverification_en_titanet_large")

assemblyai_key = "YOUR_API_KEY"

headers = {
    "authorization": assemblyai_key
}

def get_transcript(transcript_id):
    polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

    while True:
        transcription_result = requests.get(polling_endpoint, headers=headers).json()

        if transcription_result['status'] == 'completed':
            # print("Transcript ID:", transcript_id)
            return(transcription_result)
            break

        elif transcription_result['status'] == 'error':
            raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

        else:
            time.sleep(3)

def download_wav(presigned_url, output_filename):
    # Download the WAV file from the presigned URL
    response = requests.get(presigned_url)
    if response.status_code == 200:
        print("downloading...")
        with open(output_filename, 'wb') as f:
            f.write(response.content)
        print("successfully downloaded file:", output_filename)
    else:
        raise Exception("Failed to download file, status code: {}".format(response.status_code))

# Function to identify the longest monologue of each speaker from each clip
# you pass in the utterances and it returns the longest monologue from each speaker on that file
def find_longest_monologues(utterances):
    longest_monologues = {}
    current_monologue = {}
    last_speaker = None  # Track the last speaker to identify interruptions

    for utterance in utterances:
        speaker = utterance['speaker']
        start_time = utterance['start']
        end_time = utterance['end']

        if speaker not in current_monologue:
            current_monologue[speaker] = {"start": start_time, "end": end_time}
            longest_monologues[speaker] = []
        else:
            # Extend monologue only if it's the same speaker speaking continuously
            if current_monologue[speaker]["end"] == start_time and last_speaker == speaker:
                current_monologue[speaker]["end"] = end_time
            else:
                monologue_length = current_monologue[speaker]["end"] - current_monologue[speaker]["start"]
                new_entry = (monologue_length, copy.deepcopy(current_monologue[speaker]))

                if len(longest_monologues[speaker]) < 1 or monologue_length > min(longest_monologues[speaker], key=lambda x: x[0])[0]:
                    if len(longest_monologues[speaker]) == 1:
                        longest_monologues[speaker].remove(min(longest_monologues[speaker], key=lambda x: x[0]))

                    longest_monologues[speaker].append(new_entry)

                current_monologue[speaker] = {"start": start_time, "end": end_time}

        last_speaker = speaker  # Update the last speaker

    # Check the last monologue for each speaker
    for speaker, monologue in current_monologue.items():
        monologue_length = monologue["end"] - monologue["start"]
        new_entry = (monologue_length, monologue)
        if len(longest_monologues[speaker]) < 1 or monologue_length > min(longest_monologues[speaker], key=lambda x: x[0])[0]:
            if len(longest_monologues[speaker]) == 1:
                longest_monologues[speaker].remove(min(longest_monologues[speaker], key=lambda x: x[0]))
            longest_monologues[speaker].append(new_entry)

    return longest_monologues

# Create clips of each long monologue and embed the clip
# you pass in the file path and the longest monologue objects returned by the find_longest_monologues function.
# This function will create new audio file clips which contain only the longest monologue from each speaker
def clip_and_store_utterances(audio_file, longest_monologues):
    # Load the full conversation audio
    full_audio = AudioSegment.from_wav(audio_file)
    full_audio = full_audio.set_channels(1)

    utterance_clips = []

    for speaker, monologues in longest_monologues.items():
        for _, monologue in monologues:
            start_ms = monologue['start']
            end_ms = monologue['end']
            clip = full_audio[start_ms:end_ms]
            clip_filename = f"{speaker}_monologue_{start_ms}_{end_ms}.wav"
            clip.export(clip_filename, format="wav")

            utterance_clips.append({
                'clip_filename': clip_filename,
                'start': start_ms,
                'end': end_ms,
                'speaker': speaker
            })

    print("Total Number of Monologue Clips Found: ", len(utterance_clips))

    return utterance_clips

# This function uses NeMO to compare two files
def compare_embeddings(utterance_clip, reference_file):
    verification_result = speaker_model.verify_speakers(
        utterance_clip,
        reference_file
    )
    return verification_result

file_one = "YOUR_FILE_1"
file_two = "YOUR_FILE_2"
file_three = "YOUR_FILE_3"

download_wav(file_one, "testone.wav")
download_wav(file_two, "testtwo.wav")
download_wav(file_three, "testthree.wav")

# Store utterances from each clip, keyed by clip index
clip_utterances = {}

# Dictionary to track known speaker identities across all clips
# Maps current clip speaker labels to a unified speaker label
speaker_identity_map = {}

def process_clips(clip_transcript_ids, audio_files):
    global clip_utterances, speaker_identity_map

    # This will store the longest clip filenames for each speaker from the previous clips
    previous_speaker_clips = {}

    for clip_index, (transcript_id, audio_file) in enumerate(zip(clip_transcript_ids, audio_files)):
        transcript = get_transcript(transcript_id)
        utterances = transcript['utterances']
        clip_utterances[clip_index] = utterances # Store utterances for the current clip

        longest_monologues = find_longest_monologues(utterances)

        # Process the longest monologues for clipping and storing
        current_speaker_clips = {}
        for speaker, monologue_data in longest_monologues.items():
            clip_and_store_utterances(audio_file, {speaker: monologue_data})
            longest_clip = f"{speaker}_monologue_{monologue_data[0][1]['start']}_{monologue_data[0][1]['end']}.wav"
            current_speaker_clips[speaker] = longest_clip

        if clip_index == 0:
            speaker_identity_map = {speaker: speaker for speaker in longest_monologues.keys()}
            previous_speaker_clips = current_speaker_clips.copy()
        else:
            # Compare all new speakers against all base speakers from previous clips
            for new_speaker, new_clip in current_speaker_clips.items():
                for base_speaker, base_clip in previous_speaker_clips.items():
                    if compare_embeddings(new_clip, base_clip):
                        speaker_identity_map[new_speaker] = base_speaker
                        break
                else:
                    # If no match is found, assign a new label
                    new_label = chr(ord(max(speaker_identity_map.values(), key=lambda x: ord(x))) + 1)
                    speaker_identity_map[new_speaker] = new_label

            # Update the previous_speaker_clips for the next iteration
            previous_speaker_clips.update(current_speaker_clips)

        # Update utterances with the new speaker labels for the current clip
        for utterance in clip_utterances[clip_index]:
            original_speaker = utterance['speaker']
            # Update only if there's a change in speaker identity
            if original_speaker in speaker_identity_map:
                utterance['speaker'] = speaker_identity_map[original_speaker]


# Add your clip transcript IDs
clip_transcript_ids = [
    "YOUR_TRANSCRIPT_ID_1",
    "YOUR_TRANSCRIPT_ID_2",
    "YOUR_TRANSCRIPT_ID_3"
]

# Add filepaths to your downloaded files
audio_files = [
    "/testone.wav",
    "/testtwo.wav",
    "/testthree.wav"
]

process_clips(clip_transcript_ids, audio_files)

def display_transcript(transcript_data):
    for clip_index, utterances in transcript_data.items():
        print(f"Clip {clip_index + 1}:")
        for utterance in utterances:
            speaker = utterance['speaker']
            text = utterance['text']
            print(f"  Speaker {speaker}: {text}")
        print("\n")  # Add an extra newline for spacing between


display_transcript(clip_utterances)
```

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## Step-by-step instructions

### Install Dependencies

```bash
pip install pytorch
pip install nemo_toolkit['all']
pip install ffmpeg
pip install assemblyai
```

### AssemblyAI Setup, Transcript Setup, and Load the Model Using NeMO

In this section, we'll import dependencies and add functions to transcribe and store transcript IDs if needed.

```python
import assemblyai as aai
import requests
import json
import time
import requests
import copy
from pydub import AudioSegment
import os
import nemo.collections.asr as nemo_asr

speaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained("nvidia/speakerverification_en_titanet_large")

assemblyai_key = "YOUR_API_KEY"

headers = {
    "authorization": assemblyai_key
}
```

### Helper Functions

The function below requests a transcript based on a transcript ID.

```python
def get_transcript(transcript_id):
    polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

    while True:
        transcription_result = requests.get(polling_endpoint, headers=headers).json()

        if transcription_result['status'] == 'completed':
            # print("Transcript ID:", transcript_id)
            return(transcription_result)
            break

        elif transcription_result['status'] == 'error':
            raise RuntimeError(f"Transcription failed: {transcription_result['error']}")

        else:
            time.sleep(3)
```

Our main inference function will make use of these functions:

```python
def download_wav(presigned_url, output_filename):
    # Download the WAV file from the presigned URL
    response = requests.get(presigned_url)
    if response.status_code == 200:
        print("downloading...")
        with open(output_filename, 'wb') as f:
            f.write(response.content)
        print("successfully downloaded file:", output_filename)
    else:
        raise Exception("Failed to download file, status code: {}".format(response.status_code))

# Function to identify the longest monologue of each speaker from each clip
# you pass in the utterances and it returns the longest monologue from each speaker on that file
def find_longest_monologues(utterances):
    longest_monologues = {}
    current_monologue = {}
    last_speaker = None  # Track the last speaker to identify interruptions

    for utterance in utterances:
        speaker = utterance['speaker']
        start_time = utterance['start']
        end_time = utterance['end']

        if speaker not in current_monologue:
            current_monologue[speaker] = {"start": start_time, "end": end_time}
            longest_monologues[speaker] = []
        else:
            # Extend monologue only if it's the same speaker speaking continuously
            if current_monologue[speaker]["end"] == start_time and last_speaker == speaker:
                current_monologue[speaker]["end"] = end_time
            else:
                monologue_length = current_monologue[speaker]["end"] - current_monologue[speaker]["start"]
                new_entry = (monologue_length, copy.deepcopy(current_monologue[speaker]))

                if len(longest_monologues[speaker]) < 1 or monologue_length > min(longest_monologues[speaker], key=lambda x: x[0])[0]:
                    if len(longest_monologues[speaker]) == 1:
                        longest_monologues[speaker].remove(min(longest_monologues[speaker], key=lambda x: x[0]))

                    longest_monologues[speaker].append(new_entry)

                current_monologue[speaker] = {"start": start_time, "end": end_time}

        last_speaker = speaker  # Update the last speaker

    # Check the last monologue for each speaker
    for speaker, monologue in current_monologue.items():
        monologue_length = monologue["end"] - monologue["start"]
        new_entry = (monologue_length, monologue)
        if len(longest_monologues[speaker]) < 1 or monologue_length > min(longest_monologues[speaker], key=lambda x: x[0])[0]:
            if len(longest_monologues[speaker]) == 1:
                longest_monologues[speaker].remove(min(longest_monologues[speaker], key=lambda x: x[0]))
            longest_monologues[speaker].append(new_entry)

    return longest_monologues

# Create clips of each long monologue and embed the clip
# you pass in the file path and the longest monologue objects returned by the find_longest_monologues function.
# This function will create new audio file clips which contain only the longest monologue from each speaker
def clip_and_store_utterances(audio_file, longest_monologues):
    # Load the full conversation audio
    full_audio = AudioSegment.from_wav(audio_file)
    full_audio = full_audio.set_channels(1)

    utterance_clips = []

    for speaker, monologues in longest_monologues.items():
        for _, monologue in monologues:
            start_ms = monologue['start']
            end_ms = monologue['end']
            clip = full_audio[start_ms:end_ms]
            clip_filename = f"{speaker}_monologue_{start_ms}_{end_ms}.wav"
            clip.export(clip_filename, format="wav")

            utterance_clips.append({
                'clip_filename': clip_filename,
                'start': start_ms,
                'end': end_ms,
                'speaker': speaker
            })

    print("Total Number of Monologue Clips Found: ", len(utterance_clips))

    return utterance_clips

# This function uses NeMO to compare two files
def compare_embeddings(utterance_clip, reference_file):
    verification_result = speaker_model.verify_speakers(
        utterance_clip,
        reference_file
    )
    return verification_result
```

### Inference

Add the links to the WAV file clips you have stored on your server.

```python
file_one = "YOUR_FILE_1"
file_two = "YOUR_FILE_2"
file_three = "YOUR_FILE_3"

download_wav(file_one, "testone.wav")
download_wav(file_two, "testtwo.wav")
download_wav(file_three, "testthree.wav")
```

```python
from pydub import AudioSegment

# Store utterances from each clip, keyed by clip index
clip_utterances = {}

# Dictionary to track known speaker identities across all clips
# Maps current clip speaker labels to a unified speaker label
speaker_identity_map = {}

def process_clips(clip_transcript_ids, audio_files):
    global clip_utterances, speaker_identity_map

    # This will store the longest clip filenames for each speaker from the previous clips
    previous_speaker_clips = {}

    for clip_index, (transcript_id, audio_file) in enumerate(zip(clip_transcript_ids, audio_files)):
        transcript = get_transcript(transcript_id)
        utterances = transcript['utterances']
        clip_utterances[clip_index] = utterances # Store utterances for the current clip

        longest_monologues = find_longest_monologues(utterances)

        # Process the longest monologues for clipping and storing
        current_speaker_clips = {}
        for speaker, monologue_data in longest_monologues.items():
            clip_and_store_utterances(audio_file, {speaker: monologue_data})
            longest_clip = f"{speaker}_monologue_{monologue_data[0][1]['start']}_{monologue_data[0][1]['end']}.wav"
            current_speaker_clips[speaker] = longest_clip

        if clip_index == 0:
            speaker_identity_map = {speaker: speaker for speaker in longest_monologues.keys()}
            previous_speaker_clips = current_speaker_clips.copy()
        else:
            # Compare all new speakers against all base speakers from previous clips
            for new_speaker, new_clip in current_speaker_clips.items():
                for base_speaker, base_clip in previous_speaker_clips.items():
                    if compare_embeddings(new_clip, base_clip):
                        speaker_identity_map[new_speaker] = base_speaker
                        break
                else:
                    # If no match is found, assign a new label
                    new_label = chr(ord(max(speaker_identity_map.values(), key=lambda x: ord(x))) + 1)
                    speaker_identity_map[new_speaker] = new_label

            # Update the previous_speaker_clips for the next iteration
            previous_speaker_clips.update(current_speaker_clips)

        # Update utterances with the new speaker labels for the current clip
        for utterance in clip_utterances[clip_index]:
            original_speaker = utterance['speaker']
            # Update only if there's a change in speaker identity
            if original_speaker in speaker_identity_map:
                utterance['speaker'] = speaker_identity_map[original_speaker]


# Add your clip transcript IDs
clip_transcript_ids = [
    "YOUR_TRANSCRIPT_ID_1",
    "YOUR_TRANSCRIPT_ID_2",
    "YOUR_TRANSCRIPT_ID_3"
]

# Add filepaths to your downloaded files
audio_files = [
    "/testone.wav",
    "/testtwo.wav",
    "/testthree.wav"
]

process_clips(clip_transcript_ids, audio_files)
```

### New Clip Utterances

The clip utterances returned by the `process_clips` function will contain the corrected utterances, which can be seen by printing out the utterances or by using the `display_transcript` function.

```python
print(clip_utterances)
```

```python
def display_transcript(transcript_data):
    for clip_index, utterances in transcript_data.items():
        print(f"Clip {clip_index + 1}:")
        for utterance in utterances:
            speaker = utterance['speaker']
            text = utterance['text']
            print(f"  Speaker {speaker}: {text}")
        print("\n")  # Add an extra newline for spacing between


display_transcript(clip_utterances)
```

## Additional Resources

- [Async chunking](https://github.com/AssemblyAI-Solutions/async-chunking)
- [AsyncChunkPy: Near-Realtime Python Speech-to-Text App](https://github.com/AssemblyAI-Solutions/async-chunk-py)
- [Guide for Identifying speakers across multiple podcasts with AssemblyAI and TitaNet](https://docs.google.com/document/d/1xdOvY1LM2lGUNRZCf3kNQLSs5OCUfo74_MHDGm_jYc0/edit?tab=t.0#heading=h.59f375jz72wm)
- [Multi-Speaker Voice Identification and Diarization with AssemblyAI, Pinecone, and Nvidia's TitaNet Model](https://colab.research.google.com/drive/1vqpYcPLEjDjMJ9WvP8C1gvOIflBHs0VS?usp=sharing)


---
title: Separating automatic language detection from transcription
hide-nav-links: true
description: Automatically detect the language using a cost-effective Nano ALD workflow
---

In this guide, you'll learn how to perform automatic language detection (ALD) separately from the transcription process. For the transcription, the file then gets routed to either the [Universal or Nano](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model) model class, depending on the supported language.

This workflow is designed to be cost-effective, slicing the first 60 seconds of audio and running it through Nano ALD, which detects 99 languages, at a cost of $0.002 per transcript for this language detection workflow (not including the total transcription cost).

Performing ALD with this workflow has a few benefits:

- Cost-effective language detection
- Ability to detect 99 languages
- Ability to use Nano as fallback if the language is not supported in Universal
- Ability to enable [Audio Intelligence models](/audio-intelligence) if the [language is supported](/docs/speech-to-text/pre-recorded-audio/supported-languages)
- Ability to use [LeMUR](/lemur) with LLM prompts in Spanish for Spanish audio

## Before you begin

To complete this tutorial, you need:

- [Python](https://www.python.org/) installed.
- A <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">free AssemblyAI account</a>.

The entire source code of this guide can be viewed [here](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/automatic-language-detection-separate.ipynb).

## Step-by-step instructions

Install the Python SDK:

```bash
pip install assemblyai
```

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
```

Create a set with all supported languages for Universal. You can find them in our [documentation here](/docs/speech-to-text/pre-recorded-audio/supported-languages#supported-languages-for-universal).

```python
supported_languages_for_universal = {
    "en",
    "es",
    "fr",
    "de",
    "it",
    "pt",
    "nl",
    "hi",
    "ja",
    "zh",
    "fi",
    "ko",
    "pl",
    "ru",
    "tr",
    "uk",
    "vi",
}
```

Define a `Transcriber`. Note that here we don't pass in a global `TranscriptionConfig`, but later apply different ones during the `transcribe()` call.

```python
transcriber = aai.Transcriber()
```

Define two helper functions:

- `detect_language()` performs language detection on the [first 60 seconds](https://www.assemblyai.com/docs/api-reference/transcripts/submit#request.body.audio_end_at) of the audio using Nano and returns the language code.
- `transcribe_file()` performs the transcription using Universal or Nano depending on the identified language.

```python
def detect_language(audio_url):
    config = aai.TranscriptionConfig(
        audio_end_at=60000,  # first 60 seconds (in milliseconds)
        language_detection=True,
        speech_model=aai.SpeechModel.nano,
    )
    transcript = transcriber.transcribe(audio_url, config=config)
    return transcript.json_response["language_code"]

def transcribe_file(audio_url, language_code):
    config = aai.TranscriptionConfig(
        language_code=language_code,
        speech_model=(
            aai.SpeechModel.universal
            if language_code in supported_languages_for_universal
            else aai.SpeechModel.nano
        ),
    )
    transcript = transcriber.transcribe(audio_url, config=config)
    return transcript
```

Test the code with different audio files. Apply both helper functions sequentially to each file to first identify the language and then transcribe the file.

```python
audio_urls = [
    "https://storage.googleapis.com/aai-web-samples/public_benchmarking_portugese.mp3",
    "https://storage.googleapis.com/aai-web-samples/public_benchmarking_spanish.mp3",
    "https://storage.googleapis.com/aai-web-samples/slovenian_luka_doncic_interview.mp3",
    "https://storage.googleapis.com/aai-web-samples/5_common_sports_injuries.mp3",
]

for audio_url in audio_urls:
    language_code = detect_language(audio_url)
    print("Identified language:", language_code)

    transcript = transcribe_file(audio_url, language_code)
    print("Transcript:", transcript.text[:100], "...")
```

Output:

```bash
Identified language: pt
Transcript: e aí Olá pessoal, sejam bem-vindos a mais um vídeo e hoje eu vou ensinar-vos como fazer esta espada  ...
Identified language: es
Transcript: Precisamente sobre este caso, el diario estadounidense New York Times reveló este sábado un conjunto ...
Identified language: sl
Transcript: Ni lepška, kaj videl tega otroka v mrekoj svojga okolja, da mu je uspil in to v takimi miri, da pač  ...
Identified language: en
Transcript: Runner's knee runner's knee is a condition characterized by pain behind or around the kneecap. It is ...
```


---
title: Use Automatic Language Detection as a Separate Step From Transcription
---

In this guide, we'll show you how to perform automatic language detection separately from the transcription process. For the transcription, the file then gets then routed to either our [_Universal_ or _Nano_](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio/select-the-speech-model) model class, depending on the supported language.

This workflow is designed to be cost-effective, slicing the first 60 seconds of audio and running it through Nano ALD, which detects 99 languages, at a cost of $0.002 per transcript for this language detection workflow (not including the total transcription cost).

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-step instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` package and set your API key:

```python
import assemblyai as aai
aai.settings.api_key = "YOUR_API_KEY"
```

Create a set with all supported languages for _Universal_. You can find them in our [documentation here](https://www.assemblyai.com/docs/concepts/supported-languages#supported-languages-for-universal).

```python
supported_languages_for_universal = {
    "en",
    "en_au",
    "en_uk",
    "en_us",
    "es",
    "fr",
    "de",
    "it",
    "pt",
    "nl",
    "hi",
    "ja",
    "zh",
    "fi",
    "ko",
    "pl",
    "ru",
    "tr",
    "uk",
    "vi",
}
```

Define a `Transcriber`. Note that here we don't pass in a global `TranscriptionConfig`, but later apply different ones during the `transcribe()` call.

```python
transcriber = aai.Transcriber()
```

Define two helper functions:

- `detect_language()` performs language detection on the [first 60 seconds](https://www.assemblyai.com/docs/api-reference/transcripts/submit#request.body.audio_end_at) of the audio using _Nano_ and returns the language code.
- `transcribe_file()` performs the transcription. For this, the identified language is applied and either _Universal_ or _Nano_ is used depending on the supported language.

```python
def detect_language(audio_url):
    config = aai.TranscriptionConfig(
        audio_end_at=60000,  # first 60 seconds (in milliseconds)
        language_detection=True,
        speech_model=aai.SpeechModel.nano,
    )
    transcript = transcriber.transcribe(audio_url, config=config)
    return transcript.json_response["language_code"]

def transcribe_file(audio_url, language_code):
    config = aai.TranscriptionConfig(
        language_code=language_code,
        speech_model=(
            aai.SpeechModel.universal
            if language_code in supported_languages_for_universal
            else aai.SpeechModel.nano
        ),
    )
    transcript = transcriber.transcribe(audio_url, config=config)
    return transcript
```

Test the code with different audio files. For each file, we apply both helper functions sequentially to first identify the language and then transcribe the file.

```python
audio_urls = [
    "https://storage.googleapis.com/aai-web-samples/public_benchmarking_portugese.mp3",
    "https://storage.googleapis.com/aai-web-samples/public_benchmarking_spanish.mp3",
    "https://storage.googleapis.com/aai-web-samples/slovenian_luka_doncic_interview.mp3",
    "https://storage.googleapis.com/aai-web-samples/5_common_sports_injuries.mp3",
]

for audio_url in audio_urls:
    language_code = detect_language(audio_url)
    print("Identified language:", language_code)

    transcript = transcribe_file(audio_url, language_code)
    print("Transcript:", transcript.text[:100], "...")
```


---
title: Create Custom Length Subtitles
---

While our SRT/VTT endpoints do allow you to customize the maximum number of characters per caption using the chars_per_caption URL parameter in your API requests, there are some use-cases that require a custom number of words in each subtitle.

In this guide, we will demonstrate how to construct these subtitles yourself in Python!

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR-API-KEY"

transcriber = aai.Transcriber()

transcript = transcriber.transcribe("./my-audio.mp3")

def second_to_timecode(x: float) -> str:
    hour, x = divmod(x, 3600)
    minute, x = divmod(x, 60)
    second, x = divmod(x, 1)
    millisecond = int(x * 1000.)

    return '%.2d:%.2d:%.2d,%.3d' % (hour, minute, second, millisecond)

def generate_subtitles_by_word_count(transcript, words_per_line):
  output = []
  subtitle_index = 1  # Start subtitle index at 1
  word_count = 0
  current_words = []

  for sentence in transcript.get_sentences():
    for word in sentence.words:
      current_words.append(word)
      word_count += 1
      if word_count >= words_per_line or word == sentence.words[-1]:
        start_time = second_to_timecode(current_words[0].start / 1000)
        end_time = second_to_timecode(current_words[-1].end / 1000)
        subtitle_text = " ".join([word.text for word in current_words])
        output.append(str(subtitle_index))
        output.append("%s --> %s" % (start_time, end_time))
        output.append(subtitle_text)
        output.append("")
        current_words = []  # Reset for the next subtitle
        word_count = 0  # Reset word count
        subtitle_index += 1

  return output

subs = generate_subtitles_by_word_count(transcript, 6)
with open(f"{transcript.id}.srt", 'w') as o:
    final = '\n'.join(subs)
    o.write(final)

print("SRT file generated.")
```

## Step-by-Step Instructions

```python
pip install -U assemblyai

```

Import the assemblyai package and set the API key.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR-API-KEY"
```

Create a Transcriber object.

```python
transcriber = aai.Transcriber()
```

Use the Transcriber object's transcribe method and pass in the audio file's path as a parameter. The transcribe method saves the results of the transcription to the Transcriber object's transcript attribute.

```python
transcript = transcriber.transcribe("./my-audio.mp3")
```

Alternatively, you can pass in the URL of the publicly accessible audio file on the internet.

```python
transcript = transcriber.transcribe("https://storage.googleapis.com/aai-docs-samples/espn.m4a")
```

Define a function that converts seconds to timecodes

```python
def second_to_timecode(x: float) -> str:
    hour, x = divmod(x, 3600)
    minute, x = divmod(x, 60)
    second, x = divmod(x, 1)
    millisecond = int(x * 1000.)

    return '%.2d:%.2d:%.2d,%.3d' % (hour, minute, second, millisecond)
```

Define a function that iterates through the transcripts object to construct a list according to the number of words per subtitle

```python
def generate_subtitles_by_word_count(transcript, words_per_line):
  output = []
  subtitle_index = 1  # Start subtitle index at 1
  word_count = 0
  current_words = []

  for sentence in transcript.get_sentences():
    for word in sentence.words:
      current_words.append(word)
      word_count += 1
      if word_count >= words_per_line or word == sentence.words[-1]:
        start_time = second_to_timecode(current_words[0].start / 1000)
        end_time = second_to_timecode(current_words[-1].end / 1000)
        subtitle_text = " ".join([word.text for word in current_words])
        output.append(str(subtitle_index))
        output.append("%s --> %s" % (start_time, end_time))
        output.append(subtitle_text)
        output.append("")
        current_words = []  # Reset for the next subtitle
        word_count = 0  # Reset word count
        subtitle_index += 1

  return output
```

Generate your subtitle file

```python
subs = generate_subtitles_by_word_count(transcript, 6)
with open(f"{transcript.id}.srt", 'w') as o:
    final = '\n'.join(subs)
    o.write(final)

print("SRT file generated.")
```


---
title: Create Subtitles with Speaker Labels
---

## Quickstart

```python
import assemblyai as aai

# SETTINGS
aai.settings.api_key = "YOUR-API-KEY"
filename = "YOUR-FILE-NAME"
transcriber = aai.Transcriber(config=aai.TranscriptionConfig(speaker_labels=True))
transcript = transcriber.transcribe(filename)

# Maximum number of words per subtitle
max_words_per_subtitle = 6

# Color assignments for speakers
speaker_colors = {
    "A": "red",
    "B": "orange",
    "C": "yellow",
    "D": "yellowgreen",
    "E": "green",
    "F": "lightskyblue",
    "G": "purple",
    "H": "mediumpurple",
    "I": "pink",
    "J": "brown",
}

# Process transcription segments
def process_segments(segments):
    srt_content = ""
    subtitle_index = 1
    for segment in segments:
        speaker = segment.speaker
        color = speaker_colors.get(speaker, "black")  # Default color is black

        # Split text into words and group into chunks
        words = segment.words
        for i in range(0, len(words), max_words_per_subtitle):
            chunk = words[i:i + max_words_per_subtitle]
            start_time = chunk[0].start  # -1 indicates continuation
            end_time = chunk[-1].end
            srt_content += create_subtitle(subtitle_index, start_time, end_time, chunk, color)
            subtitle_index += 1

    return srt_content


# Create a single subtitle
def create_subtitle(index, start_time, end_time, words, color):
    text = ""
    for word in words:
        text += word.text + ' '
    start_srt = format_time(start_time)
    end_srt = format_time(end_time)
    return f"{index}\n{start_srt} --> {end_srt}\n<font color=\"{color}\">{text}</font>\n\n"

# Format time in SRT style
def format_time(milliseconds):
    hours, remainder = divmod(milliseconds, 3600000)
    minutes, remainder = divmod(remainder, 60000)
    seconds, milliseconds = divmod(remainder, 1000)
    return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{int(milliseconds):03}"

# Generate SRT content
sentences = transcript.get_sentences()
srt_content = process_segments(sentences)

# Save to SRT file
with open(filename + '.srt', 'w') as file:
    file.write(srt_content)

print(f"SRT file generated: {filename}.srt")
```

This Colab will demonstrate how to use AssemblyAI's [Speaker Diarization](https://www.assemblyai.com/docs/speech-to-text/speaker-diarization) model together to format subtitles according to their respective speaker.

## Step-by-step guide

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://www.assemblyai.com/dashboard/signup) for an AssemblyAI account and get your API key from your [dashboard](https://www.assemblyai.com/app/account).

```bash
pip install assemblyai
```

First, we will configure our API key as well as our file to be transcribed. Then, we decide on a number of words we want to have per subtitle.

Lastly, we transcribe our file.

```python
import assemblyai as aai

# SETTINGS
aai.settings.api_key = "YOUR-API-KEY"
filename = "YOUR-FILE-NAME"
transcriber = aai.Transcriber(config=aai.TranscriptionConfig(speaker_labels=True))
transcript = transcriber.transcribe(filename)

# Maximum number of words per subtitle
max_words_per_subtitle = 6
```

## How the code works

`speaker_colors` is a dictionary that maps speaker identifiers (like "A", "B", "C", etc.) to specific colors. Each speaker in the transcription will be associated with a unique color in the subtitles.

When Speaker Diarization is enabled, sentences in our API response have a speaker code under the `speaker` key. We use the speaker code to determine the color of the subtitle text.

```python
# Color assignments for speakers
speaker_colors = {
    "A": "red",
    "B": "orange",
    "C": "yellow",
    "D": "yellowgreen",
    "E": "green",
    "F": "lightskyblue",
    "G": "purple",
    "H": "mediumpurple",
    "I": "pink",
    "J": "brown",
}

# Process transcription segments
def process_segments(segments):
    srt_content = ""
    subtitle_index = 1
    for segment in segments:
        speaker = segment.speaker
        color = speaker_colors.get(speaker, "black")  # Default color is black

        # Split text into words and group into chunks
        words = segment.words
        for i in range(0, len(words), max_words_per_subtitle):
            chunk = words[i:i + max_words_per_subtitle]
            start_time = chunk[0].start  # -1 indicates continuation
            end_time = chunk[-1].end
            srt_content += create_subtitle(subtitle_index, start_time, end_time, chunk, color)
            subtitle_index += 1

    return srt_content

# Create a single subtitle
def create_subtitle(index, start_time, end_time, words, color):
    text = ""
    for word in words:
        text += word.text + ' '
    start_srt = format_time(start_time)
    end_srt = format_time(end_time)
    return f"{index}\n{start_srt} --> {end_srt}\n<font color=\"{color}\">{text}</font>\n\n"

# Format time in SRT style
def format_time(milliseconds):
    hours, remainder = divmod(milliseconds, 3600000)
    minutes, remainder = divmod(remainder, 60000)
    seconds, milliseconds = divmod(remainder, 1000)
    return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{int(milliseconds):03}"
```

Our last step is to generate and save our subtitle file!

```python
# Generate SRT content
sentences = transcript.get_sentences()
srt_content = process_segments(sentences)

# Save to SRT file
with open(filename + '.srt', 'w') as file:
    file.write(srt_content)

print(f"SRT file generated: {filename}.srt")
```


---
title: Generate Subtitles for Videos
---

You can export your completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos. Once your transcript status shows as completed, you can make a request to the appropriate endpoint to export your transcript in SRT or VTT format.

In this Colab, we'll walk through the process of generating subtitles for videos using the AssemblyAI API.

## SRT and VTT Subtitles Formats

### How do SRT Files Work?

SRT (SubRip Text) files are commonly used to store subtitles for videos. The format is plain text, and it contains the timing information for each subtitle along with the subtitle text itself.

Here's a breakdown of how the format works:

- Each subtitle entry consists of an index number, start time, end time, and text.
- The index number is a sequential number starting from 1.
- The start and end times are given in the format `hours:minutes:seconds,milliseconds` and are separated by `-->`.
- The text that follows the timing information is the subtitle text itself, and it may span multiple lines.
- Entires are separated by a blank line.

### How do VTT Formats Work?

WEBVTT (Web Video Text Tracks), which is a standard format for displaying timed text tracks (such as subtitles or captions) within HTML5 video.

The syntax is similar to SRT but has some differences:

- The file should begin with the header WEBVTT.
- Timing is done with a period (`.`) separating seconds and milliseconds instead of a comma (`,`).
- No blank lines are needed between entries.
- No index numbers are required.
- This format is supported by many modern browsers and can be ussed with the HTML5 `<track>` element to add subtitles to a `<video>` element.

If you're planning to upload this file to YouTube, you should be able to use it just like an SRT file. YouTube supports various subtitle formats, including WEBVTT.

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

The entire source code of this guide can be viewed [here](https://github.com/AssemblyAI-Examples/docs-snippets/tree/main/subtitles).

## Step-by-Step Instructions

Install the SDK.

```python
pip install -U assemblyai
```

Import the `assemblyai` package and set the API key.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
```

Create a `Transcriber` object.

```python
transcriber = aai.Transcriber()
```

Use the `Transcriber` object's `transcribe` method and pass in the audio file's path as a parameter. The `transcribe` method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcript = transcriber.transcribe("./my-audio.mp3")
```

Alternatively, you can pass in the URL of the publicly accessible audio file on the internet.

```python
transcript = transcriber.transcribe("https://example.org/audio.mp3")
```

Export SRT subtitles with the `export_subtitles_srt` method.

```python
print(transcript.export_subtitles_srt())
```

Export VTT subtitles with the `export_subtitles_vtt` method.

```python
print(transcript.export_subtitles_vtt())
```

## Advanced Usage

You can also customize the maximum number of characters per caption using the `chars_per_caption` URL parameter in your API requests to either the SRT or VTT endpoints. For example, adding `?chars_per_caption=32` to the SRT endpoint URL ensures that each caption has no more than 32 characters.

[API/Model Reference](https://www.assemblyai.com/docs/models/speech-recognition#export-srt-or-vtt-caption-files)

## Conclusion

AssemblyAI can produce subtitles as both `.srt` and `.vtt` files. These are standard subtitle formats, and can be used with videos both on and off the web. For example, after generating your subtitle file, you can add it to a [Mux video](https://www.assemblyai.com/blog/how-to-add-subtitles-to-your-mux-videos-with-python/) using their platform, or you can use [ffmpeg](https://amiaopensource.github.io/ffmprovisr/#embed_subtitles) to embed it in a local video file. Subtitle formats contain plain text, so you can import these formatted captions to most video editors, or fine-tune them as needed.

## Related Cookbooks

[Custom Length Subtitles](core-transcription/subtitle_creation_by_word_count.ipynb)  
[Translating Subtitles](core-transcription/translate_subtitles.ipynb)


---
title: Translate an AssemblyAI Subtitle Transcript
---

In this guide, we'll show you how to translate an AssemblyAI generated subtitle transcript. We will also be using the `Googletrans` Python library to implement the Google Translate API.

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-Step Instructions

Install the relevant packages.

1. AssemblyAI SDK
2. Googletrans

```bash
pip install -U assemblyai googletrans
```

Import the `assemblyai` package and set the API key. Import `Translator` class from `googletrans` package.

```python
import assemblyai as aai
from googletrans import Translator

aai.settings.api_key = "YOUR_API_KEY"
```

Create a `Transcriber` object.

```python
transcriber = aai.Transcriber()
```

Create a `Translator` object.

```python
translator = Translator()
```

Use the Transcriber object's `transcribe` method and pass in the audio file's path as a parameter. The transcribe method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcript = transcriber.transcribe("./my-audio.mp3")
```

Alternatively, you can pass in a path to an audio file saved on the internet.

```python
transcript = transcriber.transcribe("https://example.org/audio.mp3")
```

Export SRT subtitles with the `export_subtitles_srt` method. Create a `subtitle_transcript` variable to translate in the next step.

```python
subtitle_transcript = transcript.export_subtitles_srt()
```

Translate subtitle transcript to target language

```python
translation = translator.translate(subtitle_transcript, dest='es')
```

Print results

```python
print(translation.text)
```

```

```

## Conclusion

Using the subtitles and transcripts generated by AssemblyAI, we can also generate translated alternatives using other translation APIs. The implementation logic will be similar with other solutions like DeepL, Yandex Translate and many more.

To translate to other languages, find the full list of Supported Languages in the **Further Documentations** section.

## Further Documentation

[Googletrans Library](https://pypi.org/project/googletrans/)

[Translation Supported Languages](https://py-googletrans.readthedocs.io/en/latest/#googletrans-languages)


---
title: Schedule a DELETE request with AssemblyAI and EasyCron
---

This tutorial guides you through the process of scheduling a DELETE request for a transcript created with AssemblyAI's transcription service, utilizing EasyCron for scheduling.

## Quickstart

```python
import assemblyai as aai
import requests
from datetime import datetime, timedelta, timezone

# Set up AssemblyAI API key
# In production, store this in an environment variable
aai.settings.api_key = "YOUR_ASSEMBLYAI_API_KEY"

# Get an EasyCron API key here: https://www.easycron.com/user/token
# Set up EasyCron API key
# In production, store this in an environment variable
EASYCRON_API_TOKEN = "YOUR_EASYCRON_API_KEY"

# Create transcriber instance and transcribe audio
transcriber = aai.Transcriber()
transcript = transcriber.transcribe('https://assembly.ai/sports_injuries.mp3')

# Get the transcript ID
transcript_id = transcript.id

# Construct the URL for the DELETE request
url_to_call = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

# Calculate the time 24 hours from now for the cron expression
time_24_hours_from_now = datetime.now(timezone.utc) + timedelta(hours=24)
cron_minute = time_24_hours_from_now.minute
cron_hour = time_24_hours_from_now.hour
cron_day = time_24_hours_from_now.day
cron_month = time_24_hours_from_now.month
cron_year = time_24_hours_from_now.year

# Create the cron expression
cron_expression = f'{cron_minute} {cron_hour} {cron_day} {cron_month} * {cron_year}'

# EasyCron API endpoint for creating a new cron job
api_endpoint = 'https://www.easycron.com/rest/add'

# Data payload for EasyCron API
data = {
    'token': EASYCRON_API_TOKEN,
    'url': url_to_call,
    'cron_expression': cron_expression,
    'http_method': "DELETE",
    'http_headers': f'Authorization: {aai.settings.api_key}',
    'timezone': 'UTC'
}

# Make the request to EasyCron's API
response = requests.post(api_endpoint, data=data)

# Print the response from EasyCron
print("EasyCron API Response:")
print(response.text)
```

## Step-by-step guide

To get started, install the AssemblyAI Python SDK.

```bash
pip install "assemblyai"
```

Finally, import the `assemblyai` package and set your API token in the settings:

```python
import assemblyai as aai

# set the API key
aai.settings.api_key = f"{YOUR_API_KEY}"
```

```python
transcriber = aai.Transcriber()

# this is just an example file
transcript = transcriber.transcribe('https://assembly.ai/sports_injuries.mp3')
```

Store the transcript ID

```python
transcript_id = transcript.id
```

Using this, we now construct the URL that we want our `cron` job to call.

```python
url_to_call = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"
```

## Using EasyCron's API to schedule a DELETE request

First, sign up for an account with EasyCron [here](https://www.easycron.com/cron-jobs). Locate your EasyCron API key [in your user dashboard.](https://www.easycron.com/user/token)

Then, insert your EasyCron API key into your code.

Next, we will use the datetime module to generate a `cron` expression for 24 hours from now (although you can configure the `timedelta` argument to be whatever you want)

```python
# In production, you should store your API keys in environment variables
EASYCRON_API_TOKEN = "YOUR_EASYCRON_API_KEY"

from datetime import datetime, timedelta, timezone

# Calculate the time 24 hours from now in EasyCron's expected format
# EasyCron uses a slightly different format, but for simplicity, we'll use the standard UTC format
# and convert this into a cron expression
time_24_hours_from_now = datetime.now(timezone.utc) + timedelta(hours=24)
cron_minute = time_24_hours_from_now.minute
cron_hour = time_24_hours_from_now.hour
cron_day = time_24_hours_from_now.day
cron_month = time_24_hours_from_now.month
cron_year = time_24_hours_from_now.year

cron_expression = f'{cron_minute} {cron_hour} {cron_day} {cron_month} * {cron_year}'
```

Now, we will schedule a `cron` job 24 hours from now with EasyCron to send a delete request to AssemblyAI for the transcript that we just generated.

```python
import requests

# EasyCron API endpoint for creating a new cron job
api_endpoint = 'https://www.easycron.com/rest/add'

# The data to be sent to EasyCron's API
data = {
    'token': EASYCRON_API_TOKEN,
    'url': url_to_call,
    'cron_expression': cron_expression,
    'http_method': "DELETE",
    'http_headers': f'Authorization: {AAI_API_TOKEN}',
    'timezone': 'UTC'
}

# Make the request to EasyCron's API
response = requests.post(api_endpoint, data=data)

# Print the response from EasyCron's API
print(response.text)
```

## Troubleshooting:

`Error 3: The cron expression you entered is invalid or it cannot be matched in a realitic future.`: Check that your [EasyCron account settings](https://www.easycron.com/user/edit) has UTC configured as the default timezone.

### Other resources

[AssemblyAI DELETE endpoint API reference](https://www.assemblyai.com/docs/api-reference/transcript#delete-a-transcript)  
[EasyCron API reference](https://www.easycron.com/document/add)


---
title: Troubleshoot Common Errors
---

AssemblyAI's API always returns a response even when there is an error. This guide is designed to help you navigate and resolve common issues when implementing AssemblyAI.

## Understanding Errors with AssemblyAI

There are primarily two types of errors you might encounter when working with AssemblyAI:

- errors that occur when requesting a transcription
- errors that occur while the transcription is processing

The first category includes issues such as authentication errors, invalid request parameters, or server-related errors, which are typically flagged immediately when you attempt to initiate a request.

The second category, failed transcription jobs, pertains to errors that occur during the transcription process itself. These might be due to issues with the audio file, unsupported languages, or internal errors on our end.

## Handling Errors with AssemblyAI

### Error handling with AssemblyAI's SDKs (Recommended)

When using any of our SDKs, both types of errors are surfaced via the error key in a transcript object. For example in Python:

```python
if transcript.status == aai.TranscriptStatus.error:
    print(transcript.error)
```

### Error handling with HTTPS requests

For errors when making a request for a transcription, you will have to check the status code that we respond with. For errors that occur during the transcription process, you will need to access the "error" key in the JSON response. For other HTTP errors you can print the information from the response object. Here is an example you can use:

```python
if response.status_code != 200:
    try:
        print(response.json()['error'])
    except Exception:
        print(response.status_code, response.text, response.url)
```

## Common errors when making a request for a transcription

#### Models are not Supported for a Particular Language

```json
# Status code: 400
{
"error": "The following models are not available in this language: speaker_labels"
}
```

- **Solution**: Before you start, make sure to check our [Supported Languages page](https://www.assemblyai.com/docs/concepts/supported-languages). This page provides detailed information on what features are available for each language, helping you to choose the right options for your transcription or analysis needs.

#### Insufficient Funds

```json
# Status code: 400
{
"error": "Your current account balance is negative. Please top up to continue using the API."
}
```

**Solution**:

- **Auto-pay**: Enable auto-pay in your account settings to automatically add funds when your balance runs low.
- **Check Funds**: Regularly check your account balance to ensure you have sufficient funds for your transcription requests.
- **Add Funds**: If needed, add funds to your account to continue using our services without interruption.

#### Invalid API Token

```json
# Status code: 401
{
"error": "Authentication error, API token missing/invalid."
}
```

An invalid API token will prevent you from making successful requests.

**Solution**:

- Double-check that the API token you're using matches exactly with the token shown in your dashboard. Even a small typo can lead to authentication errors.

#### Unsupported Characters in Custom Vocabulary

```json
# Status code: 400
{
"error": "'🇸🇬' was included in the word boost list but contains unsupported characters"
}
```

Custom vocabulary only supports ASCII characters, but attempts to normalise text first before throwing an error. This error is usually caused by words or symbols that do not have an ASCII-equivalent.

**Solution**:

- Check that the word or phrase can be normalized prior to submitting it to Custom Vocabulary. Here is a code snippet that does this using Python's [unicodedata package](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize):

```python
def filter_unsupported_characters(phrase):
    cleaned_phrase = `unicodedata.`normalize('NFKD', phrase).encode('ascii', 'ignore').decode('ascii')
    if len(cleaned_phrase) != len(phrase):
        raise Error("Unsupported characters in phrase")
    return cleaned_phrase
```

#### Transcript ID not found

```json
# Status code: 400
{
"error": "Transcript lookup error, transcript id not found"
}
```

**Solution**:

- **Verify the endpoint and method that you are using**: Check that you are making a `POST` request to `http://api.assemblyai.com/v2/transcript`or a `GET` request to `http://api.assemblyai.com/v2/transcript/{transcript_id}` and not `http://api.assemblyai.com/v2/transcript/`
- **Token Verification**: Double-check that the API token you're using matches exactly with the token used to make the original request.
- If you're using Postman, ensure that `Encode URL automatically` under Settings is **disabled**.

#### Server Errors

```json
# Status code: 500
{
"error": "Server error, developers have been alerted."
}
```

Server errors rarely happen but can occasionally occur on our side.

**Solution**:

- **Retries**: Implement retries in your code for when our server returns a 500 code response.
- **Automatic Retries**: Enable automatic retries for your transcription jobs under [Account > Settings](https://www.assemblyai.com/app/account) on your dashboard. This ensures that if a job fails due to a temporary server issue, it will automatically be retried.
- **Check our Status page** to verify that we are not currently undergoing an incident
- **Reach out to Support**: Remember to provide the transcript ID, audio file used, and parameters used in your request or the full JSON response in your message. You can also email support@assemblyai.com for help!

## Common transcription processing errors

#### Audio File URL Errors

##### Attempting to transcribe webpages

```json
{
  "status": "error",
  "audio_url": "https://www.youtube.com/watch?v=r8KTOBOMm0A",
  "error": "File does not appear to contain audio. File type is text/html (HTML document, ASCII text, with very long lines (56754)).",
  ...
}
```

Our API requires a publicly accessible URL that points to an audio file to retrieve your file for transcription. To transcribe a YouTube video, [check out this Cookbook](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/transcribe_youtube_videos.ipynb).

##### Attempting to transcribe audio files that are not accessible

```json
{
  "status": "error",
  "audio_url": "https://foo.bar",
  "error": "Download error, unable to download https://foo.bar. Please make sure the file exists and is accessible from the internet."
}
```

**Solution**:

- **Public Access**: Verify that the audio file URL is publicly accessible. Our servers cannot transcribe audio from private or restricted URLs.
- **Google Drive URLs**: For audio stored on Google Drive, consult our [Google Drive Transcription Cookbook](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/transcribing-google-drive-file.md) to correctly format your URLs for access.
- **Direct Upload**: Utilize the [AssemblyAI Upload endpoint](https://www.assemblyai.com/docs/api-reference/upload) to upload files directly from your device, eliminating the need for a public URL.
- **AWS S3 Pre-signed URLs**: [This Cookbook](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/transcribe_from_s3.ipynb) shows you how to use pre-signed URLs for AWS S3 storage to provide secure, temporary access for transcription without making your files public.

#### Audio File Errors

##### Attempting to transcribe audio files that are too short

```json
{
  "status": "error",
  "audio_url": "https://foo.bar",
  "error": "Audio duration is too short."
}
```

The minimum audio duration for a file submitted to our API is 160ms.

**Solution**:

- **Add error handling for this error message**: When this error occurs, handle it safely by checking the error string and returning the error.
- **Add pre-submit checks for the duration of the audio file**: Prior to submitting a file for transcription, check the duration using a tool like soxi (part of the SoX package): `soxi -D audio.mp3`


---
title: Implement Retry Server Error Logic
---

In this guide, we'll show you how to setup automatic server error retry logic in your transcription process.

Server errors indicate a server-side issue during the transcription process. These rarely happen, but can occasionally occur on our side. If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio. If the issue persists, please reach out to our support team: support@assemblyai.com

This workflow is designed to automatically retry these transcripts if a server error is encountered.

<Tip>
  If your transcription fails due to a server error on our side, we will
  automatically retry the request up to three times. You can find this option in
  your [Account Settings](https://assemblyai.com/app/account).
</Tip>

## Quickstart

```python
import assemblyai as aai
import time

aai.settings.api_key = "YOUR_API_KEY"

def handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5):
    for attempt in range(retries + 1):
        transcript = transcriber.transcribe(audio_url, config)
        if transcript.error == "Server error, developers have been alerted":
            if attempt < retries:
                print(f"Encountered a server error. Retrying in {wait_time} second(s)...")
                time.sleep(wait_time)
            else:
                print("Retry failed with a server error. Please contact AssemblyAI Support: support@assemblyai.com")
                return None
        elif transcript.status == aai.TranscriptStatus.error:
            print(f"Transcription failed: {transcript.error}")
            return None
        else:
            return transcript

audio_url = "YOUR_AUDIO_URL"
transcriber = aai.Transcriber()
config = aai.TranscriptionConfig()

transcript = handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5)
if transcript:
    print(transcript.text)

```

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-step instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` and `time` package and set your API key:

```python
import assemblyai as aai
import time

aai.settings.api_key = "YOUR_API_KEY"
```

Create a function that handles errors that may occur during the transcription process. The default number of retires is 1. The default wait time before retranscribing is 5 seconds.

```python
def handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5):
    for attempt in range(retries + 1):
        transcript = transcriber.transcribe(audio_url, config)
        if transcript.error == "Server error, developers have been alerted":
            if attempt < retries:
                print(f"Encountered a server error. Retrying in {wait_time} second(s)...")
                time.sleep(wait_time)
            else:
                print("Retry failed with a server error. Please contact AssemblyAI Support: support@assemblyai.com")
                return None
        elif transcript.status == aai.TranscriptStatus.error:
            print(f"Transcription failed: {transcript.error}")
            return None
        else:
            return transcript
```

Define the audio file that you want to transcribe.

```python
audio_url = "YOUR_AUDIO_URL"
```

Create a `Transcriber` object and specify features in `TranscriptionConfig`.

```python
transcriber = aai.Transcriber()
config = aai.TranscriptionConfig()
```

Call the function to handle transcription with error handling. Specify number of retries and wait time. Return the transcribed text if transcription is successful.

```python
transcript = handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5)
if transcript:
    print(transcript.text)
```


---
title: Implement Retry Server Error Logic
---

In this guide, we'll show you how to setup automatic server error retry logic in your transcription process.

Server errors indicate a server-side issue during the transcription process. These rarely happen, but can occasionally occur on our side. If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio. If the issue persists, please reach out to our support team: support@assemblyai.com

This workflow is designed to automatically retry these transcripts if a server error is encountered.

<Tip>
  If your transcription fails due to a server error on our side, we will
  automatically retry the request up to three times. You can find this option in
  your [Account Settings](https://assemblyai.com/app/account).
</Tip>

## Quickstart

```python
import assemblyai as aai
import time

aai.settings.api_key = "YOUR_API_KEY"

def handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5):
    for attempt in range(retries + 1):
        transcript = transcriber.transcribe(audio_url, config)
        if transcript.error == "Server error, developers have been alerted":
            if attempt < retries:
                print(f"Encountered a server error. Retrying in {wait_time} second(s)...")
                time.sleep(wait_time)
            else:
                print("Retry failed with a server error. Please contact AssemblyAI Support: support@assemblyai.com")
                return None
        elif transcript.status == aai.TranscriptStatus.error:
            print(f"Transcription failed: {transcript.error}")
            return None
        else:
            return transcript

audio_url = "YOUR_AUDIO_URL"
transcriber = aai.Transcriber()
config = aai.TranscriptionConfig()

transcript = handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5)
if transcript:
    print(transcript.text)

```

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-step instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` and `time` package and set your API key:

```python
import assemblyai as aai
import time

aai.settings.api_key = "YOUR_API_KEY"
```

Create a function that handles errors that may occur during the transcription process. The default number of retires is 1. The default wait time before retranscribing is 5 seconds.

```python
def handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5):
    for attempt in range(retries + 1):
        transcript = transcriber.transcribe(audio_url, config)
        if transcript.error == "Server error, developers have been alerted":
            if attempt < retries:
                print(f"Encountered a server error. Retrying in {wait_time} second(s)...")
                time.sleep(wait_time)
            else:
                print("Retry failed with a server error. Please contact AssemblyAI Support: support@assemblyai.com")
                return None
        elif transcript.status == aai.TranscriptStatus.error:
            print(f"Transcription failed: {transcript.error}")
            return None
        else:
            return transcript
```

Define the audio file that you want to transcribe.

```python
audio_url = "YOUR_AUDIO_URL"
```

Create a `Transcriber` object and specify features in `TranscriptionConfig`.

```python
transcriber = aai.Transcriber()
config = aai.TranscriptionConfig()
```

Call the function to handle transcription with error handling. Specify number of retries and wait time. Return the transcribed text if transcription is successful.

```python
transcript = handle_error_transcription(audio_url, transcriber, config, retries=1, wait_time=5)
if transcript:
    print(transcript.text)
```


---
title: Implement Retry Upload Error Logic
---

In this guide, we'll show you how to set up automatic upload error retry logic in your transcription process.

Upload errors could be a result of a transient issue with our servers or they could be related to an issue with the file itself. Most likely the issue would be that the file is empty. Because the cause can be unclear at first, we recommend adding some retry logic to handle the rare occasions in which our upload service is experiencing performance issues. If the upload failure persists, you'll want to check whether the file is empty. If you're unclear on why the file is failing, please reach out to our support team at support@assemblyai.com.

This workflow is designed to automatically retry file uploads if an upload error is encountered.

## Quickstart

```python
import assemblyai as aai
import time
from assemblyai.types import TranscriptError

aai.settings.api_key = "YOUR_API_KEY"

def transcribe_with_upload_retry(file_path, retries=3, delay=5):
    transcriber = aai.Transcriber()

    for attempt in range(retries):
        try:
            # Attempt to transcribe the file
            config = aai.TranscriptionConfig(speaker_labels=True)
            transcript = transcriber.transcribe(file_path, config)
            return transcript

        except TranscriptError as e:
            # Handle specific error if upload fails
            print(f"Attempt {attempt + 1} failed. {e}")

            # Retry if a TranscriptError occurs,
            if attempt + 1 < retries:
                print(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                raise e  # Raise the error after max retries

    print("Max retries reached. Transcription failed.")
    return None

transcribe_with_upload_retry("YOUR_AUDIO_URL")

```

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-step instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` package and assemblyai's `TranscriptError`. Additionally import the and `time` package and set your API key:

```python
import assemblyai as aai
import time
from assemblyai.types import TranscriptError

aai.settings.api_key = "YOUR_API_KEY"
```

Create a function that retries upload failures. This example retries up to 3 times with a delay of 5 seconds each time.

```python
def transcribe_with_upload_retry(file_path, retries=3, delay=5):
    transcriber = aai.Transcriber()

    for attempt in range(retries):
        try:
            # Attempt to transcribe the file
            config = aai.TranscriptionConfig(speaker_labels=True)
            transcript = transcriber.transcribe(file_path, config)
            return transcript

        except TranscriptError as e:
            # Handle specific error if upload fails
            print(f"Attempt {attempt + 1} failed. {e}")

            # Retry if a TranscriptError occurs,
            if attempt + 1 < retries:
                print(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                raise e  # Raise the error after max retries

    print("Max retries reached. Transcription failed.")
    return None

transcribe_with_upload_retry("YOUR_AUDIO_URL")
```


---
title: >-
  Correct Audio Duration Discrepancies with Multi-Tool Validation and
  Transcoding
---

In this guide, you'll learn how to check the audio duration of a file using three different tools: `ffprobe`, `SoX`, and `MediaInfo`. This guide was created in response to customer feedback about transcription results showing incorrect audio durations. The issue was traced to audio files with corrupted metadata or problematic headers, leading to inaccurate duration data. If these tools report differing durations for the same file, transcription inconsistencies can arise. We will programmatically detect any duration mismatches and transcode the file to resolve them, typically resulting in a more accurate transcription.

## Quickstart

```python
import assemblyai as aai
import subprocess

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()

def get_duration_ffprobe(file_path):
    command = [
        'ffprobe', '-v', 'error', '-show_entries',
        'format=duration', '-of',
        'default=noprint_wrappers=1:nokey=1', file_path
    ]
    try:
        duration = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(duration.stdout.strip())
    except ValueError:
        print("Error: Unable to parse duration from ffprobe output.")
        return None

def get_duration_sox(file_path):
    command = ['soxi', '-D', file_path]
    try:
        duration = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(duration.stdout.strip())
    except ValueError:
        print("Error: Unable to parse duration from SoX output.")
        return None

def get_duration_mediainfo(file_path):
    command = ['mediainfo', '--Output=General;%Duration%', file_path]
    try:
        duration_ms = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        duration_str = duration_ms.stdout.strip()
        # Check if the output is empty or not a valid number
        if duration_str:
            return float(duration_str) / 1000
        else:
            print("Error: MediaInfo returned empty or invalid duration")
            return None
    except ValueError:
        print("Error: Unable to parse duration from MediaInfo output.")
        return None

def check_audio_durations(file_path):
    #Check if audio durations differ among the three tools.
    ffprobe_duration = get_duration_ffprobe(file_path)
    sox_duration = get_duration_sox(file_path)
    mediainfo_duration = get_duration_mediainfo(file_path)

    # Print all retrieved durations
    print(f"ffprobe duration: {ffprobe_duration:.6f} seconds" if ffprobe_duration is not None else "ffprobe duration: Error retrieving duration")
    print(f"SoX duration: {sox_duration:.6f} seconds" if sox_duration is not None else "SoX duration: Error retrieving duration")
    print(f"MediaInfo duration: {mediainfo_duration:.6f} seconds" if mediainfo_duration is not None else "MediaInfo duration: Error retrieving duration")

    # Return durations for further checks
    return (ffprobe_duration, sox_duration, mediainfo_duration)

def transcribe(file):
    print("Executing transcription as audio durations are consistent.")
    transcript = transcriber.transcribe(file)
    print(transcript.text)

def transcode(input_file, output_file):
    #Transcode audio file to a 16kHz WAV file.
    print(f"Transcoding file {input_file} to {output_file}...")
    command = [
        'ffmpeg', '-i', input_file, '-ar', '16000', '-ac', '1', output_file
    ]
    try:
        subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if os.path.exists(output_file):
            print(f"Transcoding complete. Output file: {output_file}")
        else:
            print("Error: Transcoding failed.")
    except subprocess.CalledProcessError as e:
        print("Warnings from ffmpeg")
        """Print errors or warnings from ffmpeg"""
        #print(e.stderr.decode())

def durations_are_consistent(durations, tolerance=0.01):
    #Check if durations are consistent within a given tolerance of 0.01 seconds.
    if None in durations:
        return False
    min_duration = min(durations)
    max_duration = max(durations)
    return (max_duration - min_duration) <= tolerance

def main(file_path):
    durations = check_audio_durations(file_path)

    if durations:
        if None in durations:
            print("Error: One or more duration values could not be retrieved.")
            transcoded_file = file_path.rsplit('.', 1)[0] + '_transcoded.wav'
            transcode(file_path, transcoded_file)
            new_durations = check_audio_durations(transcoded_file)
            if new_durations and durations_are_consistent(new_durations):
                transcribe(transcoded_file)
            else:
                print("Warning: The audio durations still differ or an error occurred with the transcoded file.")
        elif not durations_are_consistent(durations):
            print("Warning: The audio durations differ between tools.")
            transcoded_file = file_path.rsplit('.', 1)[0] + '_transcoded.wav'
            transcode(file_path, transcoded_file)
            new_durations = check_audio_durations(transcoded_file)
            if new_durations and durations_are_consistent(new_durations):
                transcribe(transcoded_file)
            else:
                print("Warning: The audio durations still differ or an error occurred with the transcoded file.")
        else:
            print("The audio durations are consistent.")
            transcribe(file_path)

audio_file="./audio.mp4"

if __name__ == "__main__":
    file_path = f"{audio_file}"
    main(file_path)
```

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## Step-by-Step Instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the `assemblyai` package along with `subprocess`, set your AssemblyAI API key, and initiate the transcriber.

```python
import assemblyai as aai
import subprocess

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()
```

For this cookbook you will need [`ffmpeg`](https://www.ffmpeg.org/), [`sox`](https://sourceforge.net/projects/sox/), and [`MediaInfo`](https://mediaarea.net/en/MediaInfo). We will use these tools to pull the duration from the audio. Matching audio duration is crucial because discrepancies may indicate issues with the audio file's metadata or headers. Such inconsistencies can lead to inaccurate transcription results, playback issues, or unexpected behaviour in media applications. By verifying that the duration is consistent across all three tools, we can detect potential problems early and correct any corrupted metadata or faulty headers before processing the audio further.

First, we will get the audio duration using `ffprobe`.

```python
def get_duration_ffprobe(file_path):
    command = [
        'ffprobe', '-v', 'error', '-show_entries',
        'format=duration', '-of',
        'default=noprint_wrappers=1:nokey=1', file_path
    ]
    try:
        duration = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(duration.stdout.strip())
    except ValueError:
        print("Error: Unable to parse duration from ffprobe output.")
        return None

```

Next, we will get the audio duration for the same file using `sox`.

```python
def get_duration_sox(file_path):
    command = ['soxi', '-D', file_path]
    try:
        duration = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return float(duration.stdout.strip())
    except ValueError:
        print("Error: Unable to parse duration from SoX output.")
        return None
```

Finally, we will get the audio duration for the same file using `MediaInfo`.

```python
def get_duration_mediainfo(file_path):
    command = ['mediainfo', '--Output=General;%Duration%', file_path]
    try:
        duration_ms = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        duration_str = duration_ms.stdout.strip()
        # Check if the output is empty or not a valid number
        if duration_str:
            return float(duration_str) / 1000
        else:
            print("Error: MediaInfo returned empty or invalid duration")
            return None
    except ValueError:
        print("Error: Unable to parse duration from MediaInfo output.")
        return None

```

The following function will return the durations from the three tools and convert them to the same format.

```python
def check_audio_durations(file_path):
    #Check if audio durations differ among the three tools.
    ffprobe_duration = get_duration_ffprobe(file_path)
    sox_duration = get_duration_sox(file_path)
    mediainfo_duration = get_duration_mediainfo(file_path)

    # Print all retrieved durations
    print(f"ffprobe duration: {ffprobe_duration:.6f} seconds" if ffprobe_duration is not None else "ffprobe duration: Error retrieving duration")
    print(f"SoX duration: {sox_duration:.6f} seconds" if sox_duration is not None else "SoX duration: Error retrieving duration")
    print(f"MediaInfo duration: {mediainfo_duration:.6f} seconds" if mediainfo_duration is not None else "MediaInfo duration: Error retrieving duration")

    # Return durations for further checks
    return (ffprobe_duration, sox_duration, mediainfo_duration)
```

Define the `transcribe` function. This will run only when the duration is consistent among the three tools.

```python
def transcribe(file):
    print("Executing transcription as audio durations are consistent.")
    transcript = transcriber.transcribe(file)
    print(transcript.text)
```

Define the `transcode` function. We will run this if one or more durations differ. The output file will be a 16kHz WAV file as that is the format AssemblyAI models are trained on. When running the `ffmpeg` command, the transcode may fail or return warnings if there are issues with the input file's format, corrupted metadata, or unsupported codecs. These warnings tend to be verbose but you can print them for troubleshooting.

```python
def transcode(input_file, output_file):
    #Transcode audio file to a 16kHz WAV file.
    print(f"Transcoding file {input_file} to {output_file}...")
    command = [
        'ffmpeg', '-i', input_file, '-ar', '16000', '-ac', '1', output_file
    ]
    try:
        subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if os.path.exists(output_file):
            print(f"Transcoding complete. Output file: {output_file}")
        else:
            print("Error: Transcoding failed.")
    except subprocess.CalledProcessError as e:
        print("Warnings from ffmpeg")
        """Print errors or warnings from ffmpeg"""
        #print(e.stderr.decode())
```

Define a function that will check if the durations are consistent. There may be small differences so it's best to allow a small tolerance. In this example the tolerance value will be 0.01 seconds.

```python
def durations_are_consistent(durations, tolerance=0.01):
    #Check if durations are consistent within a given tolerance of 0.01 seconds.
    if None in durations:
        return False
    min_duration = min(durations)
    max_duration = max(durations)
    return (max_duration - min_duration) <= tolerance
```

Finally, here is the order of operations for this program. This program will first check the duration of an audio file across different tools to ensure consistency. If any tool fails to retrieve a duration or if the durations differ, it transcodes the audio to a new 16kHz WAV file and checks the duration of the WAV file. If the durations are consistent in the transcoded file, the program proceeds to transcribe it. If inconsistencies remain after transcoding, it logs a warning to highlight the issue and will not transcribe the file.

```python
def main(file_path):
    durations = check_audio_durations(file_path)

    if durations:
        if None in durations:
            print("Error: One or more duration values could not be retrieved.")
            transcoded_file = file_path.rsplit('.', 1)[0] + '_transcoded.wav'
            transcode(file_path, transcoded_file)
            new_durations = check_audio_durations(transcoded_file)
            if new_durations and durations_are_consistent(new_durations):
                transcribe(transcoded_file)
            else:
                print("Warning: The audio durations still differ or an error occurred with the transcoded file.")
        elif not durations_are_consistent(durations):
            print("Warning: The audio durations differ between tools.")
            transcoded_file = file_path.rsplit('.', 1)[0] + '_transcoded.wav'
            transcode(file_path, transcoded_file)
            new_durations = check_audio_durations(transcoded_file)
            if new_durations and durations_are_consistent(new_durations):
                transcribe(transcoded_file)
            else:
                print("Warning: The audio durations still differ or an error occurred with the transcoded file.")
        else:
            print("The audio durations are consistent.")
            transcribe(file_path)

audio_file="./audio/8950.mp4"

if __name__ == "__main__":
    file_path = f"{audio_file}"
    main(file_path)
```

If you continue to experience unexpected behaviour with your file, please contact our support team at support@assemblyai.com for assistance in diagnosing the issue.



---
title: Translate an AssemblyAI Subtitle Transcript
---

In this guide, we'll show you how to translate an AssemblyAI generated subtitle transcript. We will also be using the `Googletrans` Python library to implement the Google Translate API.

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Step-by-Step Instructions

Install the relevant packages.

1. AssemblyAI SDK
2. Googletrans

```bash
pip install -U assemblyai googletrans
```

Import the `assemblyai` package and set the API key. Import `Translator` class from `googletrans` package.

```python
import assemblyai as aai
from googletrans import Translator

aai.settings.api_key = "YOUR_API_KEY"
```

Create a `Transcriber` object.

```python
transcriber = aai.Transcriber()
```

Create a `Translator` object.

```python
translator = Translator()
```

Use the Transcriber object's `transcribe` method and pass in the audio file's path as a parameter. The transcribe method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcript = transcriber.transcribe("./my-audio.mp3")
```

Alternatively, you can pass in a path to an audio file saved on the internet.

```python
transcript = transcriber.transcribe("https://example.org/audio.mp3")
```

Export SRT subtitles with the `export_subtitles_srt` method. Create a `subtitle_transcript` variable to translate in the next step.

```python
subtitle_transcript = transcript.export_subtitles_srt()
```

Translate subtitle transcript to target language

```python
translation = translator.translate(subtitle_transcript, dest='es')
```

Print results

```python
print(translation.text)
```

```

```

## Conclusion

Using the subtitles and transcripts generated by AssemblyAI, we can also generate translated alternatives using other translation APIs. The implementation logic will be similar with other solutions like DeepL, Yandex Translate and many more.

To translate to other languages, find the full list of Supported Languages in the **Further Documentations** section.

## Further Documentation

[Googletrans Library](https://pypi.org/project/googletrans/)

[Translation Supported Languages](https://py-googletrans.readthedocs.io/en/latest/#googletrans-languages)


---
title: Translate AssemblyAI Transcripts Into Other Languages Using Commercial Models
---

This Cookbook walks through how to translate AssemblyAI transcripts using a variety of commerical and open-source machine translation models.

Choosing a model depends on your use-case and preferences. Here are some considerations you may want to make when choosing a model to use for translation:

- Accuracy and Quality of Translation: you should compare the translations from each provider to see which translation you prefer
- Language Support: check the supported languages for [Google Translate](https://cloud.google.com/translate/docs/languages), [DeepL](https://www.deepl.com/docs-api/translate-text), and [python-translate](https://github.com/terryyin/translate-python) respectively
- Cost: while commercial models usually have a free-tier or trials, they will incur costs eventually

## Setup

To get started, paste your API token into the empty string below. If you don't already have an API token, you can get one for free [here](https://www.assemblyai.com/dashboard/signup).

```python
AAI_API_TOKEN = ""
```

**Make sure not to share this token with anyone** - it is a private key associated uniquely to your account.

Next, we'll install the AssemblyAI Python SDK, which will allow us to easily use LeMUR in just a few lines of code.

```bash
pip install "assemblyai"
```

Finally, import the `assemblyai` package and set your API token in the settings:

```python
import assemblyai as aai

# set the API key
aai.settings.api_key = f"{AAI_API_TOKEN}"
```

```python
config = aai.TranscriptionConfig(language_detection=True)
transcriber = aai.Transcriber(config=config)

transcript = transcriber.transcribe('./my-audio.mp3')
```

Specify the target language for the translation

```python
to_lang = 'en'
```

Get detected language code from the AAI JSON response

```python
from_lang = transcript.json_response['language_code']
```

# Commercial Models

## Google Translate API

https://cloud.google.com/translate/docs/reference/libraries/v2/python

Note: you will need a GCP account as well as app credentials to make this API request.

```bash
pip install google-cloud-translate==2.0.1
```

Follow Google's docs on how to generate a credentials JSON file: https://cloud.google.com/docs/authentication/application-default-credentials

```python
import os
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './translate_creds.json'
```

```python
from google.cloud import translate_v2

def translate(target: str, text: str) -> dict:
    """Translates text into the target language.

    Target must be an ISO 639-1 language code.
    See https://g.co/cloud/translate/v2/translate-reference#supported_languages
    """
    from google.cloud import translate_v2 as translate

    translate_client = translate.Client()

    if isinstance(text, bytes):
        text = text.decode("utf-8")

    # Text can also be a sequence of strings, in which case this method
    # will return a sequence of results for each text.
    result = translate_client.translate(text, target_language=target)

    print("Text: {}".format(result["input"]))
    print("Translation: {}".format(result["translatedText"]))
    print()

    return result

for sent in transcript.get_sentences():
  translate(to_lang, sent.text)
```

## DeepL API

https://www.deepl.com/en/docs-api

```bash
pip install deepl
```

You will need a DeepL account and API token, which can be found here: https://www.deepl.com/pro-api

```python
DEEPL_API_TOKEN = ''
```

```python
import deepl

def translate(text):
    translator = deepl.Translator(DEEPL_API_TOKEN)
    result = translator.translate_text(text, target_lang="EN-US") # Note: DeepL requires more formal language code
    return result.text

# Example usage
for sent in transcript.get_sentences():
  translated_text = translate(sent.text)
  print("Text: {}".format(sent.text))
  print("Translation: {}".format(translated_text))
  print()
```

# Open-Source Models

## `translate-python` Library

https://github.com/terryyin/translate-python

```bash
pip install translate
```

```python
from translate import Translator

def translate(text):
    translator = Translator(to_lang=to_lang, from_lang=from_lang)
    translation = translator.translate(text)
    return translation

for sent in transcript.get_sentences():
  translated_text = translate(sent.text)
  print("Text: {}".format(sent.text))
  print("Translation: {}".format(translated_text))
  print()
```

# Further Documentation

Cookbook: [Translate subtitles](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/translate_subtitles.ipynb)


---
title: Transform Chinese transcripts into Simplified or Traditional Text
---

When transcribing Chinese audio, our models produce output that mixes both Simplified and Traditional Chinese characters. This happens because our models are typically trained on diverse datasets containing a mix of both writing systems.

This guide demonstrates a practical workaround for this using [OpenCC](https://github.com/BYVoid/OpenCC), an open-source Chinese conversion tool. We'll show you how to implement a post-processing step that can normalize your transcription output to either consistent Simplified Chinese or Traditional Chinese, depending on your needs.

While this guide uses Python, OpenCC is available across multiple programming languages.

# Quickstart

```python
import assemblyai as aai
import opencc

aai.settings.api_key = "<YOUR-API-KEY>"

audio_file = "https://assembly.ai/chinese-interview.mp4"

config = aai.TranscriptionConfig(language_code="zh")

transcript = aai.Transcriber(config=config).transcribe(audio_file)

if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")

# t2s.json converts traditional characters to simplified
# use s2t.json to convert from simplified to traditional
converter = opencc.OpenCC('t2s.json')

simplified_transcript = converter.convert(transcript.text)

print(simplified_transcript)
```

# Step-by-step instructions

First, install the required packages:

1. AssemblyAI SDK
2. OpenCC

```bash
pip install -U assemblyai opencc
```

Import the necessary libraries and configure your API credentials:

```python
import assemblyai as aai
import opencc

aai.settings.api_key = "YOUR_API_KEY"
```

Specify your audio source and create a configuration for Chinese language transcription. Then submit your transcription request.

```python
audio_file = "https://assembly.ai/chinese-interview.mp4"

config = aai.TranscriptionConfig(language_code="zh")

transcript = aai.Transcriber(config=config).transcribe(audio_file)
```

Implement error handling to catch any transcription failures:

```python
if transcript.status == "error":
  raise RuntimeError(f"Transcription failed: {transcript.error}")
```

Apply script conversion using OpenCC with the appropriate configuration:

```python
# Script conversion options:
# - 't2s.json': Traditional to Simplified
# - 's2t.json': Simplified to Traditional

# Create converter object with desired direction
converter = opencc.OpenCC('t2s.json') # For Traditional to Simplified

# Convert the transcript text
simplified_transcript = converter.convert(transcript.text)
```

Output or save your converted transcript:

```python
print(simplified_transcript)

# Optionally save to file
with open("converted_transcript.txt", "w", encoding="utf-8") as f:
    f.write(converted_transcript)
```

## Conclusion

This guide demonstrates how to solve the common challenge of mixed Chinese script systems in transcription outputs. By combining AssemblyAI's powerful speech recognition capabilities with OpenCC's script conversion tools, you can create a reliable pipeline for producing consistently formatted Chinese text from audio sources.



---
title: Do More With Our SDKs
---

This guide will show you additional ways to make use of AssemblyAI's Python and JavaScript SDKs.

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## How to Check and Update Your Version of the SDK

Sometimes errors are encountered because the version of the SDK you are using is not up to date. To see which version you are currently running, type this code in your terminal:

<Tabs>
<Tab language="python" title="Python">
```python
pip show assemblyai
```

If this version is not the same as the current version of the [Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk) then you can update your version by typing this code in your terminal:

```python
pip install assemblyai --upgrade
```

</Tab>

<Tab language="javascript" title="JavaScript">
```js
npm info assemblyai version
```

If this version is not the same as the current version of the [JavaScript SDK](https://github.com/AssemblyAI/assemblyai-node-sdk) then you can update your version by typing this code in your terminal:

```js
npm update assemblyai --save
```

</Tab>
</Tabs>

## How to Catch and Log Errors

Catching and logging errors to the console is an easy way help you understand what is going wrong if the code does not run correctly.

<Tabs>
<Tab language="python" title="Python">
Underneath the line of code where the transcript is created, `transcript = transcriber.transcribe(audio_url, config)`, add the following code to catch and log any errors to the terminal:

```python
if transcript.error:
    print(transcript.error)
```

</Tab>

<Tab language="javascript" title="JavaScript">
Underneath the line of code where the transcript is created, `const transcript = await client.transcripts.transcribe(params)`, add the following code to catch and log any errors to the terminal:

```js
if (transcript.status === "error") {
  console.log(transcript.error);
}
```

</Tab>
</Tabs>

## How to Log the Transcript JSON and Save it in a File

<Tabs>
<Tab language="python" title="Python">
If using the error handling code above then add this below it, otherwise add it after the transcript is created, `transcript = transcriber.transcribe(audio_url, config)`:

```python
json_file = open('transcript.json', 'w', encoding='utf8')
json_str = json.dumps(transcript.json_response, ensure_ascii=False, indent=2)

json_file.write(json_str)
json_file.close()

print(json_str)
```

</Tab>

<Tab language="javascript" title="JavaScript">
In order to write data to a file, first import the [fs](https://nodejs.org/api/fs.html) package:

```js
import fs from "fs";
```

If using the error handling code above then add this below it, otherwise add it after the transcript is created, `const transcript = await client.transcripts.transcribe(params)`:

```js
const transcriptJSON = JSON.stringify(transcript, null, "\t");

fs.writeFile("transcript.json", transcriptJSON, (err) => {
  if (err) throw err;
});

console.log(transcriptJSON);
```

</Tab>
</Tabs>

## How to Log the Transcript ID and Retrieve a Previously Created Transcript

To log the transcript ID for a transcription, after the transcript is created and below any error handling, add the following code:

<Tabs>
<Tab language="python" title="Python">

To log the transcript ID for a transcription, after the transcript is created and below any error handling, add the following code:

```python
print(transcript.id)
```

Use the following code to retrieve a previous transcript:

```python
transcript = aai.Transcript.get_by_id("<TRANSCRIPT_ID>")

print(transcript.text)
```

You can also retrieve multiple transcripts, which are then returned in a single `TranscriptGroup` object:

```python
transcript_group = aai.TranscriptGroup.get_by_ids(["<TRANSCRIPT_ID_1>", "<TRANSCRIPT_ID_2>"])

for transcript in transcript_group:
    print(transcript.text)
```

</Tab>

<Tab language="javascript" title="JavaScript">
```js
console.log("Transcript ID: ", transcript.id)
```

To see a list of all previous transcriptions, use the following code:

```js
const allTranscriptsResponse = await fetch(
  "https://api.assemblyai.com/v2/transcript?limit=4",
  {
    method: "GET",
    headers: {
      Authorization: "<YOUR_API_KEY>",
    },
  }
);
const allTranscripts = await allTranscriptsResponse.json();
console.log(allTranscripts);
```

There are additional [query parameters](https://www.assemblyai.com/docs/api-reference/transcripts/list) that can be added to this request to limit the transcripts that are returned. The above example shows how to limit the number of returned transcripts to 4. These will be the four most recently created transcripts.

To get a specific transcript, use the following code:

```js
const transcriptResponse = await fetch(
  "https://api.assemblyai.com/v2/transcript/transcript_id",
  {
    method: "GET",
    headers: {
      Authorization: "<YOUR_API_KEY>",
    },
  }
);
const previousTranscript = await transcriptResponse.json();
console.log(previousTranscript);
```

Make sure when using the above code that you replace `transcript_id` in the url with the ID of the transcript you are looking to fetch.

</Tab>
</Tabs>



---
title: Setup An AI Coach With LeMUR
---

This tutorial will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to get AI coaching using the Task endpoint.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)

prompt = f"""
- You are an expert at providing valuable feedback to individuals.
- You possess exceptionally high emotional intelligence.
- You excel at analyzing the behavior of individuals in the given transcript and providing insights on how they could improve.
- You emphasize constructive criticism in your feedback.
- The feedback focuses on how people can better achieve their objectives.
- You avoid providing unjustified or unfounded feedback.
- Your communication is clear, accurate and concise, and you write with perfect English.
- Directly start with the feedback without any preamble or introduction.
"""

result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response
print(response)
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard. You will need to upgrade your account by adding a credit card to have access to LeMUR.

Find more details on the current LeMUR pricing in the AssemblyAI [pricing page](https://www.assemblyai.com/pricing#:~:text=LeMUR).

## Step-by-Step Instructions

In this guide, we'll prompt LeMUR to perform some AI coaching.

First, let's install the AssemblyAI SDK.

```bash
pip install -U assemblyai
```

Then we'll import the SDK and set our AssemblyAI API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript.

```python
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)
```

Provide detailed instructions to prompt LeMUR to provide feedback on your sales call, meeting, or other content.

```python
prompt = f"""
- You are an expert at providing valuable feedback to individuals.
- You possess exceptionally high emotional intelligence.
- You excel at analyzing the behavior of individuals in the given transcript and providing insights on how they could improve.
- You emphasize constructive criticism in your feedback.
- The feedback focuses on how people can better achieve their objectives.
- You avoid providing unjustified or unfounded feedback.
- Your communication is clear, accurate and concise, and you write with perfect English.
- Directly start with the feedback without any preamble or introduction.
"""
```

Prompt the LeMUR model using the Task Endpoint and return the response.

```python
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response
print(response)
```

---
title: Generate Action Items with LeMUR
---

This tutorial will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to create action items from a transcript using the Task endpoint.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)

prompt = f"""
Here are guidelines to follow:
- You are an expert at understanding transcripts of conversations, calls and meetings.
- You are an expert at coming up with ideal action items based on the contents of the transcripts.
- Action items are things that the transcript implies should get done.
- Your action item ideas do not make stuff up that isn't relevant to the transcript.
- You do not needlessly make up action items - you stick to important tasks.
- You are useful, true and concise, and write in perfect English.
- Your action items can be tied back to direct quotes in the transcript.
- You do not cite the quotes the action items relate to.
- The action items are written succinctly.
- Please give useful action items based on the transcript.
"""

answer_format = "Bullet Points"

if answer_format:
    prompt += f"\nYour response should have the following format: {answer_format}"

result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response
print(response)
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard. You will need to upgrade your account by adding a credit card to have access to LeMUR.

Find more details on the current LeMUR pricing in the AssemblyAI [pricing page](https://www.assemblyai.com/pricing#:~:text=LeMUR).

## Step-by-Step Instructions

In this guide, we will prompt LeMUR to generate action items from our transcript.

First, let's install the AssemblyAI SDK.

```bash
pip install -U assemblyai
```

Then we'll import the SDK and set our AssemblyAI API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript.

```python
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)
```

Provide detailed instructions to prompt LeMUR to create action items from the transcript.

```python
prompt = f"""
Here are guidelines to follow:
- You are an expert at understanding transcripts of conversations, calls and meetings.
- You are an expert at coming up with ideal action items based on the contents of the transcripts.
- Action items are things that the transcript implies should get done.
- Your action item ideas do not make stuff up that isn't relevant to the transcript.
- You do not needlessly make up action items - you stick to important tasks.
- You are useful, true and concise, and write in perfect English.
- Your action items can be tied back to direct quotes in the transcript.
- You do not cite the quotes the action items relate to.
- The action items are written succinctly.
- Please give useful action items based on the transcript.
"""
```

You can also optionally specify an action items format and append it to the prompt.

```python
answer_format = "Bullet Points"

if answer_format:
    prompt += f"\nYour response should have the following format: {answer_format}"
```

Prompt the LeMUR model using the Task Endpoint and return the response.

```python
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response
print(response)
```


---
title: Prompt A Structured Q&A Response Using LeMUR
---

This Colab will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to prompt a structured Question and Answer response using the Task Endpoint.

## Quickstart

```python
import assemblyai as aai
import xml.etree.ElementTree as ET

aai.settings.api_key = "YOUR_API_KEY"
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)

def construct_question(question):
    question_str = f"Question: {question.question}"

    if question.context:
        question_str += f"\nContext: {question.context}"

    # Set default answer_format to "short sentence" if not provided
    if not question.answer_format:
        question.answer_format = "short sentence"

    question_str += f"\nAnswer Format: {question.answer_format}"

    if question.answer_options:
        options_str = ", ".join(question.answer_options)
        question_str += f"\nOptions: {options_str}"

    return question_str + "\n"

def escape_xml_characters(xml_string):
    return xml_string.replace('&', '&amp;')

questions = [
    aai.LemurQuestion(
        question="What are the top level KPIs for engineering?",
        context="KPI stands for key performance indicator",
        answer_format="short sentence"),
    aai.LemurQuestion(
        question="How many days has it been since the data team has gotten updated metrics?",
        answer_options=["1", "2", "3", "4", "5", "6", "7", "more than 7"]),
    aai.LemurQuestion(
        question="What are the future plans for the project?")
]

question_str = '\n'.join(construct_question(q) for q in questions)

prompt = f"""You are an expert at giving accurate answers to questions about texts.
  No preamble.
  Given the series of questions, answer the questions.
  Each question may follow up with answer format, answer options, and context for each question.
  It is critical that you follow the answer format and answer options for each question.
  When context is provided with a question, refer to it when answering the question.
  You are useful, true and concise, and write in perfect English.
  Only the question is allowed between the <question> tag. Do not include the answer format, options, or question context in your response.
  Only text is allowed between the <question> and <answer> tags.
  XML tags are not allowed between the <question> and <answer> tags.
  End your response with a closing </response> tag.
  For each question-answer pair, format your response according to the template provided below:

Template for response:
<responses>
  <response>
    <question>The question</question>
    <answer>Your answer</answer>
  </response>
  <response>
    ...
  </response>
  ...
</responses>

These are the questions:
{question_str}
"""

result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response

# Escape special XML characters and strip any leading/trailing whitespace
clean_response = escape_xml_characters(response).strip()
root = ET.fromstring(clean_response)
for response in root.findall('response'):
    question = response.find('question').text
    answer = response.find('answer').text
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

Find more details on the current LeMUR pricing in the AssemblyAI [pricing page](https://www.assemblyai.com/pricing#:~:text=LeMUR).

## Step-by-Step Instructions

In this guide, we will prompt LeMUR with a structured Q&A format and generate an XML response.

First, let's install the AssemblyAI SDK.

```bash
pip install -U assemblyai
```

Then we'll import the SDK and set our AssemblyAI API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY_HERE"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript.

```python
audio_url = "https://storage.googleapis.com/aai-web-samples/meeting.mp4"

transcript = aai.Transcriber().transcribe(audio_url)
```

Construct a formatted string to structure the questions from the `LemurQuestion` object. This includes the question text, optional context, an answer format (defaulting to "short sentence" if not provided), and any answer options, then returns the formatted string.

```python
def construct_question(question):
    question_str = f"Question: {question.question}"

    if question.context:
        question_str += f"\nContext: {question.context}"

    # Set default answer_format to "short sentence" if not provided
    if not question.answer_format:
        question.answer_format = "short sentence"

    question_str += f"\nAnswer Format: {question.answer_format}"

    if question.answer_options:
        options_str = ", ".join(question.answer_options)
        question_str += f"\nOptions: {options_str}"

    return question_str + "\n"
```

Define a list of `aai.LemurQuestion` objects. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`.

```python
questions = [
    aai.LemurQuestion(
        question="What are the top level KPIs for engineering?",
        context="KPI stands for key performance indicator",
        answer_format="short sentence"),
    aai.LemurQuestion(
        question="How many days has it been since the data team has gotten updated metrics?",
        answer_options=["1", "2", "3", "4", "5", "6", "7", "more than 7"]),
    aai.LemurQuestion(
        question="What are the future plans for the project?")
]

```

Construct the formatted question string for all the Questions within the list of `aai.LemurQuestion` objects.

```python
question_str = '\n'.join(construct_question(q) for q in questions)
```

Provide detailed instructions to prompt LeMUR to answer a series of questions. This also defines a structured XML template for the responses.

```python
prompt = f"""You are an expert at giving accurate answers to questions about texts.
  No preamble.
  Given the series of questions, answer the questions.
  Each question may follow up with answer format, answer options, and context for each question.
  It is critical that you follow the answer format and answer options for each question.
  When context is provided with a question, refer to it when answering the question.
  You are useful, true and concise, and write in perfect English.
  Only the question is allowed between the <question> tag. Do not include the answer format, options, or question context in your response.
  Only text is allowed between the <question> and <answer> tags.
  XML tags are not allowed between the <question> and <answer> tags.
  End your response with a closing </response> tag.
  For each question-answer pair, format your response according to the template provided below:

Template for response:
<responses>
  <response>
    <question>The question</question>
    <answer>Your answer</answer>
  </response>
  <response>
    ...
  </response>
  ...
</responses>

These are the questions:
{question_str}
"""
```

Prompt the LeMUR model using the Task Endpoint and return the response.

```python
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

response = result.response
print(response)
```

Clean the XML output and print the question and answer pairs.

```python
import xml.etree.ElementTree as ET

def escape_xml_characters(xml_string):
    return xml_string.replace('&', '&amp;')

# Escape special XML characters and strip any leading/trailing whitespace
clean_response = escape_xml_characters(response).strip()

root = ET.fromstring(clean_response)

for response in root.findall('response'):
    question = response.find('question').text
    answer = response.find('answer').text
    print(f"Question: {question}")
    print(f"Answer: {answer}")
```


---
title: Estimate Input Token Costs for LeMUR
---

AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework is a powerful way to extract insights from transcripts generated from audio and video files. Given how varied the type of input and output could be for these use cases, the [pricing](https://www.assemblyai.com/pricing) for LeMUR is based on input and output tokens.

Output tokens can be controlled via LeMUR's [max_output_size](https://www.assemblyai.com/docs/lemur/advanced/customize-parameters#change-the-maximum-output-size) parameter, but how do you determine the amount of input tokens you'll be sending to LeMUR? How many tokens does an audio file contain? This Colab will show you how to calculate that information to help predict LeMUR's cost ahead of time.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3")
character_count = len(transcript.text)
count_in_thousands = character_count / 1000

sonnet_cost = 0.003 * count_in_thousands
opus_cost = 0.015 * count_in_thousands
haiku_cost = 0.00025 * count_in_thousands

print(f"LeMUR Claude 3.5 Sonnet | LeMUR Claude 3 Sonnet Cost: ${sonnet_cost}")
print(f"LeMUR Default | LeMUR Claude 2.1 | LeMUR Claude 3 Opus Cost: ${opus_cost}")
print(f"LeMUR Claude 3 Haiku Cost: ${haiku_cost}")
```

## Step-by-Step Guide

To get started, you'll need to install the AssemblyAI Python SDK, which we'll use to transcribe our file.

```bash
pip install -U assemblyai
```

Now we'll import these files and set our AssemblyAI API key, which can be found on your account [dashboard](https://www.assemblyai.com/app/account).

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Next we'll transcribe our file using AssemblyAI.

```python
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3")
```

LeMUR counts tokens based solely on character count, so we'll be using Python's built-in `len()` function to count the characters in our transcript.

```python
character_count = len(transcript.text)

print(character_count)
```

For this specific file, we got 4,928 tokens. LeMUR's pricing is calculated per 1K tokens, and is prorated for amounts below this.

```python
count_in_thousands = character_count / 1000

sonnet_cost = 0.003 * count_in_thousands
opus_cost = 0.015 * count_in_thousands
haiku_cost = 0.00025 * count_in_thousands

print(f"LeMUR Claude 3.5 Sonnet | LeMUR Claude 3 Sonnet Cost: ${sonnet_cost}")
print(f"LeMUR Default | LeMUR Claude 2.1 | LeMUR Claude 3 Opus Cost: ${opus_cost}")
print(f"LeMUR Claude 3 Haiku Cost: ${haiku_cost}")
```

Here we've determined how much the input tokens from our transcript will cost with LeMUR, not including the tokens of our prompt. To calculate how much the input tokens will cost for your prompt, or how much the amount of output tokens you're limited to will cost, you can follow this same method, replacing `transcript.text` with the text of your prompt, or `num_tokens` with the `max_output_size` amount you've specified to LeMUR.


---
title: Process Speaker Labels with LeMURs Custom Text Input Parameter
---

In this guide, we'll show you how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to process Speaker Labels from your transcript using the `input_text` parameter. The `input_text` option allows you to modify the transcripts before processing it using LeMUR, and in this example, format Speaker Labels in your LeMUR request.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
config = aai.TranscriptionConfig(speaker_labels=True)
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3", config=config)

text = ""

for utt in transcript.utterances:
    text += f"Speaker {utt.speaker}:\n{utt.text}\n"

result = aai.Lemur().task(
    '''
    This is a speaker-labeled transcript of a podcast episode between Michel Martin, NPR host, and Peter DeCarlo, a professor at Johns Hopkins University.
    Please answer the following questions:
    1) Who is Speaker A and Speaker B?
    2) What questions did the podcast host ask?
    3) What were the main concerns of the podcast guest?"
    ''',
    input_text=text,
    final_model=aai.LemurModel.claude3_5_sonnet,
)

print(result.response)
```

<Info>
  Calling LeMUR using `transcript_ids` is preferred as default. Depending on
  your use case, you can alternatively use the `input_text` parameter to call
  LeMUR with custom formatted transcript data including edited transcripts,
  speaker-labelled transcripts and more.
</Info>

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for an account and get your API key from your dashboard.

LeMUR features are currently only available to paid users, at two pricing tiers: LeMUR and LeMUR Basic. See [pricing](https://www.assemblyai.com/pricing) for more details.

## Step-by-Step Instructions

In this guide, we will include Speaker Labels to the conversation by using the `input_text` parameter. We will use the Custom Task endpoint to prompt LeMUR. You can use any LeMUR Endpoint and adjust the prompt parameters to suit your project's needs.

Install the SDK.

```python
pip install -U assemblyai
```

Import the `assemblyai` package and set your API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Create a `TranscriptionConfig` with `speaker_labels` set to `True`.

```python
config = aai.TranscriptionConfig(speaker_labels=True)
```

Use the `Transcriber` object's `transcribe` method and parse the configuratin and the audio file's path as a parameter . The `transcribe` method will save the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3", config=config)
```

Create an empty string variable `text` to store the output. Iterate through each utterance in the transcript to append the formatted Speaker Labels to be used as the `input_text` LeMUR parameter.

```python
text = ""

for utt in transcript.utterances:
    text += f"Speaker {utt.speaker}:\n{utt.text}\n"
```

Run `Lemur`'s `task` method on your transcript and parse the `prompt` and `input_text` parameters. The `text` variable contains the formatted Speaker Labels and Text response from your transcript.

The result is stored in `response` as a string.

```python
result = aai.Lemur().task(
    '''
    This is a speaker-labeled transcript of a podcast episode between Michel Martin, NPR host, and Peter DeCarlo, a professor at Johns Hopkins University.
    Please answer the following questions:
    1) Who is Speaker A and Speaker B?
    2) What questions did the podcast host ask?
    3) What were the main concerns of the podcast guest?"
    ''',
    input_text=text,
    final_model=aai.LemurModel.claude3_5_sonnet,
)

print(result.response)
```

The output will look similar to the example below.


---
title: Identify Speaker Names From the Transcript Using LeMUR
---

In this guide, you'll learn how to use AssemblyAI's API to transcribe audio, identify speakers, and infer their names using LeMUR. We'll walk through the process of configuring the transcriber, submitting a transcript to LeMUR with speaker labels, and generating a mapping of speaker names from the transcript.

This workflow will enable you to have speaker labels with the speaker's name in your transcripts instead of `Speaker A/B`.

## Before you begin

To complete this tutorial, you need:

- [Python](https://www.python.org/) installed.
- An upgraded [AssemblyAI account](https://www.assemblyai.com/dashboard/signup).

```bash
pip install assemblyai
```

Import the `assemblyai` and `re` packages and set your API key:

```python
import assemblyai as aai
import re

aai.settings.api_key = "YOUR-API-KEY"
```

Define a `Transcriber`, a `TranscriptionConfig` with `speaker_labels` set to `True`. Then, create a transcript.

```python
transcriber = aai.Transcriber()

audio_url = (
    "https://www.listennotes.com/e/p/accd617c94a24787b2e0800f264b7a5e/"
)

config = aai.TranscriptionConfig(speaker_labels=True)
transcript = transcriber.transcribe(audio_url, config)
```

Process the transcript with speaker labels:

```python
text_with_speaker_labels = ""

for utt in transcript.utterances:
    text_with_speaker_labels += f"Speaker {utt.speaker}:\n{utt.text}\n"
```

Count the unique speakers, then create a `LemurQuestion` for each speaker. Lastly, ask LeMUR the questions, specifying `text_with_speaker_labels` as the `input_text`.

```python
# Count the number of unique speaker labels
unique_speakers = set(utterance.speaker for utterance in transcript.utterances)

questions = []
for speaker in unique_speakers:
    questions.append(
        aai.LemurQuestion(
        question=f"Who is speaker {speaker}?",
        answer_format="<First Name> <Last Name (if applicable)>")

    )

result = aai.Lemur().question(
    questions,
    input_text=text_with_speaker_labels,
    final_model=aai.LemurModel.claude3_5_sonnet,
    context="Your task is to infer the speaker's name from the speaker-labelled transcript"
)
```

Map the speaker alphabets to their names from LeMUR:

```python
speaker_mapping = {}

for qa_response in result.response:
    pattern = r"Who is speaker (\w)\?"
    match = re.search(pattern, qa_response.question)
    if match and match.group(1) not in speaker_mapping.keys():
        speaker_mapping.update({match.group(1): qa_response.answer})
```

Print the Transcript with Speaker Names:

```python
for utterance in transcript.utterances[:10]:
   speaker_name = speaker_mapping[utterance.speaker]
   print(f"{speaker_name}: {utterance.text[:50]}...")
```


---
title: Extract Dialogue Data with LeMUR and JSON
---

In this guide, we'll show you how to use AssemblyAI's LeMUR (Leveraging Large Language Models to Understand Recognized Speech) framework to process several audio files, and then format your results in JSON (JavaScript Object Notation) format.

JSON allows you to programmatically format, parse, and transfer resopnses from LeMUR, which is useful for implementing LeMUR with a wide range of other applications.

In this example, we will leverage the JSON formatting to create a .csv file from a directory of files that must be transcribed and submitted to LeMUR. However, you can use the same concepts in this guide to generate a JSON-formatted response, which you can then use to update a database table or interact with other APIs.

## Quickstart

```python
import assemblyai as aai
import json
import os
import csv

aai.settings.api_key = "your_api_key"
# configure the name of your output .csv file
output_filename = "profiles.csv"

transcriber = aai.Transcriber()
# Create a new Transcript Group with every file in the directory "interviews"
transcript_group = transcriber.transcribe_group([os.path.join("interviews", file) for file in os.listdir("interviews")])

prompt = """
         You are an HR executive scanning through an interview transcript to extract information about a candidate.
         You are required to create a JSON response with key information about the candidate.
         You will use this template for your answer:
         {
            "Name": "<candidate-name>",
            "Position": "<job position that candidate is applying for>",
            "Past experience": "<A short phrase describing the candidate's relevant past experience for the role>",
         }

         Do not include any other text in your response. Only respond in JSON format, as your response will be parsed programmatically as JSON.
         """

with open(output_filename, "w", newline="") as file:
    writer = csv.writer(file)
    # define the header of your .csv file
    header = ["Name", "Position", "Past Experience"]
    writer.writerow(header)

    print("Prompting LeMUR")
    for transcript in transcript_group:
        result = transcript.lemur.task(prompt=prompt, final_model=aai.LemurModel.claude3_5_sonnet)
        # json.loads() method can be used to parse a valid JSON string and convert it into a Python dictionary.
        interviewee_data = json.loads(result.response)
        writer.writerow(interviewee_data.values())

print(f"Created .csv file {output_filename}")
```

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

LeMUR features are currently only available to paid users to two pricing tiers: LeMUR and LeMUR Basic. See [pricing](https://www.assemblyai.com/pricing#:~:text=LeMUR) for more detail.

## Step-by-Step Instructions

In this guide, we will ask the same questions to LeMUR about multiple files. Then, we will collate the answers in a .csv file.

Import the necessary libraries for making an HTTP request and set your API key.

```python
import assemblyai as aai
import json
import os
import csv

aai.settings.api_key = "your_api_key"
# configure the name of your output .csv file
output_filename = "profiles.csv"
```

Transcribe your audio files.

```python
transcriber = aai.Transcriber()
# Create a new Transcript Group with every file in the directory "interviews"
transcript_group = transcriber.transcribe_group([os.path.join("interviews", file) for file in os.listdir("interviews")])
```

Define your LeMUR request prompt for the Task feature.

```python
prompt = """
         You are an HR executive scanning through an interview transcript to extract information about a candidate.
         You are required to create a JSON response with key information about the candidate.
         You will use this template for your answer:
         {
            "Name": "<candidate-name>",
            "Position": "<job position that candidate is applying for>",
            "Past experience": "<A short phrase describing the candidate's relevant past experience for the role>",
         }

         Do not include any other text in your response. Only respond in JSON format, as your response will be parsed programmatically as JSON.
         """
```

Construct your .csv file and parse the JSON data.

```python
with open(output_filename, "w", newline="") as file:
    writer = csv.writer(file)
    # define the header of your .csv file
    header = ["Name", "Position", "Past Experience"]
    writer.writerow(header)

    print("Prompting LeMUR")
    for transcript in transcript_group:
        result = transcript.lemur.task(prompt=prompt, final_model=aai.LemurModel.claude3_5_sonnet)
        # json.loads() method can be used to parse a valid JSON string and convert it into a Python dictionary.
        interviewee_data = json.loads(result.response)
        writer.writerow(interviewee_data.values())

print(f"Created .csv file {output_filename}")
```

For context, this is the response from LeMUR with our prompt.

```json
{
  "Name": "John Smith",
  "Position": "software engineer",
  "Past experience": "three years of experience at Google"
}
```

You can now run your Python script and you should see that a `profiles.csv` file is generated. Your result will look similar to the example below.


---
title: Extract Quotes with Timestamps Using LeMUR + Semantic Search
---

This Colab will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to process an audio file and find the best quotes included in it through [Semantic Search](https://www.elastic.co/what-is/semantic-search).

## Quickstart

```python
import datetime
import numpy as np
import assemblyai as aai
from sklearn.neighbors import NearestNeighbors
from sentence_transformers import SentenceTransformer

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("URL_OR_FILE_PATH_HERE")

embedder = SentenceTransformer("multi-qa-mpnet-base-dot-v1")

embeddings = {}
sentences = transcript.get_sentences()

def sliding_window(elements, distance, stride):
    idx = 0
    results = []
    while idx + distance < len(elements):
        results.append(elements[idx:idx + distance])
        idx += (distance - stride)
    return results

# Sliding window to determine length of sentence groups. Tune based on desired quote length and duration.
sentence_groups = sliding_window(sentences, 5, 2)

for sentence_group in sentence_groups:
    sentence = {
        "text": " ".join([sentence.text for sentence in sentence_group]),
        "start": sentence_group[0].start,
        "end": sentence_group[-1].end,
    }
    embeddings[(sentence["start"], sentence["end"], transcript.id, sentence["text"])] = embedder.encode(sentence["text"])

questions = [
    aai.LemurQuestion(
        question="What are the 3 best quotes from this video?",
        context="Please provide exactly 3 quotes.",
    )
]

qa_results = transcript.lemur.question(questions, final_model=aai.LemurModel.claude3_5_sonnet).response

# Embed the output from LeMUR for use in our comparison.
lemur_embedding = embedder.encode(qa_results[0].answer)

# Vectorize our initial transcript embeddings.
np_embeddings = np.array(list(embeddings.values()))
metadata = list(embeddings.keys())

# Find the top 3 most similar quotes to what LeMUR surfaced.
knn = NearestNeighbors(n_neighbors=3, metric="cosine")
knn.fit(np_embeddings)
distances, indices = knn.kneighbors([lemur_embedding])

matches = []
for distance, index in zip(distances[0], indices[0]):
    result_metadata = metadata[index]
    matches.append(
        {
            "start_timestamp": result_metadata[0],
            "end_timestamp": result_metadata[1],
            "transcript_id": result_metadata[2],
            "text": result_metadata[3],
            "confidence": 1 - distance,
        }
    )

for index, m in enumerate(matches):
    print('QUOTE #{}: "{}"'.format(index + 1, m['text']))
    print('START TIMESTAMP:', str(datetime.timedelta(seconds=m['start_timestamp']/1000)))
    print('END TIMESTAMP:', str(datetime.timedelta(seconds=m['end_timestamp']/1000)))
    print('CONFIDENCE:', m['confidence'])
    print()
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://www.assemblyai.com/dashboard/signup) for an AssemblyAI account and get your API key from your [dashboard](https://www.assemblyai.com/app/account).

You'll also need to install a few libraries that this code depends on:

- The AssemblyAI [Python SDK](https://github.com/AssemblyAI/assemblyai-python-sdk).
- [Numpy](https://numpy.org/), a scientific computing library.
- [Sciki-Learn](https://scikit-learn.org/stable/), a library for predictive data analysis.
- [Sentence-Transformers](https://www.sbert.net/index.html), a framework for state-of-the-art sentence and text embedding.

## Step-by-Step Instructions

```bash
pip install -U assemblyai numpy scikit-learn sentence-transformers
```

Then we'll import all of these libraries and set our AssemblyAI API key.

```python
import datetime
import numpy as np
import assemblyai as aai
from sklearn.neighbors import NearestNeighbors
from sentence_transformers import SentenceTransformer

aai.settings.api_key = "API_KEY_HERE"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript for later use.

```python
transcriber = aai.Transcriber()

transcript = transcriber.transcribe("URL_OR_FILE_PATH_HERE")
```

Now we can iterate over all of the paragraphs in our transcript and create [embeddings](https://github.com/microsoft/semantic-kernel/blob/main/docs/EMBEDDINGS.md) for them to use as part of our Semantic Search later.

We'll be relying on SentenceTransformer's `multi-qa-mpnet-base-dot-v1` model, which has been fine-tuned specifically for Semantic Search, and is their [highest-performing model](https://www.sbert.net/docs/pretrained_models.html) for this task.

We'll also be implementing a [sliding window](https://blandthony.medium.com/methods-for-semantic-text-segmentation-prior-to-generating-text-embeddings-vectorization-6442afdb086), which allows us to group sentences together in different combinations to retain their semantic meaning and context while also enabling us to customize the length (and thus duration) of the quotes. By default, we'll group 5 sentences together while having 2 of them overlap when the window moves. This should give us quotes around 30 seconds in length at most.

```python
embedder = SentenceTransformer("multi-qa-mpnet-base-dot-v1")

embeddings = {}
sentences = transcript.get_sentences()

def sliding_window(elements, distance, stride):
    idx = 0
    results = []
    while idx + distance < len(elements):
        results.append(elements[idx:idx + distance])
        idx += (distance - stride)
    return results

# Sliding window to determine length of sentence groups. Tune based on desired quote length and duration.
sentence_groups = sliding_window(sentences, 5, 2)

for sentence_group in sentence_groups:
    sentence = {
        "text": " ".join([sentence.text for sentence in sentence_group]),
        "start": sentence_group[0].start,
        "end": sentence_group[-1].end,
    }
    embeddings[(sentence["start"], sentence["end"], transcript.id, sentence["text"])] = embedder.encode(sentence["text"])
```

Now we can query LeMUR to provide the type of quotes we want. In this case, let's prompt LeMUR to find the best 3 quotes out of a video that we transcribed.

```python
questions = [
    aai.LemurQuestion(
        question="What are the 3 best quotes from this video?",
        context="Please provide exactly 3 quotes.",
    )
]

qa_results = transcript.lemur.question(questions, final_model=aai.LemurModel.claude3_5_sonnet).response
```

Now we can take the embeddings from the transcript text, as well as the embeddings from LeMUR's output, and use them in our [k-nearest neighbors](https://www.ibm.com/topics/knn) algorithm to determine their similarity. The most similar quotes to what LeMUR identified will be surfaced as our 3 best quotes, along with their timestamps and confidence scores.

We'll be relying on [cosine similarity](https://github.com/microsoft/semantic-kernel/blob/main/docs/COSINE_SIMILARITY.md) rather than the default Euclidean distance metric since it takes into account both the magnitude and direction of our vectors.

```python
# Embed the output from LeMUR for use in our comparison.
lemur_embedding = embedder.encode(qa_results[0].answer)

# Vectorize our initial transcript embeddings.
np_embeddings = np.array(list(embeddings.values()))
metadata = list(embeddings.keys())

# Find the top 3 most similar quotes to what LeMUR surfaced.
knn = NearestNeighbors(n_neighbors=3, metric="cosine")
knn.fit(np_embeddings)
distances, indices = knn.kneighbors([lemur_embedding])

matches = []
for distance, index in zip(distances[0], indices[0]):
    result_metadata = metadata[index]
    matches.append(
        {
            "start_timestamp": result_metadata[0],
            "end_timestamp": result_metadata[1],
            "transcript_id": result_metadata[2],
            "text": result_metadata[3],
            "confidence": 1 - distance,
        }
    )

for index, m in enumerate(matches):
    print('QUOTE #{}: "{}"'.format(index + 1, m['text']))
    print('START TIMESTAMP:', str(datetime.timedelta(seconds=m['start_timestamp']/1000)))
    print('END TIMESTAMP:', str(datetime.timedelta(seconds=m['end_timestamp']/1000)))
    print('CONFIDENCE:', m['confidence'])
    print()
```


---
title: Extract Transcript Quotes with LeMURs Custom Text Input Parameter
---

This Colab will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to process an audio file and find the best quotes included in it by sending in the timestamped transcript via LeMUR's `input_text` parameter.

## Quickstart

```python
import assemblyai

assemblyai.settings.api_key = "YOUR_API_KEY"
transcriber = assemblyai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3")

result = assemblyai.Lemur().question(
    input_text=f"{[(sentence.text, sentence.start, sentence.end) for sentence in transcript.get_sentences()]}",
    final_model=assemblyai.LemurModel.claude3_5_sonnet,
    questions=[
        {
            "question": "What are the most engaging quotes from this transcript?",
            "context": "This is a list of sentences from the transcript, with each sentence having a start and end timestamp in milliseconds.",
            "answer_format": "Exact quotes with the start and end timestamp of each sentence."
        }
    ]
)

print(result.response[0].answer)
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://www.assemblyai.com/dashboard/signup/) for an AssemblyAI account and get your API key from your [dashboard](https://www.assemblyai.com/app/account).

## Step-by-Step Instructions

First, let's install the AssemblyAI SDK.

```bash
pip install -U assemblyai
```

Then we'll import the SDK and set our AssemblyAI API key.

```python
import assemblyai

assemblyai.settings.api_key = "API_KEY_HERE"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript for later use.

```python
transcriber = assemblyai.Transcriber()

transcript = transcriber.transcribe("https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3")
```

Then we'll take the timestamped `sentences` array from our transcript and provide it to the LeMUR Q&A endpoint to extract the most engaging quotes from this transcript with their associated timestamps.

```python
result = assemblyai.Lemur().question(
    input_text=f"{[(sentence.text, sentence.start, sentence.end) for sentence in transcript.get_sentences()]}",
    final_model=assemblyai.LemurModel.claude3_5_sonnet,
    questions=[
        {
            "question": "What are the most engaging quotes from this transcript?",
            "context": "This is a list of sentences from the transcript, with each sentence having a start and end timestamp in milliseconds.",
            "answer_format": "Exact quotes with the start and end timestamp of each sentence."
        }
    ]
)

print(result.response[0].answer)
```

Example response:

```
("So the concentrations of these particles in the air are just much, much higher than we typically see.", 113338, 119698)
("And exposure to those high levels can lead to a host of health problems.", 119784, 123314)
("I think New York has some of the higher concentrations right now, but that's going to change as that air moves away from the New York area.", 170950, 176930)
("Looking into the future, the fire season is starting earlier and lasting longer and we're seeing more frequent fires.", 245216, 251254)
```


---
title: Generate Transcript Citations using LeMUR
---

This guide will walk through the process of generating transcript citations using OpenAI embeddings and the LeMUR API.

## Overview

Extracting exact quotes from transcripts can be a difficult task for Large Language Models, which makes it challenging to cite sources or identify timestamps for generative text.

Embeddings are powerful representations of text that capture its semantic and contextual meaning. By leveraging embeddings, we can transform raw text data, such as transcripts, into dense numerical vectors that encode its underlying information. These embeddings enable us to perform sophisticated tasks such as similarity comparison and contextual searching.

In this guide, we demonstrate how to utilize OpenAI embeddings to retrieve transcript citations to corroborate the results from the LeMUR API. LeMUR is proficient at providing the 'what' & 'why' and now embeddings will be able to provide the 'where' & 'when'.

We'll walk through 3 use cases for this including verification of sources for specific answers, timestamping of action items, and generation of customer quotes.

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for an account and get your API key from your dashboard. You will also need an [OpenAI API token](https://platform.openai.com/docs/quickstart).

LeMUR features are currently only available to paid users. See [pricing](https://www.assemblyai.com/pricing) for more details.

## Instructions

Install the libraries required for the transcription and embedding creation.

```bash
pip install numpy sklearn openai assemblyai tiktoken
```

### Submitting a File for Transcription

```python
import assemblyai as aai
aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()

def transcribe(urls):
    return transcriber.transcribe_group(urls)
```

### Create Transcript Embeddings

We are using the text-embedding-ada-002 model to generate our embeddings.

The pricing for this model is $0.0001 / 1k tokens which equates to roughly 0.0015 to embed one hour of audio.

```python
import numpy as np
from sklearn.neighbors import NearestNeighbors
import openai

# Set up OpenAI API key
openai.api_key = "YOUR_OPENAI_TOKEN"

def embed_block(block_text):
    # Embed the block of text using OpenAI embeddings
    embedding = openai.Embedding.create(
        input=block_text,
        model='text-embedding-ada-002',
    ).to_dict()['data'][0]['embedding']

    # Store the embedding with the timestamp in the dictionary
    return embedding

def find_relevant_matches(embedded_blocks, new_block_text, k=3):
    matches = []
    # Embed the new block of text using OpenAI embeddings
    new_embedding = embed_block(new_block_text)

    # Prepare the embeddings for the KNN search
    embeddings = np.array(list(embedded_blocks.values()))
    metadata = list(embedded_blocks.keys())

    # Perform KNN search to find the most relevant matches
    knn = NearestNeighbors(n_neighbors=k)
    knn.fit(embeddings)
    distances, indices = knn.kneighbors([new_embedding])

    # Print the relevant matches
    # print(f"Relevant Matches for '{new_block_text}':")
    for distance, index in zip(distances[0], indices[0]):
        result_metadata = metadata[index]
        # print(f"Timestamp: {timestamp}, Similarity: {1-distance:.4f}")
        # print(f"Block Text: {embedded_blocks[timestamp]}")
        # print()
        matches.append({
            'timestamp': result_metadata[0],
            'transcript_id': result_metadata[1],
            'text': result_metadata[2],
            'confidence': 1-distance
        })
    return matches
```

```python
def create_transcripts_embeddings(transcripts, granularity='paragraph'):
    # Dictionary to store embeddings with timestamps
    embeddings = {}
    total_tokens_embedded = 0

    for transcript in transcripts:
        if granularity == 'sentence':
            sentences = transcript.get_sentences()
            for sentence in sentences:
                # print(sentence.start, sentence.end)
                # print(sentence.text)
                total_tokens_embedded += num_tokens_from_string(sentence.text, 'r50k_base')

                embeddings[(sentence.start, transcript.id, sentence.text)] = embed_block(sentence.text)
        else:
            paragraphs = transcript.get_paragraphs()
            for paragraph in paragraphs:
                # print(paragraph.start, paragraph.end)
                # print(paragraph.text, '\n')
                total_tokens_embedded += num_tokens_from_string(paragraph.text, 'r50k_base')

                embeddings[(paragraph.start, transcript.id, paragraph.text)] = embed_block(paragraph.text)

    print(total_tokens_embedded, 'TOKENS EMBEDDED')
    print('COST OF EMBEDDINGS: $', (total_tokens_embedded / 1000)*0.0001)
    print()
    return embeddings
```

```python
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens
```

## Examples

### Cite Answers to Specific Questions

Cite your sources to specific answers returned from the LeMUR Q&A API.

```python
questions = [
    aai.LemurQuestion(question="how does calcium relate to adheren junctions?", context='', answer_format="")
]
```

```python
import json, datetime

def get_citations(lemur_output):
    matches = find_relevant_matches(embeddings, lemur_output)

    print('CITATIONS:')
    for index, m in enumerate(matches):
        print('#{}'.format(index+1))
        print('QUOTE: "{}"'.format(m['text']))
        print('TRANSCRIPT ID:', m['transcript_id'])
        print('START TIMESTAMP:', str(datetime.timedelta(seconds=m['timestamp']/1000)))
        print('CONFIDENCE SCORE:', m['confidence'])
        print()
```

```python
transcripts = transcribe([
    '', # TODO ADD URLS
])

embeddings = create_transcripts_embeddings(transcripts)

qa_results = transcripts.lemur.question(questions).response

print(f"Question: {qa_results[0].question}")
print(f"Answer: {qa_results[0].answer}")
print()
get_citations(qa_results[0].question + ' ' + qa_results[0].answer)
```

Example output:

```
Question: how does calcium relate to adheren junctions?
Answer: Adheren junctions are calcium dependent, meaning that if calcium is removed, the cells will fall apart as the junctions disassemble.

CITATIONS:
#1
QUOTE: "If you were to put in some kind of calcium chelator like EDTA that removed the calcium from the media or the extracellular fluid in these cells, these cells would actually fall apart and these junctions would fall apart. On the cytoplasmic side, you have a number of linker proteins. Again, they've been named in this diagram, catinin Vinculin and Alpha Actinin are involved, and again, they link up to the actin filaments."
TRANSCRIPT ID: 6yb0ijyfl0-14c4-4bc1-96f2-ff029bb7e630
START TIMESTAMP: 0:16:57.786000
CONFIDENCE SCORE: 0.5054862317894155

#2
QUOTE: "If you have two cells that are attached to one another and they're undergoing physical forces, what keeps them from coming apart is these adhering junctions. There are forces you can imagine in your intestine that would rub against the epithelia as material passes through. And these adhering junctions keep those epithelia from coming apart and exposing the connected tissue below. Gap junctions are found in most cells and their real function is actually as a pore or channel that lies between two adjacent cells. And these allow for small molecules to pass ions to pass through and they're controlled pores."
TRANSCRIPT ID: 6yb0ijyfl0-14c4-4bc1-96f2-ff029bb7e630
START TIMESTAMP: 0:08:28.270000
CONFIDENCE SCORE: 0.4346457600791962

#3
QUOTE: "And we take a look at a schematic showing the intracellular surface of one cell and the intracellular surface of another. Here are the plasma membranes of one and cell two, and then the space in between. The space is about 20. Unlike the cludence junction, there is a space, and that space contains transmembrane proteins called caherons, which match up with their homolog on the adjacent cell, and they hold the two cells together. These adherence junctions are calcium dependent."
TRANSCRIPT ID: 6yb0ijyfl0-14c4-4bc1-96f2-ff029bb7e630
START TIMESTAMP: 0:16:23.290000
CONFIDENCE SCORE: 0.4293010412511604
```

### Provide References to Multiple Transcripts

When analyzing multiple transcripts, it can be helpful to have references to know which transcript the answer came from.

```python
questions = [
    aai.LemurQuestion(question="Identify pain points discussed across all user interviews", context='', answer_format="""
    [
        "pain point 1",
        "pain point 2",
        "pain point 3"
    ]
    """)
]
```

```python
import json, datetime

def get_examples(lemur_output):
    matches = find_relevant_matches(embeddings, lemur_output, k=5)

    print('EXAMPLES:')
    for index, m in enumerate(matches):
        print('#{}'.format(index+1))
        print('QUOTE: "{}"'.format(m['text']))
        print('TRANSCRIPT ID:', m['transcript_id'])
        print('START TIMESTAMP:', str(datetime.timedelta(seconds=m['timestamp']/1000)))
        print('CONFIDENCE SCORE:', m['confidence'])
        print()
    return matches
```

```python
transcripts = transcribe([
    '', # TODO ADD URLS
])

embeddings = create_transcripts_embeddings(transcripts, granularity='sentence')

qa_results = transcripts.lemur.question(questions).response

print(f"Question: {qa_results[0].question}")
print(f"Answer: {qa_results[0].answer}")
print()

pain_point_array = json.loads(qa_results[0].answer.strip())
for pp in pain_point_array:
    print('Pain Point:', pp)
    get_examples(pp)
```

Example output:

```
Question: Identify pain points discussed across all user interviews
Answer: [
"Communication challenges due to the remote nature of the team",
"Lack of alignment across different tools and preferences",
"Losing key documents and data points when employees leave the company"
]

Pain Point: Communication challenges due to the remote nature of the team
EXAMPLES:
#1
QUOTE: "And so sometimes that comes with its communication challenges and people working in different time zones."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:03:03.930000
CONFIDENCE SCORE: 0.5171660164618275

#2
QUOTE: "We are remote, so virtual whiteboard."
TRANSCRIPT ID: 6ybb50tnns-785d-4361-ad15-b0ef5db075e6
START TIMESTAMP: 0:04:11.792000
CONFIDENCE SCORE: 0.44395501749808775

#3
QUOTE: "So I think that has been some of the larger challenges with our team."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:04:19.364000
CONFIDENCE SCORE: 0.43930383535763384

#4
QUOTE: "No, I think that I would sum it up in a way that I would say just essentially that I had already mentioned that communication and one single spot for us to collaborate and communicate in is already a challenge."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:25:35.388000
CONFIDENCE SCORE: 0.43763248883808603

#5
QUOTE: "How would you collaborate with your team?"
TRANSCRIPT ID: 6ybb50cj8c-877e-4314-9547-3e1450cf08f7
START TIMESTAMP: 0:20:20.196000
CONFIDENCE SCORE: 0.4267756277366459

Pain Point: Lack of alignment across different tools and preferences
EXAMPLES:
#1
QUOTE: "What creates a challenge is that there are people that are more proficient or more comfortable working within certain apps than others are."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:03:39.838000
CONFIDENCE SCORE: 0.42358740588333854

#2
QUOTE: "Again, I think it's between tools that we use all the way to just the different time zones in everyone's schedule that sometimes it causes a delay in projects getting done or slow up or bottlenecks."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:03:11.950000
CONFIDENCE SCORE: 0.4154978400007534

#3
QUOTE: "And so if we're working all from one tool, I think it's more clear because I don't think people also revert back to their Google Drive often."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:24:56.418000
CONFIDENCE SCORE: 0.3742980144030581

#4
QUOTE: "Some people are communicating with a preference of slack first, while others are email first."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:04:14.762000
CONFIDENCE SCORE: 0.3704920121143226

#5
QUOTE: "And so sometimes that comes with its communication challenges and people working in different time zones."
TRANSCRIPT ID: 6ybb50o8da-9dda-429b-b576-3b61c98a4fc3
START TIMESTAMP: 0:03:03.930000
CONFIDENCE SCORE: 0.3631327779909689
```

### Identify Timestamps For Action Items

Quickly jump to the part of the meeting where the action item was discussed.

```python
action_item_answer_format="""[{
    "action_item":<action item>,
    "assignee":<assignee>,
    "quote":"<leave blank>",
    "timestamp":"<leave blank>"
    }]
"""

action_item_context = ''
```

```python
import json, datetime

def timestamp_action_items(action_items_array):

    for action_item in action_items_array:
        matches = find_relevant_matches(embeddings, action_item['action_item'], k=1)
        for index, m in enumerate(matches):
            action_item['quote'] = m['text']
            action_item['timestamp'] = m['timestamp']
    return action_items_array
```

```python
transcripts = transcribe([
    '', # TODO add file URLs here
])

# TODO: choose granularity either sentence or paragraph
embeddings = create_transcripts_embeddings(transcripts, 'paragraph')

action_item_results = transcripts.lemur.action_items(
    context=action_item_context,
    answer_format=action_item_answer_format).response

# Replace preamble in LeMUR response
action_item_results = action_item_results.replace('Here are action items based on the transcript:', '')

action_item_json_array = json.loads(action_item_results.strip())
action_item_json_array = timestamp_action_items(action_item_json_array)
print(json.dumps(action_item_json_array, indent=4))
```

Example output:

```
552 TOKENS EMBEDDED
COST OF EMBEDDINGS: $ 5.520000000000001e-05

[
    {
        "action_item": "Schedule a follow up call with Daniel to continue the conversation.",
        "assignee": "Rich",
        "quote": "I tell you what. I'll let you jump on that call. No sweat at all. I understand. I'll drop you a mail and we'll find a time to talk next week.",
        "timestamp": 237070
    },
    {
        "action_item": "Send an email to Daniel with availability for a call next week.",
        "assignee": "Rich",
        "quote": "I tell you what. I'll let you jump on that call. No sweat at all. I understand. I'll drop you a mail and we'll find a time to talk next week.",
        "timestamp": 237070
    },
    {
        "action_item": "Review financials and metrics for Avail to prepare for the follow up call.",
        "assignee": "Rich",
        "quote": "We are in the health and wellness space, so our space heated up extremely fast, and I found myself working 40 hours in one job and the other. So I had to make a decision. And I think there's a little bit more potential and upside with avail. Great. And so you've seen just an increased demand for your product then, over the past six weeks?",
        "timestamp": 138816
    }
]
```


---
title: Implement a Sales Playbook Using LeMUR
---

This guide will show you how to use AssemblyAI's LeMUR framework to implement a sales playbook with a call from a sales representative to a client.

This guide aims to show different ways of using the `question-answer` feature with a hypothetical sales use case to produce personalized, precise responses. Using this feature, a user can immediately evaluate large numbers of sales calls and ensure that prospecting steps are followed, including quotes in the response, which can inform future sales by identifying trends and quantitative performance tracking.

In this example, we will demonstrate how to use request variables such as context, answer_format, and answer_options to make the most of [LeMUR's Question & Answer feature](https://www.assemblyai.com/docs/api-reference/lemur). You can use the concepts in this guide to create custom specifications to evalute your sales representatives.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()
transcript = transcriber.transcribe("./sales-call.mp3") # You can also provide a URL to a publicly available audio file

context = "There are sales interactions between a salesperson who is selling an internet plan to customers who are warm leads."
answer_format = """
                Answer with JSON in the following format:
                {
                    "Answer: "<answer_options>",
                    "Reason": "<justification for the answer in one sentence including quotes>"
                }
                """

answer_options = ["Yes", "No"]

questions = [
    aai.LemurQuestion(
        question="Did the salesperson start the conversation with a professional greeting?",
        context=context,
        answer_format=answer_format,
        answer_options=["Poor", "Satisfactory", "Excellent"],
    ),
    aai.LemurQuestion(
        question="How well did the salesperson answer questions during the call?",
        context=context,
        answer_format=answer_format,
    ),
    aai.LemurQuestion(
        question="Did the salesperson discuss next steps clearly?",
        context=context,
        answer_format=answer_format,
        answer_options=answer_options,
    ),
]

result = transcript.lemur.question(questions, final_model=aai.LemurModel.claude3_5_sonnet)

for qa in result.response:
    print(qa.question)
    print(qa.answer)
```

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

LeMUR features are currently only available to paid users at two pricing tiers: LeMUR and LeMUR Basic. See the [pricing page](https://www.assemblyai.com/pricing#:~:text=LeMUR) for more detail.

## Step-by-Step Instructions

In this guide, we will ask three questions evaluating the prospecting performance of the sales representative. Each question has slightly different paremeters based on the use case but largely has a fixed `context` that we will apply to each question.

Import the `assemblyai` package and set your API key.

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
```

Use the `Transcriber` object's `transcribe` method and pass in the audio file's path as a parameter. The `transcribe` method will save the results of the transcription to the `Transcriber` object's `transcript` attribute.

```python
transcriber = aai.Transcriber()
transcript = transcriber.transcribe("./sales-call.mp3")
```

Define your LeMUR `context`, `answer_format`, and `answer_options` request variables.

```python
context = "There are sales interactions between a salesperson who is selling an internet plan to customers who are warm leads."
answer_format = """
                Answer with JSON in the following format:
                {
                    "Answer: "<answer_options>",
                    "Reason": "<justification for the answer in one sentence including quotes>"
                }
                """

answer_options = ["Yes", "No"]
```

Next, define your LeMUR request parameters for your sales playbook processes using the Question & Answer feature. Note: You can edit the variables to provide custom answers for each question.

```python
questions = [
    aai.LemurQuestion(
        question="Did the salesperson start the conversation with a professional greeting?",
        context=context,
        answer_format=answer_format,
        answer_options=["Poor", "Satisfactory", "Excellent"],
    ),
    aai.LemurQuestion(
        question="How well did the salesperson answer questions during the call?",
        context=context,
        answer_format=answer_format,
    ),
    aai.LemurQuestion(
        question="Did the salesperson discuss next steps clearly?",
        context=context,
        answer_format=answer_format,
        answer_options=answer_options,
    ),
]
```

Run the `question` method on `transcript` and print the result to your terminal.

```python
result = transcript.lemur.question(questions, final_model=aai.LemurModel.claude3_5_sonnet)

for qa in result.response:
    print(qa.question)
    print(qa.answer)
```


---
title: Pass Context from Previous LeMUR Requests
---

This guide will demonstrate how to use AssemblyAI's [LeMUR](https://www.assemblyai.com/blog/lemur/) (Leveraging Large Language Models to Understand Recognized Speech) framework to track previous LeMUR responses and pass them as additional context for future requests to build out ability to ask "follow up" questions.

<Tip>
  {" "}
  We recommend using [Claude 3
  Haiku](https://www.assemblyai.com/docs/lemur/customize-parameters) to reduce
  costs, as this can use a lot of tokens.{" "}
</Tip>

## Quickstart

```python
import assemblyai

assemblyai.settings.api_key = "YOUR_API_KEY"
transcriber = assemblyai.Transcriber()

transcript = transcriber.transcribe("https://storage.googleapis.com/aai-web-samples/5_common_sports_injuries.mp3") # You can also replace this with the path to a local file

# Define the initial prompt.
initial_prompt = input("Enter the initial prompt: ")

# Apply LeMUR.
result = transcript.lemur.task(initial_prompt)

# Store result in a list.
results_list = [result.response]

print(result.response)

while True:
    user_prompt = input("Enter the next prompt (or type 'end' to stop): ")

    if user_prompt.lower() == 'end':
        print("Ending the conversation.")
        break

    context = " ".join(results_list)
    next_prompt = f"{user_prompt} (Context: {context})"

    next_result = transcript.lemur.task(next_prompt, final_model=assemblyai.LemurModel.claude3_5_sonnet)

    results_list.append(next_result.response)

    print(next_result.response)
```

## Getting Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://www.assemblyai.com/dashboard/signup/) for an AssemblyAI account and get your API key from your [dashboard](https://www.assemblyai.com/app/account).

First, let's install the AssemblyAI SDK.

```bash
pip install -U assemblyai
```

Then we'll import the SDK and set our AssemblyAI API key.

```python
import assemblyai

assemblyai.settings.api_key = "API_KEY_HERE"
```

Next, we'll use AssemblyAI to transcribe a file and save our transcript for later use.

```python
transcriber = assemblyai.Transcriber()

transcript = transcriber.transcribe("https://storage.googleapis.com/aai-web-samples/5_common_sports_injuries.mp3")
```

Then we define the initial LeMUR prompt for the Task endpoint and store the first response in a list. These list values will be recalled and added to subsequent LeMUR prompts.

```python
# Step 2: Define the initial prompt.
initial_prompt = input("Enter the initial prompt: ")

# Step 3: Apply LeMUR.
result = transcript.lemur.task(initial_prompt)

# Step 4: Store result in a list.
results_list = [result.response]

print(result.response)
```

After the intitial prompt, the code will continuously prompt the user for new prompts until they decide to stop. Stop at any time by typing 'end'. Each response is collected in the `results_list` and added as context to the next user prompt. This enables an ability to recall past responses and allow follow up questions.

```python
while True:
    user_prompt = input("Enter the next prompt (or type 'end' to stop): ")

    if user_prompt.lower() == 'end':
        print("Ending the conversation.")
        break

    context = " ".join(results_list)
    next_prompt = f"{user_prompt} (Context: {context})"

    next_result = transcript.lemur.task(next_prompt, final_model=assemblyai.LemurModel.claude3_5_sonnet)

    results_list.append(next_result.response)

    print(next_result.response)
```

To see these results with and without the context provided from past responses, here is a comparison table.

| Prompt number |                                                Prompt                                                 |                                                                                                                                                                                                                                                                                  With Context                                                                                                                                                                                                                                                                                  |                                                                                                                                                      Without Context                                                                                                                                                       |
| :-----------: | :---------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|       1       | create a brief story and character called Jenna using one of the injuries described in the transcript |                                                                                                                                       Here is a brief story about a character named Jenna who suffers from a sprained ankle: Jenna loved playing soccer more than anything. She had been on her high school's varsity team since freshman year and was now a senior. Jenna hoped to play soccer in college next year on a scholarship. …                                                                                                                                       |             Here is a brief story about a character named Jenna who suffers from a sprained ankle: Jenna loved playing soccer more than anything. She had been on her high school's varsity team since freshman year and was now a senior. Jenna hoped to play soccer in college next year on a scholarship. …             |
|       2       |                                 tell me about jenna's comeback story                                  |                     Here is a possible comeback story for Jenna after suffering a sprained ankle: Jenna was heartbroken when she got the news that her badly sprained ankle would keep her out for the rest of the soccer season. All her dreams of playing college soccer felt like they were slipping away. But Jenna was determined not to give up. She diligently went to physical therapy sessions and did rehab exercises at home. Even though she couldn't be on the field with her team, Jenna supported them from the sidelines at every game. …                      | Unfortunately there is no mention of a person named Jenna or a comeback story in the provided transcripts. The transcripts discuss various sports injuries like runner's knee, sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. There are no details provided about anyone named Jenna or a comeback story. |
|       3       |                                             Who is Jenna?                                             | Based on the provided context, Jenna is a high school student who loves playing soccer and hopes to play in college on a scholarship. She suffered a badly sprained ankle during a rivalry game which kept her out for the rest of the season, jeopardizing her college soccer dreams. However, she worked hard in rehab and physical therapy to come back stronger the next season, eventually earning several college scholarship offers and committing to play soccer at her dream school. The injury ended up only being a temporary setback in Jenna achieving her goals. |                                   Based on the transcripts, there is no mention of anyone named Jenna. The transcripts discuss various sports injuries like runner's knee, sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. No individuals are named in the transcripts.                                    |


---
title: Automatically generate action items from a meeting with LeMUR
hide-nav-links: true
description: Generate action items from a meeting transcript using LeMUR
---

In this guide, you'll learn how to use AssemblyAI [LeMUR](https://www.assemblyai.com/docs/api-reference/lemur/task) to process an audio file and then automatically generate action items from the meeting's transcript.

## Overview

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

LeMUR features are currently only available to paid users at two pricing tiers, LeMUR Default and LeMUR Basic. Refer to our [pricing page](https://www.assemblyai.com/pricing) for more detail.

## Step-by-step instructions

In this guide, you'll use AssemblyAI to transcribe a meeting recording. Then, you'll request a list of action items from LeMUR.

<Steps>
<Step>
<Tabs>
  <Tab python-sdk typescript>

Install the SDK.

  </Tab>
  <Tab fallback>

Create a new file and

  </Tab>
</Tabs>

<Tabs groupId="language">

  <Tab language="python-sdk" title="Python SDK" default>

```python
pip install -U assemblyai
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
npm install assemblyai
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com/v2";

$headers = array(
  "authorization: <YOUR_API_KEY>",
  "content-type: application/json"
);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
require 'net/http'
require 'json'
require 'rest-client'
require 'httparty'

base_url = "https://api.assemblyai.com/v2"

headers = {
    "authorization" => "<YOUR_API_KEY>",
    "content-type" => "application/json"
}
```

  </Tab>

<Tab language="csharp" title="C#">

```csharp
using System.Net.Http;
using System.Threading;

string apiKey = "<YOUR_API_KEY>";
```

</Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Import the `assemblyai` package and set the API key.

  </Tab>

  <Tab language="typescript" title="TypeScript">

Import the `assemblyai` package.

  </Tab>
  <Tab fallback>

Upload your local file to the AssemblyAI API.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript

```

  </Tab>

  <Tab language="php" title="PHP">

```php
$path = "meeting.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
path = "meeting.mp3"
response = RestClient.post("#{base_url}/upload", File.read(path), headers)
upload_url = JSON.parse(response.body)["upload_url"]
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
public async Task<string> UploadFileAsync(string apiKey, string path)
{
    using var client = new HttpClient();
    client.DefaultRequestHeaders.Authorization = new System.Net.Http.Headers.AuthenticationHeaderValue(apiKey);

    using var fileContent = new ByteArrayContent(File.ReadAllBytes(path));
    fileContent.Headers.ContentType = new System.Net.Http.Headers.MediaTypeHeaderValue("application/octet-stream");

    HttpResponseMessage response;
    try
    {
        response = await client.PostAsync("https://api.assemblyai.com/v2/upload", fileContent);
    }
    catch (Exception e)
    {
        Console.Error.WriteLine($"Error: {e.Message}");
        return null;
    }

    if (response.IsSuccessStatusCode)
    {
        string responseBody = await response.Content.ReadAsStringAsync();
        var json = JObject.Parse(responseBody);
        return json["upload_url"].ToString();
    }
    else
    {
        Console.Error.WriteLine($"Error: {response.StatusCode} - {response.ReasonPhrase}");
        return null;
    }
}
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Create a `Transcriber` object.

  </Tab>
  <Tab language="typescript" title="TypeScript">

Set your API key.

  </Tab>

  <Tab fallback>

Submit a transcription with the `audio_url` parameter set to the value of `upload_url`.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
transcriber = aai.Transcriber()
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const client = new AssemblyAI({
  apiKey = "<YOUR_API_KEY>",
});
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$data = array(
    "audio_url" => upload_url
);

$url = $base_url . "/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

curl_close($curl);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
data = {
    "audio_url" => upload_url
}

uri = URI.parse("#{base_url}/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
var data = new Dictionary<string, dynamic>(){
    { "audio_url", upload_url }
};

using (var client = new HttpClient())
{
    client.DefaultRequestHeaders.Add("authorization", apiKey);
    var content = new StringContent(JsonConvert.SerializeObject(data), Encoding.UTF8, "application/json");
    HttpResponseMessage response = await client.PostAsync("https://api.assemblyai.com/v2/transcript", content);
    var responseContent = await response.Content.ReadAsStringAsync();
    var responseJson = JsonConvert.DeserializeObject<dynamic>(responseContent);
}
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Pass the URL or file path to `transcriber.transcribe()`. You can access the transcript from the returned `transcript` object.

  </Tab>
  <Tab language="typescript" title="TypeScript">

Pass the URL or file path to `client.transcripts.transcribe()`. You can access the transcript from the returned `transcript` object.

  </Tab>

  <Tab fallback>

After making the request, you'll receive an ID for the transcription. Use it to poll the API every few seconds to check the status of the transcript job. Once the status is `completed`, you can retrieve the transcript from the API response.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
transcript = transcriber.transcribe("meeting.mp3")
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const transcript = await client.transcripts.transcribe({
  audio: "meeting.mp3",
});
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$transcript_id = $response['id'];
$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === 'completed') {
        break;
    } else if ($transcription_result['status'] === 'error') {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
transcript_id = response.parsed_response["id"]
polling_endpoint = "https://api.assemblyai.com/v2/transcript/#{transcript_id}"

while true
    polling_response = HTTParty.get(polling_endpoint, headers: headers)
    transcription_result = polling_response.parsed_response

    if transcription_result["status"] == "completed"
        break
    elsif transcription_result["status"] == "error"
        raise "Transcription failed: #{transcription_result["error"]}"
    else
        sleep(3)
    end
end
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
string transcriptId = response.Content.ReadAsAsync<dynamic>().Result.id;
string pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcriptId}";

while (true) {
    var pollingRequest = new HttpRequestMessage {
        Method = HttpMethod.Get,
        RequestUri = new Uri(pollingEndpoint),
        Headers = {
            { "authorization", "<YOUR_API_KEY>" }
        }
    };

    var pollingResponse = httpClient.SendAsync(pollingRequest).Result;
    var transcriptionResult = JObject.Parse(pollingResponse.Content.ReadAsStringAsync().Result);

    if (transcriptionResult["status"].ToString() == "completed") {
        break;
    } else if (transcriptionResult["status"].ToString() == "error") {
        throw new Exception($"Transcription failed: {transcriptionResult["error"]}");
    } else {
        Thread.Sleep(3000);
    }
}
```

  </Tab>

</Tabs>

</Step>
<Step>

Define a prompt to tell LeMUR to extract action items from this meeting.

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
prompt = """This is a Product team meeting from 2019-07-09.
            Generate Action Items from the meeting in the following format:

            Action Item Title: A brief, descriptive title that summarizes the action item.
            Assignee: The person who is responsible for completing the action item.
            Due Date: The deadline for completing the action item.
            Status: The current status of the action item (e.g., "In progress", "Completed", "Deferred").
            Notes: Any additional notes or information about the action item."""
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const prompt = `This is a Product team meeting from 2019-07-09.
                Generate Action Items from the meeting in the following format:

                Action Item Title: A brief, descriptive title that summarizes the action item.
                Assignee: The person who is responsible for completing the action item.
                Due Date: The deadline for completing the action item.
                Status: The current status of the action item (e.g., "In progress", "Completed", "Deferred").
                Notes: Any additional notes or information about the action item.`;
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$prompt = <<<EOD
This is a Product team meeting from 2019-07-09.
Generate Action Items from the meeting in the following format:

Action Item Title: A brief, descriptive title that summarizes the action item.
Assignee: The person who is responsible for completing the action item.
Due Date: The deadline for completing the action item.
Status: The current status of the action item (e.g., "In progress", "Completed", "Deferred").
Notes: Any additional notes or information about the action item.
EOD;
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
prompt = "This is a Product team meeting from 2019-07-09." \
         "Generate Action Items from the meeting in the following format:" \

         "Action Item Title: A brief, descriptive title that summarizes the action item." \
         "Assignee: The person who is responsible for completing the action item." \
         "Due Date: The deadline for completing the action item." \
         "Status: The current status of the action item (e.g., 'In progress', 'Completed', 'Deferred')." \
         "Notes: Any additional notes or information about the action item."
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
string prompt = @"This is a Product team meeting from 2019-07-09.
                Generate Action Items from the meeting in the following format:

                Action Item Title: A brief, descriptive title that summarizes the action item.
                Assignee: The person who is responsible for completing the action item.
                Due Date: The deadline for completing the action item.
                Status: The current status of the action item (e.g., 'In progress', 'Completed', 'Deferred').
                Notes: Any additional notes or information about the action item.";
```

  </Tab>

</Tabs>

</Step>
<Step>

<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Pass your `prompt` to `transcript.lemur.task()` and access the response from LeMUR by printing the `.response` attribute.

  </Tab>

  <Tab language="typescript" title="TypeScript">

Pass your `prompt` and transcript ID to `client.lemur.task()` and access the response from LeMUR by printing the `.response` attribute.

  </Tab>
  <Tab fallback>

Create a JSON payload containing the transcript ID and your `prompt` as part of a POST request to the LeMUR Task endpoint.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
result = transcript.lemur.task(prompt)

print(result.response)
```

  </Tab>

  <Tab language="typescript" title="TypeScript">

```typescript
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
});

console.log(response);
```

  </Tab>

  <Tab language="php" title="PHP">

```php
$data = array(
    "transcript_ids" => [$transcript_id],
    "prompt" => $prompt
);

$url = "https://api.assemblyai.com/lemur/v3/generate/task";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURL_RETURNTRANSFER, true);

$result = json_decode(curl_exec($curl), true);
curl_close($curl);

echo $result["response"];
```

  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
data = {
    "transcript_ids" => [transcript_id],
    "prompt" => prompt
}

uri = URI.parse("https://api.assemblyai.com/lemur/v3/generate/task")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

result = http.request(request)
puts result.parsed_response["response"]
```

  </Tab>

  <Tab language="csharp" title="C#">

```csharp
var data = new Dictionary<string, dynamic>(){
    { "transcript_ids", new string [] {transcriptId} },
    { "prompt", prompt }
};

using (var client = new HttpClient())
{
    client.DefaultRequestHeaders.Add("authorization", apiKey);
    var content = new StringContent(JsonConvert.SerializeObject(data), Encoding.UTF8, "application/json");
    HttpResponseMessage response = await client.PostAsync("https://api.assemblyai.com/lemur/v3/generate/task", content);
    var responseContent = await response.Content.ReadAsStringAsync();
    var result = JsonConvert.DeserializeObject<dynamic>(responseContent);

    Console.WriteLine(response.Content.ReadAsAsync<dynamic>().Result.response);
}
```

  </Tab>

</Tabs>

</Step>
</Steps>

## Understanding the response

The result of your LeMUR output should be something like this:

```plain
Action Item Title: Review competitor product features
Assignee: Connor
Due Date: 2019-07-16
Status: In progress
Notes: Compare our product features against our main competitors. Look for gaps and areas we can improve.

Action Item Title: Plan social media campaign
Assignee: Rohan
Due Date: 2019-07-123
Status: Not started
Notes: Draft a proposal for an Instagram/Facebook campaign to promote our new product launch.

Action Item Title: Contact suppliers about part shortages
Assignee: Jason
Due Date: 2019-07-12
Status: Completed
Notes: Reached out to 3 suppliers about expected delivery dates for backordered parts. Parts should arrive by end of month.

Action Item Title: Set up product demo for key customers
Assignee: Matt
Due Date: 2019-08-15
Status: In progress
Notes: Identify 5-10 key customers to invite to product demo day. Send invites by end of month.
```


---
title: Segment A Phone Call using LeMUR
---

In this guide we will show you how to use AssemblyAI's LLM, [LeMUR](https://www.assemblyai.com/docs/lemur/examples), to segment a phone call.

## Quickstart

```python
import assemblyai as aai

aai.settings.api_key = "YOUR_API_KEY"
transcriber = aai.Transcriber()
audio_url = "YOUR_AUDIO_URL" # You can also provide a path to a local audio file

transcript = transcriber.transcribe(audio_url)

phases = [
    "Introduction",
    "Complaint",
    "Resolution",
    "Goodbye"
]

prompt = f'''
Analyze the following transcript of a phone call conversation and divide it into the following phases:
{', '.join(phases)}

You will be given the transcript in the format of VTT captions.

For each phase:
1. Identify the start and end timestamps (in seconds)
2. Provide a brief summary of what happened in that phase

Format your response as a JSON object with the following structure:
{{
    "phases": [
        {{
            "name": "Phase Name",
            "start_time": start_time_in_seconds,
            "end_time": end_time_in_seconds,
            "summary": "Brief summary of the phase"
        }},
        ...
    ]
}}

Ensure that all parts of the conversation are covered by a phase, using "Other" for any parts that don't fit into the specified phases.
'''

result = aai.Lemur().task(
    input_text=transcript.export_subtitles_vtt(),
    prompt=prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

print(result.response)
```

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard. You will need to upgrade your account by adding a credit card to have access to LeMUR.

Find more details on the current LeMUR pricing in the AssemblyAI [pricing page](https://www.assemblyai.com/pricing).

## Step-by-Step Instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the SDK and set your AssemblyAI API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Use AssemblyAI to transcribe a file and save the transcript.

```python
audio_url = "YOUR_AUDIO_URL"

transcript = aai.Transcriber().transcribe(audio_url)
```

Define the phases you want to identify.

Here is an example of `phases` that can be used for customer support calls:

```python
phases = [
    "Introduction",
    "Complaint",
    "Resolution",
    "Goodbye"
]
```

Prompt LeMUR using the Task Endpoint, analyze the transcript to divide it into phases, and return the response. This is an example prompt, which you can modify to suit your specific requirements. See our documentation for more information about [prompt engineering](https://www.assemblyai.com/docs/lemur/improving-your-prompt).

```python
prompt = f'''
Analyze the following transcript of a phone call conversation and divide it into the following phases:
{', '.join(phases)}

You will be given the transcript in the format of VTT captions.

For each phase:
1. Identify the start and end timestamps (in seconds)
2. Provide a brief summary of what happened in that phase

Format your response as a JSON object with the following structure:
{{
    "phases": [
        {{
            "name": "Phase Name",
            "start_time": start_time_in_seconds,
            "end_time": end_time_in_seconds,
            "summary": "Brief summary of the phase"
        }},
        ...
    ]
}}

Ensure that all parts of the conversation are covered by a phase, using "Other" for any parts that don't fit into the specified phases.
'''

result = aai.Lemur().task(
    input_text=transcript.export_subtitles_vtt(),
    prompt=prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)

print(result.response)
```

Example output of the analysis of a transcript divided into phases and formatted as a JSON object:

```

{
    "phases": [
        {
            "name": "Introduction",
            "start_time": 1.52,
            "end_time": 15.57,
            "summary": "The customer service representative greets the caller and asks how they can help. The caller states they want to know the status of their order refund."
        },
        {
            "name": "Complaint",
            "start_time": 15.57,
            "end_time": 59.41,
            "summary": "The representative asks for the order ID, which the caller provides. The representative confirms the order details and that it was cancelled. The caller mentions they couldn't complete their test."
        },
        {
            "name": "Resolution",
            "start_time": 59.41,
            "end_time": 210.01,
            "summary": "The representative informs the caller that the refund was initiated on April 8th and will be credited by April 21st. They explain the refund timeline and bank processing days. The caller expresses some confusion about the timeline, and the representative clarifies the process."
        },
        {
            "name": "Goodbye",
            "start_time": 210.01,
            "end_time": 235.8,
            "summary": "The caller accepts the explanation. The representative asks if there's anything else they can help with, requests feedback, and concludes the call with a farewell."
        }
    ]
}
```


---
title: Generate SOAP Notes using LeMUR
---

The acronym SOAP stands for Subjective, Objective, Assessment, and Plan. This standardized method of documenting patient encounters allows providers to concisely record patient information. This guide walks through how to generate SOAP notes with AssemblyAI's LLM, [LeMUR](https://www.assemblyai.com/docs/lemur/examples).

## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard. You will need to upgrade your account by adding a credit card to have access to LeMUR.

Find more details on the current LeMUR pricing in the AssemblyAI [pricing page](https://www.assemblyai.com/pricing).

## Step-by-Step Instructions

Install the SDK:

```bash
pip install assemblyai
```

Import the SDK and set your AssemblyAI API key.

```python
import assemblyai as aai

aai.settings.api_key = "API_KEY"
```

Initialize your `Transcription Config` variable.

```python
config = aai.TranscriptionConfig()
```

Optionally, enable the PII Redaction model. This will remove any personal info from the transcript & SOAP note.

```python
config.set_redact_pii = True
config.set_redact_pii_policies = [
    aai.PIIRedactionPolicy.banking_information,
    aai.PIIRedactionPolicy.credit_card_cvv,
    aai.PIIRedactionPolicy.credit_card_expiration,
    aai.PIIRedactionPolicy.credit_card_number,
    aai.PIIRedactionPolicy.date_of_birth,
    aai.PIIRedactionPolicy.phone_number,
    aai.PIIRedactionPolicy.us_social_security_number,
    aai.PIIRedactionPolicy.drivers_license,
    aai.PIIRedactionPolicy.location,
    aai.PIIRedactionPolicy.political_affiliation,
    aai.PIIRedactionPolicy.person_name,
    aai.PIIRedactionPolicy.organization,
    aai.PIIRedactionPolicy.email_address,
    # aai.PIIRedactionPolicy.person_age,
    # aai.PIIRedactionPolicy.religion,
    # aai.PIIRedactionPolicy.occupation,
    # aai.PIIRedactionPolicy.nationality,
    # aai.PIIRedactionPolicy.medical_process,
    # aai.PIIRedactionPolicy.medical_condition,
    # aai.PIIRedactionPolicy.blood_type,
    # aai.PIIRedactionPolicy.drug,
    # aai.PIIRedactionPolicy.injury,
]
```

Initialize your `transcriber` variable and transcribe your file.

```python
transcriber = aai.Transcriber(config=config)

# Replace this with the path to your local file or your file URL.
transcript = transcriber.transcribe("example.mp3")

print(transcript.id)
```

## Two different methods of generating SOAP notes

### LeMUR Summary

This method uses the [Summary endpoint](https://www.assemblyai.com/docs/lemur/summarize-audio#summary-with-specialized-endpoint) to generate the SOAP note in its entirety. The LeMUR prompt includes some context about the audio file, a specific format for the SOAP note, and more context on what each category of the note should include.

```python
result = transcript.lemur.summarize(
    context="this is a doctor appointment between patient and provider. the personal identification information has been redacted",
    answer_format="""
    Generate a SOAP note summary in the following format:

    Subjective
    This is typically the shortest section (only 2-3 sentences) and it describes the patient's affect, as the
professional sees it. This information is all subjective (it isn't measureable).
    - Include information that may have affected the patient's performance, such as if they were sick, tired,
attentive, distractible, etc.
    - Was the patient on time or did they come late?
    - May include a quote of something the patient said, or how they reported feeling

    Objective
    This section includes factual, measurable, and objective information. This may include:
    - Direct patient quotes
    - Measurements
    - Data on patient performance

    Assessment
    This section should be the meat of the SOAP note. It contains a narrative of what actually happened during the
session. There may be information regarding:
    - Whether improvements have been made since the last session
    - Any potential barriers to success
    - Clinician's interpretation of the results of the session

    Plan
    This is another short section that states the plan for future sessions. In most settings, this section may be
    bulleted
    """
).response

print(result.strip())
```

### LeMUR Q&A

This method uses the [Q&A endpoint](https://www.assemblyai.com/docs/lemur/ask-questions#qa-with-specialized-endpoint) to generate each section of the SOAP note separately. This makes it easy to regenerate one or more of the categories individually if the user chooses to. The LeMUR prompt includes a question, some context on the question (with an example) and a specified answer format.

```python
questions = [
    aai.LemurQuestion(
        question="What are the patient's current symptoms or concerns?",
        context="""
            Gather information about the patient's subjective experience.
            Example: The patient reports experiencing persistent headaches and dizziness.
            """,
        answer_format="<patient's symptoms or concerns>, [exact quote from patient]"
    ),
    aai.LemurQuestion(
        question="What are the measurable and observable findings from the examination?",
        context="""
            Collect data on the patient's objective signs and measurements.
            Example: The examination reveals an elevated body temperature and increased heart rate.
            """,
        answer_format="<measurable and observable findings>"
    ),
    aai.LemurQuestion(
        question="Based on the patient's history and examination, what is your assessment or diagnosis?",
        context="""
            Formulate a professional assessment based on the gathered information.
            Example: Based on the patient's symptoms, examination, and medical history, the preliminary diagnosis is migraine.
            """,
        answer_format="<assessment or diagnosis>"
    ),
    aai.LemurQuestion(
        question="What is the plan of action or treatment for the patient?",
        context="""
            Outline the intended course of action or treatment.
            Example: The treatment plan includes prescribing medication, recommending rest, and scheduling a follow-up appointment in two weeks.
            """,
        answer_format="<plan of action or treatment>, [exact quote from provider]"
    )
]
result = transcript.lemur.question(
    questions=questions,
    context="this is a doctor appointment between patient and provider. the personal identification information has been redacted",
    max_output_size=4000
).response

for x in result:
    print(x.question)
    print(x.answer)
    print()
```

